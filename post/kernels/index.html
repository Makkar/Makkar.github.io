<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Aditya Makkar">

  
  
  
    
  
  <meta name="description" content="Functional analysis background for kernel theory.">

  
  <link rel="alternate" hreflang="en-us" href="https://makkar.github.io/post/kernels/">

  


  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata:wght@200;300;400;500;600;700;800;900&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://makkar.github.io/post/kernels/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Aditya Makkar">
  <meta property="og:url" content="https://makkar.github.io/post/kernels/">
  <meta property="og:title" content="Kernels - Part 0 | Aditya Makkar">
  <meta property="og:description" content="Functional analysis background for kernel theory."><meta property="og:image" content="https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-07-03T21:35:10-04:00">
    
    <meta property="article:modified_time" content="2020-07-03T21:35:10-04:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://makkar.github.io/post/kernels/"
  },
  "headline": "Kernels - Part 0",
  
  "datePublished": "2020-07-03T21:35:10-04:00",
  "dateModified": "2020-07-03T21:35:10-04:00",
  
  "author": {
    "@type": "Person",
    "name": "Aditya Makkar"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Aditya Makkar",
    "logo": {
      "@type": "ImageObject",
      "url": "https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Functional analysis background for kernel theory."
}
</script>

  

  


  


  





  <title>Kernels - Part 0 | Aditya Makkar</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/hilbert/"><span>Hilbert's Hotel - Blog</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Kernels - Part 0</h1>

  
  <p class="page-subtitle">Functional analysis background for kernel theory.</p>
  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jul 3, 2020
  </span>
  

  

  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\conj}[1]{\overline{#1}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
$$</p>
<div style="text-align: justify">
<h1 id="introduction">Introduction</h1>
<p>The word &ldquo;kernel&rdquo; is 
<a href="https://en.wikipedia.org/wiki/Kernel" target="_blank" rel="noopener">heavily overloaded</a>, but for our purposes it is, intuitively, a similarity measure that can be thought of as an inner product in some feature space. Kernel methods provide an elegant, theoretically well-founded, and powerful approach to solving many learning problems.</p>
<p>We usually have the following framework: The input space $\X$ which contains our observations/inputs/features is either not rich enough (for example, if there is no linear boundary separating the two classes in a binary classification problem) or not convenient (for example, if our inputs are strings), and therefore we want to work is some other space we call feature space $\H$. Suppose we have a map which takes our inputs from $\X$ to $\H$:
$$ \Phi : \X \to \H $$
Then the class of kernels we are interested in are those for which is it possible to write
$$ K(x,y) = \langle \Phi(x), \Phi(y) \rangle, \quad x,y \in \X $$
What kind of functions, $K : \H \times \H \to \C$ admit such a representation? We aim to be able to answer this question and many others in this series of articles on kernels.</p>
<p>In this article, I aim to introduce the necessary concepts from analysis, linear algebra, and functional analysis so as to understand Reproducing Kernel Hilbert Spaces (RKHS) and a few of their basic properties. In the 
<a href="http://makkar.github.io/post/kernels1/" target="_blank" rel="noopener">next article</a> I will discuss the kernel trick and some important theorems like Mercer&rsquo;s theorem and Representer theorem.</p>
<h1 id="background">Background</h1>
<p>A topic like this requires quite a bit of background before we can get to the interesting results like the one above. It&rsquo;s always easy to get lazy and assume all the necessary background from the reader and get straight to the meat, but I want to write an article which I would have found useful had I had it when I started learning about kernel theory. With that being said, I am under no illusion and believe that a much better way to learn this background would be to read a functional analysis text if you have the time.</p>
<h2 id="linear-spaces">Linear spaces</h2>
<p><strong>Definition 1:</strong> A <em>linear space</em>, or alternatively a <em>vector space</em>, over a field $\F$ ($\F$ is $\R$ or $\C$ for our purposes) is a set $V$ of elements called <em>vectors</em> (the elements of $\F$ are called <em>scalars</em>) satisfying:</p>
<p>(A) To every pair, $x$ and $y$, of vectors in $V$ there corresponds a vector $x+y$, called the sum of $x$ and $y$, in such a way that</p>
<ol>
<li>addition is commutative, $x+y = y+x$,</li>
<li>addition is associative, $x+(y+z) = (x+y)+z$,</li>
<li>there exists in $V$ a unique vector $0$ such that $x+0=x$ for every vector $x$, and</li>
<li>to every vector $x \in V$ there corresponds a unique vector $-x$ such that $x+(-x)=0$.</li>
</ol>
<p>(B) To every pair, $\alpha \in \F$ and $x \in V$, there corresponds a vector $\alpha x \in V$, called the product of $\alpha$ and $x$, in such a way that</p>
<ol>
<li>multiplication by scalars is associative, $\alpha(\beta x) = (\alpha \beta)x$, and</li>
<li>$1x = x$ for every vector $x$.</li>
</ol>
<p>(C) Finally the distributive properties</p>
<ol>
<li>$\alpha(x+y) = \alpha x + \alpha y$, and</li>
<li>$(\alpha + \beta) x = \alpha x + \beta x$.</li>
</ol>
<h3 id="examples">Examples</h3>
<ol>
<li>The Euclidean spaces $\R^n$ are vector spaces over the real field.</li>
<li>$\C^n$ are vector spaces over $\C$.</li>
<li>The set of all polynomials, with complex coefficients, in a variable $t$ is a vector space over $\C$.</li>
<li>The set $C$ of all continuous complex functions on the unit interval $[0,1]$ is a vector space over $\C$.</li>
</ol>
<p><strong>Definition 2:</strong> A <em>linear transformation</em> of a linear space $V$ into a linear space $W$ is a mapping $T: V \to W$ such that
$$ T(\alpha x + \beta y) = \alpha T(x) + \beta T(y), \quad  (x,y \in V;\; \alpha, \beta \in \F) $$</p>
<p><strong>Definition 3:</strong> In the special case in which $W$ above is a field, $T$ is called a <em>linear functional</em>.</p>
<p>Note that we often write $Tx$ instead of $T(x)$, if $T$ is linear, hinting that a linear transformation mapping a finite dimensional vector to another finite dimensional vector space is equivalent to a matrix vector product.</p>
<p><strong>Definition 4:</strong> Let $\mu$ be a positive measure on an arbitrary measurable space $X$. We define $L^1(\mu)$ to be the collection of all complex measurable functions $f$ on $X$ for which
$$ \int_X |f| \dmu &lt; \infty $$</p>
<p>It can be shown that for every $f, g \in L^1(\mu)$ and for every $\alpha, \beta \in \C$, we have $\alpha f + \beta g \in L^1(\mu)$, and
$$ \int_X (\alpha f + \beta g) \dmu = \alpha \int_X f \dmu + \beta \int_X g \dmu $$</p>
<p>Thus, $L^1(\mu)$ is a vector space, and the mapping $F: L^1(\mu) \to \R$ defined by
$$ F(f) = \int_X |f| \dmu, \quad f \in L^1(\mu) $$
is a linear functional.</p>
<h2 id="inner-products-and-norms">Inner products and norms</h2>
<p><strong>Definition 5:</strong> If $V$ be a linear space over $\C$, an <em>inner product</em> on $V$ is a function $\langle \cdot , \cdot \rangle: V \times V \to \C$ such that for all $\alpha, \beta \in \C$, and all $x,y,z \in V$, the following are satisfied:</p>
<ol>
<li>Linearity in the first argument: $\langle \alpha x + \beta y, z \rangle = \alpha \langle x,z \rangle + \beta \langle y,z \rangle$,</li>
<li>Conjugate symmetry: $\inner{x}{y} = \conj{\inner{y}{x}}$,</li>
<li>Positivity: $\inner{x}{x} \geq 0$,</li>
<li>If $\inner{x}{x} = 0$, then $x=0$.</li>
</ol>
<p>A function satisfying only the first three properties is called a <em>semi-inner product</em> on $V$.</p>
<p>An immediate consequences of this definition: For every $y \in V$, the mapping $F: V \to \C$ defined by
$$ F(x) = \inner{x}{y}, \quad (x \in V) $$
is a linear functional on $V$.</p>
<p><strong>Definition 6:</strong> If $V$ be a linear space over $\C$, a <em>norm</em> on $V$ is a non-negative function $\norm{\cdot} : V \to \R$ such that for all $\alpha \in \C$, and all $x,y \in V$, the following are satisfied:</p>
<ol>
<li>Subadditivity: $\norm{x+y} \leq \norm{x} + \norm{y}$,</li>
<li>Absolutely homogenous: $\norm{\alpha x} = |\alpha| \norm{x}$,</li>
<li>Positive definite: $\norm{x} = 0 \implies x = 0$.</li>
</ol>
<p>Given an inner product, we can define a <em>norm</em> as follows:
$$ \norm{x} = \sqrt{\inner{x}{x}} $$</p>
<p>A classic result useful in many proofs:</p>
<p><strong>Theorem 1:</strong> Schwarz inequality
$$|\inner{x}{y}| \leq \norm{x} \, \lVert y \rVert$$
Equality hold for $y = \alpha x$ or $y=0$.</p>
<p>The proof is not too difficult.</p>
<p><strong>Definition 7:</strong> The virtue of norm on a vector space $V$ is that
$$ d(x,y) := \norm{ x-y } $$
defines a <em>metric</em> on $V$ so that $V$ becomes a metric space.</p>
<p>Note that technically the tuple $(V, d)$ defines a metric space, but I will often write just $V$ instead of $(V, d)$ when it&rsquo;s clear from context what the metric $d$ is.</p>
<p><strong>Definition 8:</strong> If $0 &lt; p &lt; \infty$, $f$ is a complex measurable function on $X$, and $\mu$ is a nonnegative measure on $X$, define
$$ \norm{f}_p := \left( \int_X |f|^p \dmu \right)^{1/p} $$
and let $L^p(\mu)$ consist of all $f$ for which $\norm{f}_p &lt; \infty$. We call $\norm{f}_p$ the $L^p$-norm of $f$.</p>
<h2 id="hilbert-spaces">Hilbert spaces</h2>
<p><strong>Definition 9:</strong> An <em>inner product space</em>, or alternatively a <em>pre-Hilbert space</em>, is a linear space with an inner product defined on it.</p>
<p>We need the concept of completeness to define Hilbert space. But before that let me define Cauchy sequences.</p>
<p><strong>Definition 10:</strong> Given a metric space $(M, d)$, a sequence $(x_n)_{n \in \N}$ of elements in $M$ is called a <em>Cauchy sequence</em> if for every positive real number $\e &gt; 0$ there exists a positive integer $N \in \N$ such that $m, n &gt; N$ implies that $d(x_m, x_n) &lt; \e$.</p>
<p>Recall that we say a sequence $(x_n)_{n \in \N}$ in a metric space $(M, d)$ <em>converges</em> if there exists a point $x \in M$ with the following property: for every $\e &gt; 0$ there exists a positive integer $N \in \N$ such that $n &gt; N$ implies that $d(x_n, x) &lt; \e$.</p>
<p>It can be shown that every convergent sequence is a Cauchy sequence: Let the sequence $(x_n)_{n \in \N}$ in a metric space $(M, d)$ converge to $x \in M$. If $\e &gt; 0$, there is an integer $N \in \N$ such that $d(x_n, x) &lt; \e$ for all $n &gt; N$. Hence</p>
<p>$$d(x_n, x_m) \leq d(x_n, x) + d(x, x_m) &lt; 2\e$$</p>
<p>if $n,m &gt; N$. Thus $(x_n)_{n \in \N}$ is a Cauchy sequence.</p>
<p>The converse is not necessarily true. But if it holds in some space, we anoint the space with a special name.</p>
<p><strong>Definition 11:</strong> A metric space $(M, d)$ is called <em>complete</em> if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.</p>
<p><strong>Definition 12:</strong> A pre-Hilbert space $\H$ is called a <em>Hilbert space</em> if it is complete in the metric $d$ (see Definition 7).</p>
<h3 id="examples-1">Examples</h3>
<ol>
<li>
<p>The sets $\R^n$ and $\C^n$ are Hilbert spaces if we define $\inner{x}{y} := \sum_{i=1}^n x_i \conj{y_i}$.</p>
</li>
<li>
<p>The set $C$ of all continuous complex functions on the unit interval $[0,1]$ defined above is an inner product space if we define
$$ \inner{f}{g} := \int_0^1 f(x) \conj{g(x)} \dx $$
but is not a Hilbert space.</p>
</li>
<li>
<p>$L^2(\mu)$ is a Hilbert space, with inner product
$$ \inner{f}{g} := \int_X f \, \conj{g} \dmu $$</p>
</li>
<li>
<p>The space of square-summable real-valued sequences, namely
$$ \ell^2(\N) := \left\{ (x_n)_{n \in \N} \; : \; x_n \in \R,\, \sum_n x_n^2 &lt; \infty \right\} $$</p>
<p>This set, when endowed with the inner product $\inner{x}{y} := \sum_{n \in \N} x_n y_n$, defines a Hilbert space. It will play an important role in our discussion of eigenfunctions for Reproducing Kernel Hilbert spaces.</p>
</li>
</ol>
<p><strong>Definition 13:</strong> Consider a linear space $\FF$ of functions each of which is a mapping from a set $X$ into $\F$. For $x \in X$, a <em>linear evaluation functional</em> is a linear functional $E_x$ that is defined as
$$ E_x(f) = f(x), \quad (f \in \FF) $$
In other words, a linear evaluation functional with respect to $x \in X$ evaluates each function at $x$.</p>
<p>In general, the evaluation functional is not continuous. This means we can have $f_n \to f$ but $E_x(f_n)$ does not converge to $E_x(f)$. Intuitively, this is because Hilbert spaces can contain very unsmooth functions. We will later consider a special type of Hilbert space, Reproducing Kernel Hilbert Space where evaluation functional is continuous.</p>
<p>A lemma that will be useful later on:</p>
<p><strong>Lemma 1:</strong> Let $\H$ be a Hilbert space and $L:\H \to \F$ a linear functional. The following statements are equivalent:</p>
<ol>
<li>$L$ is continuous.</li>
<li>$L$ is continuous at $0$.</li>
<li>$L$ is continuous at some point.</li>
<li>$L$ is bounded, i.e., there is a constant $c &gt; 0$ such that $|L(f)| \leq c \norm{f}$ for every $f \in \H$.</li>
</ol>
<p><strong>Proof:</strong> It is clear that $(1) \implies (2) \implies (3)$, and $(4) \implies (2)$. Let&rsquo;s show that $(3) \implies (1)$, and $(2) \implies (4)$.</p>
<p>$(3) \implies (1)$: Suppose $L$ is continuous at $f$ and $g$ is any point in $\H$. If $g_n \to g$ in $\H$, then $g_n - g + f \to f$. By assumption $L(f) = \limn L(g_n - g + f) = \limn L(g_n) - L(g) + L(f)$. Hence $L(g) = \limn L(g_n)$.</p>
<p>$(2) \implies (4)$: The definition of continuity at $0$ implies that $L^{-1}(\{\alpha \in \F : |\alpha| &lt; 1\})$ contains an open ball centered at $0$. Let $\delta &gt; 0$ be the radius of that open ball centered at $0$. Then for $f \in \H$ and $\norm{f} &lt; \delta$ we have $|L(f)| &lt; 1$. If $f$ is an arbitrary element of $\H$ and $\e &gt; 0$, then
$$ \left\lVert \frac{\delta f}{\norm{f} + \e} \right\rVert &lt; \delta $$
Hence,
$$ 1 &gt; \left\lvert L\left( \frac{\delta f}{\norm{f} + \e} \right) \right\rvert = \frac{\delta }{\norm{f} + \e} |L(f)| $$
Letting $\e \to 0$ we see that $(4)$ holds with $c = 1/\delta$. $\square$</p>
<h3 id="orthonormal-bases">Orthonormal bases</h3>
<p>We generalize the idea of orthonormal basis to infinite dimensional case. This will be needed when we discuss Mercer&rsquo;s theorem.</p>
<p><strong>Definition 14:</strong> A collection of vectors $\{v_{\alpha} \, : \, \alpha \in A \}$ in a Hilbert space $\H$ for some index set $A$ is called <em>orthonormal</em> if it satisfies $\inner{v_{\alpha}}{v_{\beta}} = \delta_{\alpha \beta}$ where $\delta_{\alpha \beta}$ is the Kronecker delta, which equals $1$ if $\alpha = \beta$ and $0$ otherwise.</p>
<p><strong>Definition 15:</strong> A collection of vectors $\{v_{\alpha} \, : \, \alpha \in A \}$ in a Hilbert space $\H$ is <em>complete</em> if for any $u \in \H$, $\inner{u}{v_{\alpha}} = 0$ for all $\alpha \in A$ implies that $u = 0$.</p>
<p><strong>Definition 16:</strong> An <em>orthonormal basis</em> a complete orthonormal system.</p>
<p>Note, we can also define an orthonormal basis as a maximal orthonormal set in $\H$. To say $\{v_{\alpha}\}$ is maximal means that no vector of $\H$ can be added to $\{v_{\alpha}\}$ in such a way that the resulting set is still orthonormal. This happens precisely when there is no $u \neq 0$ in $\H$ that is orthogonal to every $v_{\alpha}$.</p>
<h3 id="separable-hilbert-spaces">Separable Hilbert spaces</h3>
<p>Another idea we need is separability. Let us define that now.</p>
<p><strong>Definition 17:</strong> A topological space is called <em>separable</em> if it contains a countable, dense subset; that is, there exists a sequence $(x_{n})_{n=1}^{\infty }$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.</p>
<p><strong>Definition 18:</strong> A Hilbert space is separable if and only if it has a countable orthonormal basis. It follows that any separable, infinite-dimensional Hilbert space is isometric to the space $\ell^2(\N)$ of square-summable sequences.</p>
<p>We will be dealing with separable Hilbert spaces in our discussion.</p>
<h2 id="riesz-representation-theorem">Riesz representation theorem</h2>
<p>We now come to a very important theorem called the Riesz representation theorem. The name Riesz has 
<a href="https://en.wikipedia.org/wiki/Riesz_theorem" target="_blank" rel="noopener">many theorems</a> attached to it, but the one relevant to us is the following:</p>
<p><strong>Theorem 2:</strong> For each continuous linear functional $L$ on a Hilbert space $\H$, there exists a unique $g \in \H$ such that
$$L(f) = \inner{f}{g}\, , \quad (f \in \H) $$</p>
<p>I&rsquo;ll skip the proof as it&rsquo;s not easy and will unnecessarily make this article abstruse.</p>
<p>Side-note: In the mathematical treatment of quantum mechanics, this theorem can be seen as a justification for the popular bra–ket notation.</p>
<h1 id="reproducing-kernel-hilbert-spaces-rkhs">Reproducing Kernel Hilbert Spaces (RKHS)</h1>
<p><strong>Definition 19:</strong> Let $\X$ be a set. We will call a set $\H$ of functions from $\X$ to $\F$ a <em>Reproducing Kernel Hilbert Space (RKHS)</em> on $\X$ if</p>
<ol>
<li>$\H$ is a vector space,</li>
<li>$\H$ is endowed with an inner product, $\inner{\cdot}{\cdot}$, with respect to which $\H$ is a Hilbert space,</li>
<li>for every $x \in \X$, the linear evaluation functional $E_x : \H \to \F$, is bounded (or continuous, as seen from Lemma-1).</li>
</ol>
<p>If $\H$ is an RKHS on $\X$, then an application of the Riesz representation theorem shows that the linear evaluation functional is given by the inner product with a unique vector in $\H$. Therefore, for each $x \in \X$, there exists a unique vector $k_x \in \H$, such that for every $f \in \H$,
$$ f(x) = E_x(f) = \inner{f}{k_x} $$</p>
<p><strong>Definition 20:</strong> The function $k_x$ is called the <em>reproducing kernel for the point $x$</em>. The function $K: \X \times \X \to \F$ defined by
$$ K(x,y) = k_y(x) $$
is called the <em>reproducing kernel for $\H$</em>.</p>
<p>Note that we have
$$ K(x,y) = k_y(x) = \inner{k_y}{k_x} = \conj{\inner{k_x}{k_y}} = \conj{K(y,x)}$$</p>
<p>Also,
$$ \norm{E_y}^2 = \norm{k_y}^2 = \inner{k_y}{k_y} = K(y,y) $$</p>
<h3 id="example">Example</h3>
<p>The first question that comes to mind is if any Reproducing Kernel Hilbert Spaces exist. The following example answers this question in the affirmative.</p>
<p>We saw before that $\C^n$ is a Hilbert space. We can show that $\C^n$ is in fact an RKHS. Let $\X = \{1, 2, \ldots, n\}$, then we can view $v \in \C$ as a function $V : \X \to \C$, where $V(j) = v_j$. The linear evaluation functionals are of course bounded for every $x \in \X$ and we have
$$ V(j) = v_j = \inner{V}{e_j}\,, \quad (j \in \X) $$
where $e_j$ is a vector with $1$ at $j$<sup>th</sup> position and $0$ everywhere else. Therefore, the reproducing kernel for the point $x \in \X$ is $e_x$ and the reproducing kernel can be thought as the identity matrix.</p>
<p>Can there be multiple reproducing kernels for an RKHS? The following theorem answers this question.</p>
<p><strong>Theorem 3:</strong> If an RKHS $\H$ of functions on a set $\X$ admits a reproducing kernel, $K$, then  $K$ is uniquely determined by $\H$.</p>
<p><strong>Proof:</strong> Suppose that there exists another reproducing kernel $K'$ for $\H$. Then
$$
\norm{k_y - k&rsquo;_y} = \inner{k_y - k&rsquo;_y}{k_y - k&rsquo;_y} = \inner{k_y - k&rsquo;_y}{k_y} - \inner{k_y - k&rsquo;_y}{k&rsquo;_y} = (k_y - k&rsquo;_y)(y) - (k_y - k&rsquo;_y)(y) = 0
$$
for any $y \in \X$. In other words, $k_y(x) = k&rsquo;_y(x)$ for every $x \in \X$ by the positive definite property of norms and hence the kernel is unique. $\square$</p>
<h1 id="epilogue">Epilogue</h1>
<p>We covered quite a lot of ground in this blog post but I didn&rsquo;t even define a kernel as we commonly use in machine learning!
In the 
<a href="http://makkar.github.io/post/kernels1/" target="_blank" rel="noopener">next post</a> I will do that and cover its fundamental properties.</p>
<p>Some resources I recommend to go into more depth on what&rsquo;s covered here are:</p>
<ol>
<li>Halmos, P: Finite-Dimensional Vector Spaces.</li>
<li>Rudin, W: Real and Complex Analysis. (Chapter - 4)</li>
<li>Rudin, W: Functional Analysis.</li>
<li>Conway, J: A course in functional analysis.</li>
</ol>
</div>
    </div>

    



















  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-square" src="/author/aditya-makkar/avatar_huafef36e0e38068a1162b930b5509c06d_432110_270x270_fill_q90_lanczos_center.jpg" alt="Aditya Makkar">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://makkar.github.io/">Aditya Makkar</a></h5>
        
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/adityamakkar/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MakkarAditya" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/makkar" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  














  
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script async defer src="https://maps.googleapis.com/maps/api/js?key="></script>
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
      
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = true;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.6f7ce8be710290b8c431bbc97f405d15.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">

    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
