<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Aditya Makkar">

  
  
  
    
  
  <meta name="description" content="Some intuition behind this concept">

  
  <link rel="alternate" hreflang="en-us" href="https://makkar.github.io/post/regularcondprob/">

  


  
  
  
  <meta name="theme-color" content="rgb(0, 136, 204)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light" disabled>
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark">
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata:wght@200;300;400;500;600;700;800;900&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://makkar.github.io/post/regularcondprob/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Aditya Makkar">
  <meta property="og:url" content="https://makkar.github.io/post/regularcondprob/">
  <meta property="og:title" content="A note on conditional probability | Aditya Makkar">
  <meta property="og:description" content="Some intuition behind this concept"><meta property="og:image" content="https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2021-02-05T17:24:39-05:00">
    
    <meta property="article:modified_time" content="2021-02-05T17:24:39-05:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://makkar.github.io/post/regularcondprob/"
  },
  "headline": "A note on conditional probability",
  
  "datePublished": "2021-02-05T17:24:39-05:00",
  "dateModified": "2021-02-05T17:24:39-05:00",
  
  "author": {
    "@type": "Person",
    "name": "Aditya Makkar"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Aditya Makkar",
    "logo": {
      "@type": "ImageObject",
      "url": "https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Some intuition behind this concept"
}
</script>

  

  


  


  





  <title>A note on conditional probability | Aditya Makkar</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#research"><span>Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/hilbert/"><span>Hilbert's Hotel - Blog</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>A note on conditional probability</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 5, 2021
  </span>
  

  

  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Prob}[1]{\PP \left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 \; : \; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 ; | ; #2 \right\}}
\newcommand{\Ex}[1]{\E\left(#1\right)}
\newcommand{\Exc}[2]{\E\left(#1 \mid #2\right)}
\newcommand{\Pc}[2]{\PP\left( \left. #1 \, \right\vert \, #2\right)}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ph}[1]{\varphi^{-1}\left(#1\right)}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathrm{Var}}
\DeclareMathOperator{\Log}{\mathrm{Log}}
\DeclareMathOperator{\O}{\mathcal{O}}
$$</p>
<h1 id="introduction">Introduction</h1>
<p>The concept of conditional probability is central to probability theory and excellent treatment of it can be found in many books. My aim with this blog post is to consolidate in one place some ideas around it which helped me form a better intuition. These ideas will be useful if you have already been exposed to this concept from a textbook and just want one more person&rsquo;s ramblings about it.</p>
<p>I will start by defining conditional expectation and stating some of its properties. It will be a grave injustice to claim my discussion of it is complete since I don&rsquo;t even prove its existence; this section exists solely for establishing notation. I will then spend some time discussing conditional probability, relating it to the traditional notion of</p>
<p>$$
\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
$$</p>
<p>These discussions will naturally lead to the notions of regular conditional probability and regular conditional distribution which I discuss next.</p>
<h1 id="conditional-expectation">Conditional expectation</h1>
<p>Recall the concept of conditional expectation.</p>
<p><span style="color:darkgray; font-size: 16pt">Theorem 1: </span> Let $\probsp$ be a probability space, and $X$ a random variable with $\Ex{|X|} &lt; \infty$. Let $\G$ be a sub-$\sigma$-algebra of $\F$. Then there exists a random variable $Y$ such that</p>
<ol>
<li>$Y$ is $\G$ measurable,</li>
<li>$\Ex{|Y|} &lt; \infty$,</li>
<li>$\int_G Y \dP = \int_G X \dP$ for every $G \in \G$.</li>
</ol>
<p><span style="color:darkgray; font-size: 16pt">Remarks: </span> (1.) It is easy to see from the $\pi-\lambda$ theorem that the last condition can be relaxed such that $\int_G Y \dP = \int_G X \dP$ for every $G$ in some $\pi$-system which contains $\Omega$ and generates $\G$. (2.) If $Y'$ is another random variable with the three properties above then $Y&rsquo; = Y$ a.s.. Therefore, $Y$ in the theorem above is called a <em>version</em> of the conditional expectation. The notation $\Exc{X}{\G}$ is used to denote this unique (up to a.e. equivalence) random variable.</p>
<p>The proof of this standard theorem can be found in any probability text. See [1,2] for example.</p>
<p><span style="color:darkgray; font-size: 16pt">Definition 1: </span> In the setting of Theorem 1, if $Z$ is a random variable, we write $\Exc{X}{Z}$ for $\Exc{X}{\sigma(Z)}$.</p>
<p>The fact that conditional expectation is defined as a random variable might come as surprising, but the correspondence with the traditional usage of conditional expectation as a number becomes clear once you realize that here we are conditioning on a $\sigma$-algebra instead of a single event. For example, consider the life expectancy of a new born baby conditioned on sex. This is a random variable that takes one value for males and another value for females.</p>
<h2 id="properties-of-conditional-expectation">Properties of conditional expectation</h2>
<p>For completeness I state some useful properties of conditional expectation. You can find the proofs in [1,2] for example. Most of them are parallels to well-known properties of (unconditional) expectation. Assume that all the $X$'s satisfy $\Ex{|X|} &lt; \infty$ and let $\G, \cH$ be sub-$\sigma$-algebras of $\F$.</p>
<ol>
<li>[Linearity] $\Exc{a_1 X_1 + a_2 X_2}{\G} = a_1 \Exc{X_1}{\G} + a_2 \Exc{X_2}{\G}$ a.s.</li>
<li>[Positivity] If $X \ge 0$, then $\Exc{X}{\G} \ge 0$ a.s.</li>
<li>[Monotone convergence theorem for conditional expectation] If $\Ex{|Y|} &lt; \infty$ and $Y \le X_n \uparrow X$ a.s., then $\Exc{X_n}{\G} \uparrow \Exc{X}{\G}$ a.s.</li>
<li>[Fatou&rsquo;s lemma for conditional expectation] If $\Ex{|Y|} &lt; \infty$ and $Y \le X_n$ for all $n \ge 1$ a.s., then $\Exc{\limi X_n}{\G} \le \limi \Exc{X_n}{\G}$ a.s.</li>
<li>[Dominated convergence theorem for conditional expectation] If $|X_n| \le |Y|$ for all $n \ge 1$, $\Ex{Y} &lt; \infty$, and $X_n \to X$ a.s., then $\Exc{X_n}{\G} \to \Exc{X}{\G}$ a.s.</li>
<li>[Tower property] If $\cH \subseteq \G$, then $\Exc{\Exc{X}{\G}}{\cH} = \Exc{\Exc{X}{\cH}}{\G} = \Exc{X}{\cH}$ a.s.</li>
<li>[Taking out what&rsquo;s known] If $Y$ is $\G$-measurable and bounded, then
$$\Exc{Y X}{\G} = Y \Exc{X}{\G} \quad \text{a.s.}\tag{1} \label{eq1}$$
If $p &gt; 1$, $1/p + 1/q = 1$, $X \in L^p(\Omega, \F, \PP)$ and $Y \in L^q(\Omega, \G, \PP)$, then \eqref{eq1} again holds. If $X$ is a nonnegative $\F$-measurable random variable, $Y$ is a nonnegative $\G$-measurable random variable, $\Ex{X} &lt; \infty$ and $\Ex{XY} &lt; \infty$, then \eqref{eq1} again holds.</li>
<li>[Role of independence] If $\cH$ is independent of $\sigma(\sigma(X), \G)$, then $\Exc{X}{\sigma(\G, \cH)} = \Exc{X}{\G}$ a.s.</li>
</ol>
<h1 id="conditional-probability">Conditional probability</h1>
<p><span style="color:darkgray; font-size: 16pt">Definition 2: </span> In the setting of Theorem 1, if $A \in \F$, we let $\Pc{A}{\G}$ to mean $\Exc{\ind{A}}{\G}$ and call it the <em>conditional probability of</em> $A$ <em>given</em> $\G.$ Here $\ind{A}$ is the indicator random variable. If $B \in \F$, we let $\Pc{A}{B}$ to mean $\Exc{\ind{A}}{\ind{B}}$.</p>
<p>Just like conditional expectation, conditional probability, as defined above, is a random variable! Unlike conditional expectation this isn&rsquo;t very palpable and deserves more rumination [3]. We have our probability space $\probsp$ and let $A,B \in \F$ be such that $\PP(B) \neq 0$ and $\PP(B^\comp) \neq 0$. Then our traditional notion of conditional probability tells us that the conditional probability of $A$ given $B$ is defined by</p>
<p>$$
\PP_B(A) = \frac{\PP(A \cap B)}{\PP(B)}
$$</p>
<p>Let us investigate how $\PP_B(A)$ depends on $B$. To this end, introduce the discrete measurable space $(\Lambda, 2^\Lambda)$ with $\Lambda = \se{\lambda_1, \lambda_2}$, and a measurable mapping $T \colon \Omega \to \Lambda$ such that</p>
<p>$$
T(\omega) = \begin{cases}
\lambda_1 &amp; \text{ if } \omega \in B   \\<br>
\lambda_2 &amp; \text{ if } \omega \in B^\comp
\end{cases}
$$</p>
<p>Define the two measures $\nu_A$ and $\nu$ on $(\Lambda, 2^\Lambda)$ as follows for any $E \subseteq \Lambda$</p>
<p>$$
\nu_A(E) = \Prob{A \cap T^{-1}(E)}
$$</p>
<p>$$
\nu(E) = \Prob{T^{-1}(E)}
$$</p>
<p>Then it is easy to see that</p>
<p>$$
\PP_B(A) = \frac{\nu_A(\se{\lambda_1})}{\nu(\se{\lambda_1})}
$$</p>
<p>$$
\PP_{B^\comp}(A) = \frac{\nu_A(\se{\lambda_2})}{\nu(\se{\lambda_2})}
$$</p>
<p>In other words conditional probability may be viewed as a measurable function on $\Lambda$. This can easily be generalized to any finite setting as follows. Let $\se{A_1, \ldots, A_n} \subseteq \F$ be a partition of $\Omega$, i.e., $A_i \cap A_j = \emp$ for $i \neq j$ and $\bigcup_i A_i = \Omega$. Introduce the discrete measurable space $(\Lambda, 2^\Lambda)$ with $\Lambda = \se{\lambda_1, \ldots, \lambda_n}$. Define a measurable mapping $T \colon \Omega \to \Lambda$ such that $T(\omega) = \lambda_i$ whenever $\omega \in A_i$. Define the measures $\nu_{A_1}, \ldots, \nu_{A_n}, \nu$ on $(\Lambda, 2^\Lambda)$ as follows for any $E \subseteq \Lambda$</p>
<p>$$
\nu_{A_i}(E) = \Prob{A_i \cap T^{-1}(E)} \quad \text{for all } i = 1, \ldots, n
$$</p>
<p>$$
\nu(E) = \Prob{T^{-1}(E)}
$$</p>
<p>Then once again we have for any $A \in \F$</p>
<p>$$
\PP_{A_i}(A) = \frac{\PP(A \cap A_i)}{\PP(A_i)} = \frac{\nu_{A_i}(\se{\lambda_i})}{\nu(\se{\lambda_i})} \quad \text{for all } i = 1, \ldots, n
$$</p>
<p>These considerations are what motivated the definition of conditional probability in general cases as you see in Definition 2. If $T$ is any measurable mapping from $\probsp$ into a measurable space $(\Lambda, \LL)$, and if we write $\nu_A(E) = \Prob{A \cap T^{-1}(E)}$ where $A \in \F$ and $E \in \LL$, then it is clear that $\nu_E$ and $\PP \circ T^{-1}$ are measures on $\LL$ such that $\nu_A \ll \PP \circ T^{-1}$. Radon-Nikodym theorem now implies that there exists an $\PP \circ T^{-1}$-integrable function $p_A$, unique upto $\PP \circ T^{-1}$-a.e., such that</p>
<p>$$
\Prob{A \cap T^{-1}(E)} = \int_E p_A(\lambda) \; \PP \circ T^{-1}(\dd \lambda) \quad \text{for all } E \in \LL
$$</p>
<p>We anoint $p_A(\lambda)$ as the conditional probability of $A$ given $\lambda$ or the conditional probability of $A$ given that $T(\omega) = \lambda$. Note that here we are conditioning on a measurable mapping $T$ instead of a sub-$\sigma$-algebra, but these two notions are equivalent since we can interpret conditioning on $T$ as conditioning on $\sigma(T)$ and vice-versa. Keep this &ldquo;rumination&rdquo; in mind when we discuss regular conditional distribution later.</p>
<p>Let&rsquo;s look at our definition of conditional probability from the other direction and show that $\Pc{A}{B}$ as defined in Definition 2 conforms to our traditional usage. To start, note that $\sigma(\ind{B}) = \se{\emp, B, B^\comp, \Omega}$, and since $\Pc{A}{B}$ is $\sigma(\ind{B})$ measurable, it must be constant on each of the sets $B, B^\comp$, thereby necessitating</p>
<p>$$
\Pc{A}{B}(\omega) = \begin{cases}
\frac{\PP(A \cap B)}{\PP(B)} &amp; \text{ if } \omega \in B   \\<br>
\frac{\PP(A \cap B^\comp)}{\PP(B^\comp)} &amp; \text{ if } \omega \in B^\comp
\end{cases}
$$
because of property 3. of Theorem 1 by taking $G$ to be $B$ and $B^\comp$. Of course, if any of the sets $B$ or $B^\comp$ is of measure $0$, then you can take the corresponding value for $\Pc{A}{B}(\omega)$ to be anything in $[0,1]$ and it won&rsquo;t matter since the concept of conditional probability is defined up to sets of measure $0$.</p>
<h2 id="properties-of-conditional-probability">Properties of conditional probability</h2>
<p>Positivity property (property 2. above) and monotone convergence property (property 3. above) imply $0 \le \Pc{A}{\G} \le 1$ a.s. for any $A \in \F$, $\Pc{A}{\G} = 0$ a.s. if and only if $\PP(A) = 0$, and $\Pc{A}{\G} = 1$ a.s. if and only if $\PP(A) = 1.$</p>
<p>Let $A_1, A_2, \ldots \in \F$ be a sequence of disjoint sets. By linearity (property 1. above) and monotone convergence theorem for conditional expectation, we see that</p>
<p>$$
\Pc{\bigcup_n A_n}{\G} = \sum_n \Pc{A_n}{\G} \quad \text{a.s.} \tag{2} \label{eq2}
$$</p>
<p>If $A_n \in \F$, $n \ge 1$ and $\limn A_n = A$, then we also have $\limn \Pc{A_n}{\G} = \Pc{A}{G}$ a.s..</p>
<p>It seems very tempting from the foregoing discussion to claim that $\Pc{\cdot}{\G}$ is a probability measure on $\F$ for almost all $\omega \in \Omega$, but except for some nice spaces, which we will discuss below, this isn&rsquo;t true. Let us first try to see this intuitively [2]. Equation \eqref{eq2} holds for all $\omega \in \Omega$ EXCEPT for some null set which may well depend on the particular sequence $\se{A_n}$.  It does NOT stipulate that there exists a fixed null set $N \in \F$ such that</p>
<p>$$
\Pc{\bigcup_n A_n}{\G}(\omega) = \sum_n \Pc{A_n}{\G}(\omega), \quad \omega \in N^\comp
$$</p>
<p>for every disjoint sequence $\se{A_n} \subseteq \F$. Except in trivial cases, there are uncountably many disjoint sequences, and therefore we will need uncountable union of such null sets to be of measure $0$, which of course may not even be defined let alone be of measure $0$. To further drive this point home you can take a look at an explicit example of how this can fail in an exercise in [3] page 210.</p>
<h1 id="regular-conditional-probability">Regular conditional probability</h1>
<p>Motivated by our discussion above, we define regular conditional probability as follows.</p>
<p><span style="color:darkgray; font-size: 16pt">Definition 3: </span> Let $\probsp$ be a probability space and $\G, \cH$ be sub-$\sigma$-algebras of $\F$. A <em>regular conditional probability</em> on $\cH$ given $\G$ is a function $\PP(\cdot, \cdot) \colon \cH \times \Omega \to [0,1]$ such that</p>
<ol>
<li>for a.e. $\omega \in \Omega$, $\PP(\cdot, \omega)$ is a probability measure on $\cH$,</li>
<li>for each $A \in \cH$, $\PP(A, \cdot)$ is a $\G$-measurable function on $\Omega$ coinciding with the conditional probability of $A$ given $\G$, i.e., $\PP(A, \cdot) = \Pc{A}{\G}$ a.s.</li>
</ol>
<p>Let&rsquo;s show that this definition is not outrageous by showing that it agrees with our traditional notion of conditional pdf and conditional expectation. So suppose that $X$ and $Y$ are random variables which have a joint probability density function $f_{X,Y}(x,y).$ This means that we are considering the probability space $(\R^2, \B(\R^2), \PP)$ with $X$ and $Y$ being the coordinate random variables, i.e. $(x,y) \mapsto x$ and $(x,y) \mapsto y$ respectively, and having an absolutely continuous distribution function $F_{X,Y}(x,y)$ such that</p>
<p>$$
F_{X,Y}(x,y) = \int_{-\infty}^y \int_{\infty}^x f_{X,Y}(s,t) \; \dd s \, \dd t
$$</p>
<p>We recall that $f_X(x) = \int_\R f_{X,Y}(x,y) \; \dd y$ and $f_Y(y) = \int_\R f_{X,Y}(x,y) \; \dd x$ act as probability density functions for $X$ and $Y$ respectively, and</p>
<p>$$
f_{X \mid Y}(x \mid y) = \begin{cases}
\frac{f_{X,Y}(x,y)}{f_Y(y)} &amp; \text{ if } f_Y(y) \neq 0   \\<br>
0 &amp; \text{ otherwise}
\end{cases}
$$
defines the elementary conditional pdf $f_{X \mid Y}$ of $X$ given $Y$. By Fubini&rsquo;s theorem $f_X$ and $f_Y$ are Borel functions on $\R$ and so $f_{X \mid Y}$ is a Borel function on $\R^2$. Let $\cH = \B(\R^2) = \sigma(X, Y)$ and $\G = \sigma(Y) = \R \times \B(\R)$. For $A \in \cH$ and $\omega = (x,y) \in \R^2$ we define</p>
<p>$$
\PP(A, \omega) = \int_{\set{s}{(s,y) \in A}} f_{X \mid Y}(s \mid y) \; \dd s
$$</p>
<p>Then for each $\omega \in \R^2$, $\PP(\cdot, \omega)$ is a probability measure on $\cH$, and for each $A \in \cH$, $\PP(A, \cdot)$ is a Borel function in $y$ and hence $\G$-measurable. To verify that $\PP(A, \cdot) = \Pc{A}{\G}$ for any $A \in \cH$ we just need to verify property 3. of Theorem 1. To this end, fix $A \in \cH$  and $G \in \G$, and note that $G$ must be of the form $G = \R \times B$ for $B \in \B(\R)$. Thus</p>
<p>\begin{align*}
\int_G \PP(A, \omega) \, \dP(\omega) &amp;= \int_B \int_R \PP(A, (s,t)) f_{X \mid Y}(s,t) \; \dd s \, \dd t \quad \text{(by absolute continuity and Fubini&rsquo;s theorem)} \\<br>
&amp;= \int_B \int_R \left[ \int_{\set{u}{(u,t) \in A}} f_{X \mid Y}(u \mid t) \; \dd u \right] f_{X \mid Y}(s,t) \; \dd s \, \dd t \\<br>
&amp;= \int_B \left[ \int_{\set{u}{(u,t) \in A}} f_{X \mid Y}(u \mid t) \; \dd u \right] f_{Y}(t) \; \dd t \\<br>
&amp;= \int_B \int_{\set{u}{(u,t) \in A}} f_{X, Y}(u, t) \; \dd u \, \dd t \\<br>
&amp;= \int_B \int_{\R} \ind{A}(u,t) f_{X, Y}(u, t) \; \dd u \, \dd t \\<br>
&amp;= \int_{G} \ind{A}(\omega) \; \dP(\omega)
\end{align*}</p>
<p>and so $\PP(A, \cdot) = \Exc{\ind{A}}{\G} = \Pc{A}{\G}$. Hence, $\PP(A, \omega)$ is a regular probability measure on $\cH$ given $\G$.</p>
<p>For the corresponding analysis for conditional expectation, let $h$ be a Borel function on $\R^2$ such that $\Ex{|h(X,Y)|} = \int_\R \int_\R |h(x,y)| f_{X,Y}(x,y) \; \dd x \, \dd y &lt; \infty$. Set</p>
<p>$$
g(y) = \int_\R h(s, y) f_{X \mid Y}(s \mid y) \; \dd s
$$</p>
<p>$g(y)$ is the traditional conditional density of $h(X, Y)$ given $Y = y$.
Then the claim is that $g(Y) = \Exc{h(X,Y)}{\sigma(Y)}$ a.s.. The typical element of $\sigma(Y)$ has the form $\set{\omega \in \R^2}{Y(\omega) \in B}$, where $B \in \B(\R).$ Hence, we must show that</p>
<p>$$
L = \Ex{h(X,Y) \ind{B}(Y)} = \Ex{g(Y) \ind{B}(Y)} = R
$$</p>
<p>But we can write $L$ and $R$ as</p>
<p>\begin{align*}
L &amp;= \int \int h(x,y) \ind{B}(y) f_{X,Y}(x,y) \; \dd x \, \dd y \\<br>
R &amp;= \int g(y) \ind{B}(y) f_Y(y) \; \dd y
\end{align*}</p>
<p>and they are equal by Fubini&rsquo;s theorem.</p>
<p>In general, we have the following useful theorem (taken from [2]) which allows us to view conditional expectations as ordinary expectations relative to the measure induced by regular conditional probability.</p>
<p><span style="color:darkgray; font-size: 16pt">Theorem 2: </span> Consider the setting of Definition 3 and denote $\PP_\omega(\cdot) = \PP(\cdot, \omega)$. Let $X$ be an $\cH$-measurable function with $\Ex{X} &lt; \infty$. Then</p>
<p>$$
\Exc{X}{\G}(\omega) = \int_\Omega X \; \dP_\omega \quad \text{a.s.} \tag{3} \label{eq3}
$$</p>
<p><span style="color:darkgray; font-size: 16pt">Proof: </span> Recall the monotone class theorem for functions:</p>
<blockquote>
<p><span style="color:silver"> Let $\mathscr{H}$ be a family of nonnegative functions on $\Omega$ which contains all indicators of sets of some class $\cH$ of subsets of $\Omega$. If either (i) $\cH$ is a $\pi$-class and $\mathscr{H}$ is a $\lambda$-system, or (ii) $\cH$ is a $\sigma$-algebra and $\mathscr{H}$ is a monotone system, then $\mathscr{H}$ contains all nonnegative $\sigma(\cH)$-measurable functions.</span></p>
</blockquote>
<p>By separate considerations of $X^+$ and $X^-$, it may be supposed that $X \ge 0$. Let</p>
<p>$$
\mathscr{H} = \set{X}{X \ge 0, X \text{ is } \cH \text{-measurable, and \eqref{eq3} holds for } X}
$$</p>
<p>By the definition of regular conditional probability, $\ind{A} \in \mathscr{H}$ for $A \in \cH$. $\cH$ is a $\sigma$-algebra. Let&rsquo;s show that $\mathscr{H}$ is a monotone system. If $X_1, X_2 \in \mathscr{H}$ and $c_1, c_2 \ge 0$, then $c_1 X_1 + c_2 X_2 \ge 0$, $c_1 X_1 + c_2 X_2$ is $\cH$-measurable and Equation \eqref{eq3} holds because of linearity of expectation and conditional expectation, and thus $c_1 X_1 + c_2 X_2 \in \mathscr{H}$. If $\se{X_n} \subseteq \mathscr{H}$ such that $X_n \uparrow X$, then $X \ge 0$, $X$ is $\cH$-measurable, and Equation \eqref{eq3} holds for $X$ because of monotone convergence theorem for expectation and conditional expectation, and thus $X \in \mathscr{H}$. Therefore, by the monotone class theorem $\mathscr{H}$ contains all nonnegative $\cH$-measurable functions. $\square$</p>
<h1 id="regular-conditional-distribution">Regular conditional distribution</h1>
<p>In some cases even the concept of regular conditional probability in inadequate, and that motivates the concept of regular conditional distributions.</p>
<p><span style="color:darkgray; font-size: 16pt">Definition 4: </span> Let $\probsp$ be a probability space, $\G \subseteq \F$ a $\sigma$-algebra, $(\Lambda, \LL)$ a measurable space, and $T \colon \Omega \to \Lambda$ a measurable mapping. A <em>regular conditional distribution</em> for $T$ given $\G$ is a function $\PP_T \colon \LL \times \Omega \to [0,1]$ such that</p>
<ol>
<li>for a.e. $\omega \in \Omega$, $\PP_T(\cdot, \omega)$ is a probability measure on $\LL$,</li>
<li>for each $A \in \LL$, $\PP_T(A, \cdot)$ is a $\G$-measurable function on $\Omega$ coinciding with the conditional probability of $T^{-1}(A)$ given $\G$, i.e., $\PP_T(A, \cdot) = \Pc{T^{-1}(A)}{\G}$ a.s.</li>
</ol>
<p>It is clear that when $\Lambda = \Omega$, $\LL = \cH \subseteq \F$ and $T$ is the identity map, $\PP_T$ is exactly the regular conditional probability as defined in Definition 3.</p>
<p>Now would be a good time to read the first &ldquo;rumination&rdquo; in the section Conditional Probability and realize that the definition of regular conditional distribution is in fact well motivated.</p>
<p>A corresponding version of Theorem 2 exists, proof of which I&rsquo;ll leave as an easy exercise:</p>
<p><span style="color:darkgray; font-size: 16pt">Theorem 3: </span> In the setting of Definition 4, if $\PP_T^\omega(A) = \PP_T(A, \omega)$ and $h \colon \Lambda \to \R$ is a Borel function with $\Ex{|h(T)|} &lt; \infty$, then</p>
<p>$$
\Exc{h(T)}{\G}(\omega) = \int_\Lambda h(\lambda) \; \PP_T^\omega(\dd \lambda)
$$</p>
<h1 id="existence-of-regular-conditional-distribution">Existence of regular conditional distribution</h1>
<p>Before we discuss their existence, let us define the concept of standard Borel space [6].</p>
<p><span style="color:darkgray; font-size: 16pt">Definition 5: </span> Let $(X, \X)$ and $(Y, \Y)$ be measurable spaces. They are called <em>isomorphic</em> is there exists a bijection $f \colon X \to Y$ such that $f$ and $f^{-1}$ are both measurable. The function $f$ is called an <em>isomorphism</em>.</p>
<p><span style="color:darkgray; font-size: 16pt">Definition 6: </span> A measurable space $(X, \X)$ is called a <em>standard Borel space</em> if it satisfies any of the following equivalent conditions:</p>
<ol>
<li>$(X, \X)$ is isomorphic to some compact metric space with the Borel $\sigma$-algebra;</li>
<li>$(X, \X)$ is isomorphic to some Polish space (i.e., a separable complete metric space) with the Borel $\sigma$-algebra;</li>
<li>$(X, \X)$ is isomorphic to some Borel subset of some Polish space with the Borel $\sigma$-algebra.</li>
</ol>
<p>As you can guess most spaces we deal with are standard Borel spaces. Durrett [5] calls these space <em>nice</em> since we already have too many things named after Borel. I am not sure I agree with his reasoning but I like Durrett&rsquo;s terminology.</p>
<p>The next two theorems show the existence of regular conditional distribution and are taken from [5, Section 4.1.3]. See also [4, Section V.8].</p>
<p><span style="color:darkgray; font-size: 16pt">Theorem 4: </span> Regular conditional distribution exists if $(\Lambda, \LL)$ is nice.</p>
<p>A generalization of the last theorem:</p>
<p><span style="color:darkgray; font-size: 16pt">Theorem 5: </span> Suppose $(\Lambda, \LL)$ is a nice space, $T$ and $S$ are measurable mappings from $\Omega$ to $\Lambda$, and $\G = \sigma(S)$. Then there exists a function $\mu \colon \Lambda \times \LL \to [0,1]$ such that</p>
<ol>
<li>for a.e. $\omega \in \Omega$, $\mu(S(\omega), \cdot)$ is a probability measure on $\LL$,</li>
<li>for each $A \in \LL$, $\mu(S(\cdot), A) = \Pc{T^{-1}(A)}{\G}$ a.s.</li>
</ol>
<p>It is instructive to prove Theorem 3 in the special case when $(\Lambda, \LL) = (\R^n, \B(\R^n))$. The theorem and the proof is taken from [2].</p>
<p><span style="color:darkgray; font-size: 16pt">Theorem 6: </span> In the setting of Definition 4, let $(\Lambda, \LL) = (\R^n, \B(\R^n))$ and $T = (T_1, \ldots, T_n) \colon \Omega \to \R^n$. Then there exists a regular conditional distribution for $T$ given $\G$.</p>
<p><span style="color:darkgray; font-size: 16pt">Proof: </span> Let&rsquo;s recall the definition of an $n$-dimensional distribution function on $\R^n$ (the Russian convention of left-continuous distribution function).</p>
<blockquote>
<p><span style="color:silver"> An $n$-dimensional distribution function on $\R^n$ is a function $F$ satisfying:
$$\lim_{x_j \to - \infty} F(x_1, \ldots, x_n) = 0, \quad 1 \le j \le n \tag{i}$$
$$\lim_{\substack{x_j \to \infty \\ 1 \le j \le n}} F(x_1, \ldots, x_n) = 1 \tag{ii}$$
$$\lim_{y_j \uparrow x_j} F(x_1, \ldots, x_{j-1}, y_j, x_{j+1}, \ldots, x_n) = F(x_1, \ldots, x_j, \ldots, x_n), \quad 1 \le j \le n \tag{iii}$$
\begin{align*}\Delta_n^{a,b} &amp;:= F(b_1, \ldots, b_n) - \sum_{j=1}^n F(b_1, \ldots, b_{j-1}, a_j, b_{j+1}, \ldots, b_n) \\ &amp;+ \sum_{1 \le j &lt; k \le n} F(b_1, \ldots, b_{j-1}, a_j, b_{j+1}, \ldots, b_{k-1}, a_k, b_{k+1}, \ldots, b_n) - \cdots (-1)^n F(a_1, \ldots, a_n) \\ &amp;\ge 0 \tag{iv}\end{align*}
</span></p>
</blockquote>
<p>We will try to construct a distribution function on $\R^n$. To this end, for any rational number $r_1, \ldots, r_n$ and $\omega \in \Omega$, define</p>
<p>$$
F_n^\omega(r_1, \ldots, r_n) = \Pc{\bigcap_{i=1}^n \se{T_i &lt; r_i}}{\G}(\omega) \tag{4} \label{eq4}
$$</p>
<p>It&rsquo;s evident that the properties of conditional probability discussed above imply there is a null set $N \in \G$ such that for $\omega \in N^\comp$ and all rational numbers $r_i, r_i&rsquo;, q_{i,m}$ the following holds</p>
<p>$$
F_n^\omega(r_1, \ldots, r_n) \ge F_n^\omega(r_1&rsquo;, \ldots, r_n&rsquo;) \text{ if } r_i &gt; r_i&rsquo;, \, 1 \le i \le n
$$</p>
<p>$$
F_n^\omega(r_1, \ldots, r_n) = \lim_{\substack{q_{i,m} \uparrow r_i \\ 1 \le i \le n}} F_n^\omega(q_{1, m}, \ldots, q_{n, m})
$$</p>
<p>$$
\lim_{r_i \to -\infty} F_n^\omega(r_1, \ldots, r_n) = 0, \quad 1 \le i \le n
$$</p>
<p>$$
\lim_{\substack{r_i \to \infty \\ 1 \le i \le n}} F_n^\omega(r_1, \ldots, r_n) = 1
$$</p>
<p>$$
\Delta_n^{r, r&rsquo;} F_n^\omega \ge 0 \text{ if } r \le r&rsquo;
$$</p>
<p>where $r \le r'$ means $r_i \le r_i'$ for all $1 \le i \le n$. Having defined $F_n^\omega$ for rational values, define for any real numbers $x_1, \ldots, x_n$ as follows</p>
<p>$$
F_n^\omega(x_1, \ldots, x_n) = \begin{cases}
\lim_{\substack{r_i \uparrow x_i \\ r_i \in \Q \\ 1 \le i \le n}} F_n^\omega(r_1, \ldots, r_n) &amp; \text{ if } \omega \in N^\comp   \\<br>
\Prob{\bigcap_{i=1}^n \se{T_i &lt; r_i}} &amp; \text{ if } \omega \in N
\end{cases} \tag{5} \label{eq5}
$$</p>
<p>Then for each $\omega \in \Omega$, $F_n^\omega(x_1, \ldots, x_n)$ is an $n$-dimensional distribution function and hence determines a Lebesgue-Stieltjes measure $\mu_\omega$ on $\B(\R^n)$ with $\mu_\omega(\R^n) = 1$. For $B \in \B(\R^n)$ and $\omega \in \Omega$ define</p>
<p>$$
\PP_T(B, \omega) = \mu_\omega(B)
$$</p>
<p>If</p>
<p>\begin{align*}
\cH &amp;= \set{B \in \B(\R^n)}{\PP_T(B, \cdot) = \Pc{T^{-1}(B)}{\G} \text{ a.s.} } \\<br>
\D &amp;= \set{B \in \B(\R^n)}{B = [-\infty, r_1) \times \cdots \times [-\infty, r_n), \, r_i \in \Q}
\end{align*}</p>
<p>then a moment&rsquo;s reflection will convince you that that $\cH$ is a $\lambda-$class, $\D$ is a $\pi-$class, and $\cH \subseteq \D$. Hence, by the $\pi-\lambda$ theorem $\cH \subseteq \sigma(\D) = \B(\R^n)$, or in other words, $\PP_T(B, \omega)$ is a regular conditional distribution for $T$ given $\G$. $\square$</p>
<p>In fact, this theorem is easily extended to $(\R^\infty, \B(\R^\infty))$ as follows: For all $n \ge 1$, define $F_n^\omega$ as in Equation \eqref{eq4}. Select the null set $N \in \G$ such that in addition to the conditions it satisfies above we also have the consistency condition</p>
<p>$$
\lim_{r_{n+1} \to \infty} F_n^\omega(r_1, \ldots, r_n, r_{n+1}) = F_n^\omega(r_1, \ldots, r_n), \quad n \ge 1
$$</p>
<p>For reals $x_1, \ldots, x_n$ define just like Equation \eqref{eq5}. Then for each $\omega \in \Omega$, $\se{F_n^\omega, \, n \ge 1}$ is a consistent family of distribution functions, and hence by the Kolmogorov extension theorem there exists a unique measure $\mu_\omega$ on $(\R^\infty, \B(\R^\infty))$ whose finite dimensional distributions are $\se{F_n^\omega, \, n \ge 1}$. Define $\PP_T(B, \omega) = \mu_\omega(B)$ for $B \in \B(\R^\infty)$. If</p>
<p>\begin{align*}
\cH &amp;= \set{B \in \B(\R^\infty)}{\PP_T(B, \cdot) = \Pc{T^{-1}(B)}{\G} \text{ a.s.} } \\<br>
\D &amp;= \bigcup_{n=1}^\infty \set{B \in \B(\R^\infty)}{B = [-\infty, r_1) \times \cdots \times [-\infty, r_n) \times \R \times \R \times \cdots, \, r_i \in \Q}
\end{align*}</p>
<p>then $\cH$ is a $\lambda-$class, $\D$ is a $\pi-$class, and $\cH \subseteq \D$. Hence, by the $\pi-\lambda$ theorem $\cH \subseteq \sigma(\D) = \B(\R^\infty)$.</p>
<h1 id="references">References</h1>
<ol>
<li>Williams, David. <em>Probability with Martingales</em>. Cambridge mathematical textbooks, Cambridge University Press, 1991.</li>
<li>Chow, Yuan Shih and Teicher, Henry. <em>Probability Theory: Independence, Interchangeability, Martingales</em>, 3rd edn. Springer-Verlag New York, 1997.</li>
<li>Halmos, P. R.. <em>Measure Theory</em>. Van Nostrand, Princeton, N. J., 1950; Springer-Verlag, Berlin and New York, 1974.</li>
<li>Parthasarathy, K. R.. <em>Probability Measures on Metric Spaces</em>. AMS Chelsea Publishing, 1967.</li>
<li>Durrett, R.: <em>Probability: Theory and Examples</em>, 5th edn. Cambridge University Press, 2019.</li>
<li>
<a href="https://encyclopediaofmath.org/wiki/Standard_Borel_space" target="_blank" rel="noopener">https://encyclopediaofmath.org/wiki/Standard_Borel_space</a></li>
</ol>

    </div>

    



















  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/aditya-makkar/avatar_huafef36e0e38068a1162b930b5509c06d_432110_270x270_fill_q90_lanczos_center.jpg" alt="Aditya Makkar">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://makkar.github.io/">Aditya Makkar</a></h5>
        
        
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/adityamakkar/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/MakkarAditya" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/makkar" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  














  
  





  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script async defer src="https://maps.googleapis.com/maps/api/js?key="></script>
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
      
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = true;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.6f7ce8be710290b8c431bbc97f405d15.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">

    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
