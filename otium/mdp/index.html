<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
  
  <link rel="stylesheet" href="/css/franklin.css">
  <link rel="stylesheet" href="/css/basic.css">
  <link rel="icon" href="/assets/favicon.png">
   <title>Markov Decision Processes</title>  
</head>
<body>
  <header>
<!--<img src="/assets/photo.png" alt="Brownian motion in 2D" width="50" height="auto">-->
<div class="blog-name"><a href="/">Aditya Makkar</a></div>
<nav>
  <ul>
    <li><a href="/">Home</a></li>
    <!-- <li><a href="/research/">Research</a></li> -->
    <li><a href="/otium/">Otium</a></li>
    <!-- <li><a href="/notes/">Notes</a></li> -->
    <li><a href="/misc/">Misc</a></li>
  </ul>
  <img src="/assets/hamburger.svg" id="menu-icon">
</nav>
</header>


<!-- Content appended here -->
<div class="franklin-content"><div class="main-heading">Markov Decision Processes</div>
<div class="franklin-toc"><ol><li><a href="#introduction">INTRODUCTION</a></li><li><a href="#setting_the_stage">SETTING THE STAGE</a><ol><li><a href="#standard_borel_space">Standard Borel Space</a><ol><li><a href="#polish_space">Polish Space</a></li></ol></li><li><a href="#standard_borel_space__2">Standard Borel Space</a></li><li><a href="#stochastic_kernels">Stochastic Kernels</a><ol><li><a href="#examples">Examples</a></li></ol></li><li><a href="#regular_conditional_distribution">Regular Conditional Distribution</a><ol><li><a href="#regular_conditional_probability">Regular Conditional Probability</a></li></ol></li><li><a href="#regular_conditional_distribution__2">Regular Conditional Distribution</a></li><li><a href="#disintegration">Disintegration</a></li></ol></li><li><a href="#markov_decision_process">MARKOV DECISION PROCESS</a><ol><li><a href="#markov_decision_model">Markov Decision Model</a><ol><li><a href="#state_space">State Space</a></li></ol></li><li><a href="#action_space">Action Space</a></li><li><a href="#admissible_actions">Admissible Actions</a></li><li><a href="#transition_law">Transition Law</a></li><li><a href="#reward_function">Reward Function</a></li><li><a href="#stationary_markov_decision_model">Stationary Markov Decision Model</a></li><li><a href="#a_note_about_admissible_actions">A Note About Admissible Actions</a></li><li><a href="#policy">Policy</a><ol><li><a href="#history">History</a></li></ol></li><li><a href="#classes_of_policies">Classes of Policies</a></li><li><a href="#canonical_construction">Canonical Construction</a></li><li><a href="#markov_state_process">Markov State Process</a></li></ol></li><li><a href="#optimal_policies">OPTIMAL POLICIES</a><ol><li><a href="#performance_criteria">Performance Criteria</a><ol><li><a href="#expected_total_reward">Expected Total Reward</a></li></ol></li><li><a href="#long-run_average_expected_reward">Long-run Average Expected Reward</a></li><li><a href="#deterministic_markov_policy">Deterministic Markov Policy</a></li><li><a href="#dynamic_programming">Dynamic Programming</a></li></ol></li><li><a href="#epilogue">EPILOGUE</a></li><li><a href="#references">REFERENCES</a></li></ol></div>

<hr style="border: 0.1px solid lightgrey">

<h2 id="introduction"><a href="#introduction" class="header-anchor">INTRODUCTION</a></h2>
<p>Markov decision processes &#40;MDPs&#41;, also known as discrete-time controlled Markov processes in the control literature, are a class of discrete-time stochastic control processes with a simple structure that makes them useful in many applications. For example, in reinforcement learning it is assumed that the underlying dynamics can be modeled using MDPs. This blog post isn’t about the applications though, but rather about the more technical existence and uniqueness questions&mdash;questions which are often ignored. We will start with carefully defining the Markov decision process and then show a few intuitive results, the last of them being the famous dynamic programming equation.</p>
<p>In an MDP there is an underlying discrete-time Markov process whose trajectory can be influenced by a decision maker by choosing actions from a pre-specified class of admissible actions. The goal is choose actions in such a way so as to optimize a performance criterion. The state of the system at time \(n+1\) then depends, in a Markovian way, on the state of the system and the action chosen at time \(n.\) Depending on how general the state spaces and action space are, this framework can model a wide variety of interesting problems.</p>
<p>Let us discuss a toy problem—inventory control problem—for motivation. You are the manager of a distribution centre which sells the product to shops and buys the product in bulk from a factory. Suppose the maximum capacity of the distribution centre is \(C\) units, and therefore the state space is \(\mathsf S = [0, C],\) i.e., the state of the distribution centre at time \(n\) is a random variable \(X_n\) taking values in \(\mathsf S.\) The action that you can take is order some quantity of product from the factory. The action space \(\mathsf A\) also equals \([0,C]\). At time \(n\) you can observe the state \(X_n\) of the system, and with a policy in mind you order \(A_n\) units from the factory. Of course, \(A_n\) must take values in \([0, C-X_n]\). The demand from the shops is given by the i.i.d. random variables \(\{\xi_n\}_{n \ge 0}\) with a common distribution \(\mu\) on \(\mathsf S.\) It costs \(h\) dollars per unit of product to hold the product, \(b\) dollars per unit of product to buy the product, and earns \(s\) dollars per unit of product to sell the product. Any unmet demand is simply lost revenue. How should you choose a policy \(\pi = \{A_n\}_{n=0}^N\) to maximize the revenue over, say, \(N\) days?</p>
<p>Let us write the model more precisely. Suppose on day \(0\) you start with \(X_0 = x_0\) units of product, which is a constant. Then we have <div class="nonumber">\[\begin{aligned} X_{n+1} = \max\{0, X_{n}+A_{n}-\xi_{n}\} =: f(X_{n}, A_{n}, \xi_{n}), \quad 0 \le n < N.\end{aligned}\]</div></p>
<p>The revenue on day \(n\) is <div class="nonumber">\[\begin{aligned} R_n = s \min\{\xi_n, X_n\} - b A_n - h X_n, \quad 0 \le n \le N.\end{aligned}\]</div></p>
<p>The total expected revenue is <div class="nonumber">\[\begin{aligned} J_N(\pi, x_0) = \mathbb E \left[ \sum_{n=1}^N R_n \right].\end{aligned}\]</div></p>
<p>Does there exist an admissible policy \(\pi^*\) such that it maximises the total expected revenue? That is, <div class="nonumber">\[\begin{aligned} J_N(\pi^*, x_0) = \sup_{\pi} J_N(\pi, x_0).\end{aligned}\]</div></p>
<p>Is \(\pi^*\) optimal for every starting state \(x_0\)? It will be more convenient to model the transition dynamics of such a model using stochastic kernels, which informally model the transition probabilities of the process. That is, we want to find a stochastic kernel \(\kappa\) from \(\mathsf S \times \mathsf A\) to \(\mathsf S\) which gives the probability of the state at day \(n+1\) being in \(B \subseteq \mathsf S\) given that on day \(n\) the state was \(X_{n}\) and we took action \(A_n\). We can do this as follows <div class="nonumber">\[\begin{aligned} \kappa((X_n,A_n), B) = \mu(\{s \in \mathsf S : f(X_n,A_n,s) \in B\}).\end{aligned}\]</div></p>
<p>We haven’t been very precise about our formulation. To this end, let us start by defining the objects like stochastic kernels and sets which will define our action and state spaces.</p>
<h2 id="setting_the_stage"><a href="#setting_the_stage" class="header-anchor">SETTING THE STAGE</a></h2>
<p>I had initially intended for this section to be a small one, but as I wrote I couldn’t help but go on a ramble since it helped me learn. If you are already comfortable with the notions of standard Borel spaces, stochastic kernels &#40;also known as Markov kernels or transition probability kernels&#41; and regular conditional distributions, or if you don’t want to be fully rigorous and are okay with understanding the core ideas without bothering too much with rigour and notation, feel free to skip directly to the section on <a href="/otium/mdp#markov_decision_process">Markov Decision Process</a>.</p>
<p><strong>Note on notation</strong>: I will be using sans serif typestyle for spaces, for example \(\mathsf{S}, \mathsf{T}, \mathsf{A}\) etc., and script typestyle for \(\sigma-\)algebras, for example, \(\mathscr{S}, \mathscr{T}, \mathscr{A}\) etc. In this section I will be fastidiously careful about writing the \(\sigma-\)algebra, but starting from the next section on <a href="/otium/mdp#markov_decision_process">Markov Decision Processes</a> I often won’t write the \(\sigma-\)algebra explicitly because we will dealing with topological spaces and Borel \(\sigma-\)algebra is to be assumed implicit. But if I am forced to write, then I’ll use the script typestyle of a letter to denote the Borel \(\sigma-\)algebra of the space written in sans serif typestyle of the corresponding letter; for example, \(\mathscr{A}\) for \(\mathsf A.\) We denote the vector space of measurable functions from \((\mathsf X, \mathscr X)\) to \(\mathbb R\) by \(\mathscr X\) — the context will make it clear if we are referring to a function or a set, and what the domain of the function is. We denote the cone &#40;i.e., stable under sums and multiplication by positive constants&#41; of measurable functions from \((\mathsf X, \mathscr X)\) to \([0, +\infty]\) by \(\mathscr X_+.\)</p>
<h3 id="standard_borel_space"><a href="#standard_borel_space" class="header-anchor">Standard Borel Space</a></h3>
<p>I discussed the notion of standard Borel spaces in the <a href="https://makkar.github.io/otium/regularcondprob">blog post on conditional probability</a>, but since they play an important role in our discussion on Markov decision processes, let me elaborate on them a bit more here. Standard Borel spaces are usually studied under the banner of descriptive set theory and are intimately connected with other descriptive set theoretic concepts like analytic spaces, Lusin spaces and Souslin spaces. We will not focus on this side of the standard Borel spaces though, but rather on some important probabilistic results that can be formulated at this level of generality.</p>
<p>Generally speaking, very few meaningful statements can be made about Markov decision processes when we impose no regularity structure on the spaces involved and let them be any measurable spaces. This is where standard Borel spaces come in — powerful results can be concluded with this structure, and yet they are sufficiently general to be satisfying.</p>
<h4 id="polish_space"><a href="#polish_space" class="header-anchor">Polish Space</a></h4>
<p>Let us start by recalling Polish spaces.</p>
<div class="def"><strong>Definition 1 &#40;Polish Space&#41;</strong>: A <em>Polish space</em> is a separable, completely metrizable topological space.</div>
<p><strong>Examples</strong></p>
<ol>
<li><p>Any set with the discrete topology is completely metrizable, and therefore if it is countable it is Polish. So, for example, \(\{0,1\}\), \(\mathbb N\) and \(\Z\), each with the discrete topology, are Polish.</p>
</li>
<li><p>\(\mathbb R^n\) with the usual topology is Polish, for each \(n \in \mathbb N.\)</p>
</li>
<li><p>The topology of any &#40;real or complex&#41; Banach space is completely metrizable, and therefore separable Banach spaces are Polish.</p>
</li>
<li><p>The completion of a separable metric space is Polish.</p>
</li>
<li><p>Any closed subspace of a Polish space is Polish.</p>
</li>
<li><p>The disjoint union of countably many Polish spaces is Polish.</p>
</li>
<li><p>The product of countably many Polish spaces is Polish.</p>
</li>
</ol>
<p>All of these examples follow from elementary properties of metric spaces. So, for example, to show the last example, first show that the product of countably many separable metric spaces is itself separable using the definition of product topology, and then show that the product of countably many complete metric spaces is itself a complete metric space using the idea of the product metric and the definition of Cauchy sequences.</p>
<p>Polish spaces are the probabilist’s favourite spaces to work with since they are simple to deal with and a lot of spaces are Polish, while almost all results that hold in \(\mathbb R\) still hold in a Polish space. For example, if \((\mathsf X,d_{\mathsf X})\) is a compact metric space and \((\mathsf Y, d_{\mathsf Y})\) is Polish, then \(C(\mathsf X,\mathsf Y)\), the set of continuous functions from \(\mathsf X\) to \(\mathsf Y\) with the uniform metric \(d(f,g) = \sup_{x \in \mathsf X} d_{\mathsf Y}(f(x), g(y))\), is Polish.</p>
<h4 id="standard_borel_space__2"><a href="#standard_borel_space__2" class="header-anchor">Standard Borel Space</a></h4>
<p>Recall the notion of isomorphism between algebraic objects or the notion of homeomorphism between topological spaces. Both these notions give a bijective correspondence between two objects that preserves the algebraic or the topological structure. In a similar spirit we have the notion of isomorphism between two measurable spaces.</p>
<div class="def"><strong>Definition 2 &#40;Isomorphism and Borel isomorphism&#41;</strong>: Two measurable spaces \((\mathsf S, \mathscr{S})\) and \((\mathsf T, \mathscr{T})\) are called <em>isomorphic</em> iff there is a bijective function \(f \colon \mathsf S \to \mathsf  T\) such that both \(f\) and its inverse \(f^{-1}\) are measurable. The function \(f\) is called an <em>isomorphism</em>. In the special case where \(\mathsf S\) and \(\mathsf T\) are topological spaces equipped with their Borel \(\sigma-\)algebras and \(f \colon \mathsf S \to \mathsf T\) is an isomorphism, \(f\) is called a <em>Borel isomorphism</em>, and \(\mathsf S\) and \(\mathsf T\) are called <em>Borel isomorphic</em>.</div>
<div class="def"><strong>Definition 3 &#40;Standard Borel space&#41;</strong>: A <em>standard Borel space</em> is a measurable space isomorphic to a Polish space endowed with its Borel \(\sigma-\)algebra.</div>
<p>Let me list some properties about standard Borel spaces without proof–mostly to justify using them as our default spaces.</p>
<div class="thm"><strong>Proposition 1</strong>: The product of countably many standard Borel spaces is a standard Borel space.</div>
<div class="thm"><strong>Proposition 2</strong>: A measurable subset \(\mathsf S\) of a standard Borel space \((\mathsf T, \mathscr T)\), where we endow \(\mathsf S\) with the \(\sigma-\)algebra \(\mathscr S = \{ T \cap \mathsf S \,: \, T \in \mathscr T\}\), is a standard Borel space. For the other direction, if \(\mathsf S\) is a metrizable topological space endowed with its Borel \(\sigma-\)algebra \(\mathscr S\), then \((\mathsf S, \mathscr S)\) is a standard Borel space if and only if \(\mathsf S\) is homeomorphic to a Borel subset of a Polish space.</div>
<p>It is obvious from the definition that two countable metrizable spaces are Borel isomorphic if and only if they have the same cardinality. For the uncountable case, it is not immediately obvious when two spaces are Borel isomorphic. The next proposition is a deep and surprising result.</p>
<div class="thm"><strong>Proposition 3</strong>: All uncountable standard Borel spaces are mutually isomorphic.</div>
<p>Observe that \([0,1]\) endowed with its Borel \(\sigma-\)algebra is a Polish space, and therefore every uncountable standard Borel space is isomorphic to \(([0,1], \mathscr{B}([0,1])),\) where the notation \(\mathscr B(\mathsf T)\) denotes the Borel \(\sigma-\)algebra of a topological space \(\mathsf T.\) This observation will prove to be very helpful in proving existence result for regular conditional distributions, in proving the disintegration theorem etc.</p>
<div class="thm"><strong>Proposition 4</strong>: If a bijective map between standard Borel spaces is measurable, then the inverse map is also measurable.</div>
<p>This, in particular, implies that if both \((\mathsf S, \mathscr S_1)\) and \((\mathsf S, \mathscr S_2)\) are standard Borel spaces, then \(\mathscr S_1 = \mathscr S_2.\) Now recall that the Borel \(\sigma-\)algebra on \([0,1]\) is a strict subset of the Lebesgue \(\sigma-\)algebra on it. This gives us a simple example of a measurable space which is <em>not</em> a standard Borel space.</p>
<p>A very useful result that further justifies the case for using standard Borel spaces is the following: Let \((\mathsf S, \mathscr S)\) be a standard Borel space and let \(\mathcal{P}(\mathsf S)\) denote the collection of all probability measures on \((\mathsf S, \mathscr S).\) Endow \(\mathcal{P}(\mathsf S)\) with the topology of weak convergence, i.e., the topology generated by the evaluation maps \(\mathcal{P}(\mathsf S) \ni\mu \mapsto \mu(S) \in [0,1]\), where \(S\) varies over \(\mathscr S\) &#40;or equivalently, by the maps \(\mu \mapsto \int f \, \mathrm{d}\mu\), where \(f\) varies over \(C_b(\mathsf S)\)&#41;. Then \(\mathcal{P}(\mathsf S)\) equipped with its Borel \(\sigma-\)algebra is a standard Borel space. This fact becomes useful when dealing with partially observable MDPs. Here a standard approach is to transform them into an equivalent “completely observable” MDP &#40;the one in Definition 14 ahead&#41; with a larger state space, a space of probability measures.</p>
<p>A cool fact, which has no bearing on our discussion ahead, is the following.</p>
<div class="thm"><strong>Proposition 5</strong>: If \((\mathsf X, \mathscr X)\) and \((\mathsf Y, \mathscr Y)\) are two standard Borel spaces, then the map \(f \colon \mathsf X \to \mathsf Y\) is measurable if and only if its graph \(\llbracket f \rrbracket := \{(x, f(x)) \,:\, x \in \mathsf X\}\) is a measurable subset of the product space \((\mathsf X \times \mathsf Y, \mathscr X \otimes \mathscr Y).\)</div>
<h3 id="stochastic_kernels"><a href="#stochastic_kernels" class="header-anchor">Stochastic Kernels</a></h3>
<p>Suppose that a particle is moving randomly in a state space given by a measurable space \((\mathsf S, \mathscr{S})\). We would like to be able to know the probability, denoted with say \(\kappa(x, B)\), of the particle finding itself in a set \(B \in \mathscr{S}\) after a unit of time given that it started at state \(x \in \mathsf S.\) Stochastic kernels model this notion of transition probabilities in a natural manner and we will rely heavily on them in our discussion.</p>
<div class="def"><p><strong>Definition 4 &#40;Stochastic kernel&#41;</strong>: Let \((\mathsf S, \mathscr{S})\) and \((\mathsf T, \mathscr{T})\) be two measurable spaces. A map <div class="nonumber">\[
\kappa \colon \mathsf  S \times \mathscr{T} \to [0,1]
\]</div> is called a <em>stochastic kernel</em> or a <em>Markov kernel</em> or a <em>transition probability kernel</em> from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T})\)–or, briefly, from \(\mathsf S\) to \(\mathsf T\)–if</p>
<ol>
<li><p>\(x \mapsto \kappa(x, B)\) is \(\mathscr{S}-\)measurable for each \(B \in \mathscr{T}\), and</p>
</li>
<li><p>\(B \mapsto \kappa(x,B)\) is a probability measure on \((\mathsf T, \mathscr{T})\) for each \(x \in \mathsf S.\)</p>
</li>
</ol></div>
<p>We often denote \(\kappa(x, B)\) with \(\kappa_x(B)\) and the mapping \(x \mapsto \kappa(x, B)\) with \(\kappa(B).\) If \((\mathsf T, \mathscr{T}) = (\mathsf S, \mathscr{S})\), we simply say a kernel on \((\mathsf S, \mathscr{S})\) or, even more simply, on \(\mathsf S\). Instead of saying a stochastic kernel from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T}),\) some books and papers say a stochastic kernel on \((\mathsf T, \mathscr{T})\) given \((\mathsf S, \mathscr{S}).\) We can view the stochastic kernel \(\kappa\) as a measurable function from \(\mathsf S\) to the space \(\mathcal{P}(\mathsf T)\) of probability measures on \((\mathsf T, \mathscr{T})\), where the \(\sigma-\)algebra on \(\mathcal{P}(\mathsf T)\) is generated by the weak topology, as mentioned above. For this reason, stochastic kernel are also called <em>random probability measures</em>. We can also view the stochastic kernel \(\kappa\) as a probabilistic generalization of the concept of function from \(\mathsf S\) to \(\mathsf T\)–instead of associating with each \(x \in \mathsf S\) a unique value from \(\mathsf T\), we associate a probability distribution on \(\mathsf T.\)</p>
<p>The term <em>kernel</em>, without the adjective stochastic, will be used to denote a similar object as a stochastic kernel except that the property 2 in the definition above states \(B \mapsto \kappa(x,B)\) is a measure on \((\mathsf T, \mathscr{T})\) for each \(x \in \mathsf S.\)</p>
<h4 id="examples"><a href="#examples" class="header-anchor">Examples</a></h4>
<ol>
<li><p>Given a measurable space \((\mathsf S, \mathscr{S})\), the function <div class="nonumber">\[\begin{aligned} \mathsf S \times \mathscr{S} \ni (x,B) \mapsto \kappa(x,B) = \begin{cases}
    1 &\text{if } x \in B, \\
    0 &\text{if } x \in \mathsf S \setminus B
    \end{cases}\end{aligned}\]</div>  defines a stochastic kernel on \((\mathsf S, \mathscr{S})\), called the <em>unit kernel</em>. For each \(x \in \mathsf S\), the measure \(B \mapsto \kappa(x,B)\) is the Dirac measure \(\delta_x\) on \((\mathsf S, \mathscr{S})\) at \(x.\)</p>
</li>
<li><p>Given two measurable spaces \((\mathsf S, \mathscr{S})\) and \((\mathsf T, \mathscr{T})\) and a measure \(\nu\) on \((\mathsf T, \mathscr{T})\), the function <div class="nonumber">\[\begin{aligned} \mathsf S \times \mathscr{T} \ni (x,B) \mapsto \kappa(x, B) := \nu(B)\end{aligned}\]</div>  defines a kernel from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T})\) which does not depend on \(x.\) Therefore, every measure can be seen as a special type of a kernel.</p>
</li>
<li><p>Given two measurable spaces \((\mathsf S, \mathscr{S})\) and \((\mathsf  T, \mathscr{T})\) and a measurable function \(f \colon \mathsf{S} \to \mathsf T\), the function <div class="nonumber">\[\begin{aligned} \mathsf S \times \mathscr T \ni (x,B) \mapsto \kappa(x,B) := \mathbf{1}_B(f(x))\end{aligned}\]</div>  defines a stochastic kernel from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T}).\)</p>
</li>
<li><p>Consider the two measurable spaces \((\mathsf S = \{1, 2, \ldots, n\}, \mathfrak{P}(\mathsf S))\) and \((\mathsf T = \{1, 2, \ldots, m\}, \mathfrak{P}(\mathsf T))\) for two positive integers \(n\) and \(m\), with \(\mathfrak{P}(\mathsf X)\) denoting the power set of a set \(\mathsf X.\) Suppose we are given an \(n \times m\) matrix \(\mathbf K\) containing non-negative elements such that the sum along each row is \(1.\) Then the function \(\kappa\) defined by <div class="nonumber">\[\begin{aligned} \mathsf S \times \mathfrak{P}(\mathsf T) \ni (i,B) \mapsto \kappa(i, B) := \sum_{j \in B} \mathbf K_{i,j}\end{aligned}\]</div>  is a stochastic kernel from \(\mathsf S\) to \(\mathsf T.\) The case where \(\mathsf S\) or \(\mathsf T\) are countably infinite is similar.</p>
</li>
</ol>
<h3 id="regular_conditional_distribution"><a href="#regular_conditional_distribution" class="header-anchor">Regular Conditional Distribution</a></h3>
<p>In most discussions of MDPs the notion of conditional distribution and the associated notation are left frustratingly ambiguous. To remedy this let us spend some time discussing regular conditional distribution and disintegration in this subsection and the next. I also discussed the idea of regular conditional distribution in a <a href="https://makkar.github.io/otium/regularcondprob">previous blog post</a>.</p>
<p>Let us start with a well-known but important result.</p>
<div class="thm"><strong>Lemma 1 &#91;Doob-Dynkin factorization lemma&#93;</strong>: Let \((\Omega, \mathscr{F})\) and \((\mathsf T, \mathscr T)\) be arbitrary measurable spaces, and \((\mathsf S, \mathscr S)\) be a standard Borel space endowed with its Borel \(\sigma-\)algebra. Let \(f \colon \Omega \to \mathsf S\) and \(g \colon \Omega \to \mathsf T\) be measurable functions. Then \(f\) is \(\sigma(g)-\)measurable if and only if \(f = h \circ g\) for some measurable function \(h \colon \mathsf T \to \mathsf S.\)</div>

<center>
<figure>
    <img src="/assets/doob_dynkin.png" alt="missing" />
</figure>
</center>

<div class="proof"><p>The sufficiency is very easy to verify. Indeed, if \(f = h \circ g\) for some measurable function \(h \colon \mathsf T \to \mathsf S,\) then for any \(S \in \mathscr S\) we have \(f^{-1}(S) = g^{-1}(h^{-1}(S)) \in \sigma(g).\)</p>
<p>For the necessity, we first prove the lemma for the case \(\mathsf S = [0,1].\) Then using Propositions 2 and 3 we will extend the argument to an arbitrary standard Borel space. This is the typical way to deal with standard Borel spaces.</p>
<p>So suppose that \(\mathsf S = [0,1].\) If \(f = \mathbf{1}_A\) for \(A \in \sigma(g),\) then by the definition of \(\sigma(g)\) there exists a set \(B \in \mathscr{T}\) such that \(A = g^{-1}(B).\) Then we can write \(f = \mathbf{1}_B \circ g,\) where \(\mathbf{1}_B \colon \mathsf T \to \mathsf S\) is of course measurable. Linearity allows us to deduce our desired conclusion for the case when \(f\) is a simple function. For a general \(\sigma(g)-\)measurable \(f,\) we know that we can find a sequence \(\{f_n\}\) of nonnegative simple \(\sigma(g)-\)measurable functions with \(f_n \uparrow f.\) Therefore, for each \(n\) we may choose measurable \(h_n \colon \mathsf T \to \mathsf S\) such that \(f_n = h_n \circ g.\) Then \(h := \sup_n h_n\) is again measurable, and we have <div class="nonumber">\[\begin{aligned} f = \sup_n f_n = \sup_n (h_n \circ g) = \left(\sup_n h_n\right) \circ g = h \circ g.\end{aligned}\]</div></p>
<p>Finally, we extend the proof to the general case of \(\mathsf S\) being a standard Borel space. Propositions 2 and 3 imply that there exists an isomorphism \(\iota \colon \mathsf S \to [0,1].\) The preceding argument applied to the \(\sigma(g)-\)measurable function \(\iota \circ f \colon \Omega \to [0,1]\) implies that there exists a measurable function \(h_\iota \colon \mathsf T \to [0,1]\) such that \(\iota \circ f = h_\iota \circ g.\) But since \(\iota\) is a bijection, we can write \(f = (\iota^{-1} \circ h_\iota) \circ g.\) The function \(h := \iota^{-1} \circ h_\iota \colon \mathsf T \to \mathsf S\) is our desired measurable function.</p></div>
<p>Under the setting of the above lemma, suppose \(X \colon \Omega \to [0, \infty]\) is a random variable and \(Y \colon \Omega \to \mathsf T\) is a measurable function. Then the lemma above implies that the conditional expectation \(\mathbb{E}[X \mid Y]\) can be written as \(h \circ Y\) for some measurable \(h \colon \mathsf T \to [0, \infty].\)</p>
<p><strong>Notation</strong>: We use \(\mathbb{E}[X \mid Y=y]\) to denote \(h(y).\)</p>
<p>Here the non-negativity of \(X\) is inessential, except for ensuring that the conditional expectation exists—we could have assumed \(X\) to be real-valued and integrable.</p>
<p>Before we discuss regular conditional distribution in full generality, let us discuss the simpler concept of regular conditional probability.</p>
<h4 id="regular_conditional_probability"><a href="#regular_conditional_probability" class="header-anchor">Regular Conditional Probability</a></h4>
<p>Let \((\Omega, \mathscr{F}, \mathbb P)\) be a probability space and \(\mathscr G\) be a sub-\(\sigma\)-algebra of \(\mathscr F.\) For \(A \in \mathscr F,\) if we denote with \(\kappa(A)\) a version of the conditional probability \(\mathbb{P}(A \mid \mathscr G),\) and use \(\kappa_\omega(A)\) to denote \(\kappa(A)(\omega),\) then we have</p>
<ol>
<li><p>\(\kappa(\varnothing) = 0\) a.s.;</p>
</li>
<li><p>\(\kappa(\Omega) = 1\) a.s.;</p>
</li>
<li><p>the mapping \(\omega \mapsto \kappa_\omega(A)\) is \(\mathscr G-\)measurable for each \(A \in \mathscr F\); and</p>
</li>
<li><p>by the monotone convergence theorem for conditional expectations, for every disjointed sequence \(\{A_n\}\) in \(\mathscr F,\) <div class="nonumber">\[\begin{aligned} \kappa_\omega\left( \bigcup_n A_n \right) = \sum_n \kappa_\omega(A_n)\end{aligned}\]</div>  for every \(\omega\) outside a null set.</p>
</li>
</ol>
<p>With these properties the function \((\omega, A) \mapsto \kappa_\omega(A)\) looks very much like a stochastic kernel. But the fact that the null set in property 4 depends on the sequence \(\{A_n\}\) and that there could be uncountably many such sequences, means that \(\kappa\) is, in general, not a stochastic kernel. When \(\kappa\) can be made into a stochastic kernel we get a very special object called a <em>regular version of the conditional probability</em>, or simply <em>regular conditional probability</em>.</p>
<div class="def"><strong>Definition 5 &#40;Regular conditional probability&#41;</strong>: Let \((\Omega, \mathscr{F}, \mathbb P)\) be a probability space and \(\mathscr G\) be a sub-\(\sigma\)-algebra of \(\mathscr F.\) For \(A \in \mathscr F\) let \(\kappa(A)\) be a version of the conditional probability \(\mathbb{P}(A \mid \mathscr G).\) Then \(\Omega \times \mathscr F \ni (\omega, A) \mapsto \kappa_\omega(A) \in [0,1]\) is said to be a <em>regular version of the conditional probability \(\mathbb{P}(\cdot \mid \mathscr G)\)</em> if it is a stochastic kernel from \((\Omega , \mathscr G)\) to \((\Omega, \mathscr F).\)</div>
<p>Recall <a href="https://makkar.github.io/otium/regularcondprob">Theorem 2 from a previous blog post</a> which I state here again:</p>
<div class="thm"><strong>Proposition 6</strong>: Under the setting of Definition 5, if \(X\) is a real-valued random variable whose expectation exists, then <div class="nonumber">\[\begin{aligned} \omega \mapsto \int_\Omega X \, \mathrm{d}\kappa_\omega\end{aligned}\]</div> is a version of the conditional expectation \(\mathbb{E}\left[ X \mid \mathscr G \right].\)</div>
<p>When does a regular conditional probability exist? Intuitively, we need to impose conditions on either the sub-\(\sigma\)-algebra \(\mathscr G\) or on the probability space \((\Omega, \mathscr{F}, \mathbb P).\) In the first case, suppose \(\mathscr G\) is generated by a countable partition \(\{A_n\}\) of \(\Omega,\) which is the case when \(\mathscr G\) is generated by a random variable taking values in a countable space. Then define <div class="nonumber">\[\begin{aligned} \kappa_\omega(A) = \sum_n \mathbb{P}_{A_n}(A) \;\mathbf{1}_{A_n}(\omega), \quad \omega \in \Omega, \, A \in \mathscr F,\end{aligned}\]</div> where \(\mathbb{P}_B(A)\) is defined to be any number in \([0,1]\) satisfying \(\mathbb{P}(A \cap B) = \mathbb{P}(B)\, \mathbb{P}_B(A).\) This makes \(\kappa\) a regular conditional probability because \(A \mapsto \mathbb{P}_B(A)\) is a probability measure.</p>
<p>For the case where \(\mathscr{G}\) could be arbitrary, we have the following result.</p>
<div class="thm"><strong>Theorem 1</strong>: If \((\Omega, \mathscr{F})\) is a standard Borel space, then a regular version of the conditional probability \(\mathbb{P}(\cdot \mid \mathscr G)\) as in Definition 5 exists.</div>
<p>The proof of this theorem will be a simple consequence of the existence of regular conditional distributions which we discuss next.</p>
<h4 id="regular_conditional_distribution__2"><a href="#regular_conditional_distribution__2" class="header-anchor">Regular Conditional Distribution</a></h4>
<div class="def"><strong>Definition 6 &#40;Regular conditional distribution&#41;</strong>:  Let \((\Omega, \mathscr{F}, \mathbb P)\) be a probability space, \(\mathscr G\) be a sub-\(\sigma\)-algebra of \(\mathscr F,\) \((\mathsf S, \mathscr S)\) be a measurable space, and \(X \colon \Omega \to \mathsf S\) be a random element. The <em>regular conditional distribution of \(X\) given \(\mathscr G\)</em>, or simply, <em>conditional distribution of \(X\) given \(\mathscr G\)</em> is any stochastic kernel \(\kappa\) from \((\Omega, \mathscr G)\) to \((\mathsf S, \mathscr S)\) such that <a id="r1" class="anchor"></a>\[\begin{aligned} 
\kappa(B) = \mathbb{P}(\{X \in B\} \mid \mathscr G) \text{ a.s.}, \quad B \in \mathscr S.\end{aligned}\]</div>
<div class="thm"><strong>Theorem 2</strong>: Under the setting of Definition 6, if \((\mathsf S, \mathscr S)\) is a standard Borel space, then there exists a version of the regular conditional distribution of \(X\) given \(\mathscr G.\)</div>
<div class="proof"><p>&#40;From <span class="bibref"><a href="#cinlar">(Cinlar, 2011)</a></span> Theorem IV.2.10&#41; As in the proof of Lemma 1, we first give the proof for a special case of \(\mathsf S\), which here we assume \(\mathsf S = \overline{\mathbb R} := [-\infty, +\infty]\) and \(\mathscr S\) being its Borel \(\sigma-\)algebra. For each \(q \in \mathbb{Q},\) define <div class="nonumber">\[\begin{aligned} C_q := \mathbb{P}(\{X \le q\} \mid \mathscr G).\end{aligned}\]</div></p>
<p>We shall construct the regular conditional distribution \(\kappa\) of \(X\) given \(\mathscr G\) from these countably many random variables \(\{C_q\}_{q \in \mathbb Q}.\) For \(q < r,\) we have \(\{X \le q \} \subseteq \{X \le r\},\) and therefore by the monotonicity of conditional expectations the event <div class="nonumber">\[\begin{aligned} \Omega_{qr} = \{C_q \le C_r\}\end{aligned}\]</div> is an almost sure event and is in \(\mathscr G.\) Then the countable intersection <div class="nonumber">\[\begin{aligned} \Omega_0 = \bigcap_{\stackrel{q,r \in \mathbb Q}{q < r}} \Omega_{qr}\end{aligned}\]</div> is also an almost sure event and is in \(\mathscr G.\) Fix an arbitrary \(\omega_0 \in \Omega_0.\) Then the mapping <div class="nonumber">\[\begin{aligned} \mathbb{Q} \ni q \mapsto C_q(\omega_0) \in [0,1]\end{aligned}\]</div> is non-decreasing, and thus, for each \(t \in \mathbb R,\) we can define <div class="nonumber">\[\begin{aligned} C_t(\omega_0) = \lim_{\stackrel{q \in \mathbb Q}{q > t}} C_q(\omega_0).\end{aligned}\]</div></p>
<p>The resulting extension \(\mathbb{R} \ni t \mapsto C_t(\omega_0) \in [0,1]\) is non-decreasing, right-continuous, and satisfies \(\lim_{t \to -\infty} C_t(\omega_0) = 0\) and \(\lim_{t \to \infty} C_t(\omega_0) = 1,\) and therefore is a probability distribution function. Denote with \(\overline{\kappa}_{\omega_0}\) the probability measure on \(\mathscr S\) associated with this probability distribution function. Now define <div class="nonumber">\[\begin{aligned} \kappa_\omega(B) := \mathbf{1}_{\Omega_0}(\omega) \;\overline{\kappa}_\omega(B) + \mathbf{1}_{\Omega \setminus \Omega_0}(\omega) \; \delta_0(B), \quad \omega \in \Omega, B \in \mathscr S.\end{aligned}\]</div></p>
<p>We claim that \(\kappa\) is a regular conditional distribution of \(X\) given \(\mathscr G.\) Let us verify the required properties one by one—the first two being the two properties in Definition 4 and the last being <span class="eqref">(<a href="#r1">1</a>)</span>.</p>
<ol>
<li><p>We will use Dynkin’s theorem &#40;<a href="https://almostsuremath.com/2019/10/06/the-monotone-class-theorem">Theorem 4 here</a>&#41;, which is a type of monotone class argument. Consider the collection <div class="nonumber">\[\begin{aligned} \mathscr D = \left\{B \in \mathscr S \; : \; \omega \mapsto \kappa_\omega(B) \text{ is  } \mathscr G\text{-measurable}\right\}.\end{aligned}\]</div></p>
<p>\(\kappa_\omega(\mathsf S) = 1\) for all \(\omega \in \Omega\) and therefore \(\omega \mapsto \kappa_\omega(\mathsf S)\) is \(\mathscr G-\)measurable, showing \(\mathsf S \in \mathscr D.\) For \(A \subseteq B\) with \(A, B \in \mathscr S,\) we have \(\kappa_\omega(B \setminus A) = \kappa_\omega(B) - \kappa_\omega(A),\) and therefore if \(A\) and \(B\) are in \(\mathscr D\) with \(A \subseteq B\) then so does \(B \setminus A.\) Finally, if \(B_1 \subseteq B_2 \subseteq \cdots\) is an increasing sequence in \(\mathscr D,\) then using the continuity of measure from below and the fact that the pointwise limit of measurable functions is again measurable, we get that \(\bigcup_n B_n \in \mathscr D.\) We conclude that \(\mathscr D\) is a d-system.</p>
<p>By the Dynkin’s theorem, to show that \(\mathscr D = \mathscr S,\) it is enough to show that \(\mathscr D\) contains the \(\pi\)-system \(\left\{ \lbrack - \infty, t \rbrack \,:\,  t \in \overline{\mathbb R}\right\}.\) For \(t = +\infty\) we have already shown that \([-\infty, t] \in \mathscr D.\) Similarly it is easy to see that for \(t = -\infty\) we have \([-\infty, t] = \varnothing \in \mathscr D.\) Fix a \(t \in \mathbb R\) and note that we can write <div class="nonumber">\[\begin{aligned}
    \kappa_\omega([-\infty, t]) &= \mathbf{1}_{\Omega_0}(\omega) \; C_t(\omega) + \mathbf{1}_{\Omega \setminus \Omega_0}(\omega) \; \delta_0([-\infty, t]) = \lim_{\stackrel{q \in \mathbb Q}{q > t}} \mathbf{1}_{\Omega_0}(\omega) \; C_q(\omega) + \mathbf{1}_{\Omega \setminus \Omega_0}(\omega) \; \delta_0([-\infty, t]).
    \end{aligned}\]</div></p>
<p>Since each of \(\left\{ C_q,\, q \in \mathbb Q\right\}, \mathbf{1}_{\Omega_0}\) and \(\mathbf{1}_{\Omega \setminus \Omega_0}\) is \(\mathscr G-\)measurable, and \(\delta_0([-\infty, t])\) is a constant, the map \(\omega \mapsto \kappa_\omega([-\infty, t])\) is \(\mathscr G-\)measurable.</p>
<p>We conclude that \(\omega \mapsto \kappa_\omega(B)\) is \(\mathscr G-\)measurable for each \(B \in \mathscr S.\)</p>
</li>
<li><p>For each \(\omega \in \Omega,\) \(B \mapsto \kappa_\omega(B)\) is clearly a probability measure on \(\mathscr S.\)</p>
</li>
<li><p>To verify <span class="eqref">(<a href="#r1">1</a>)</span>, we need to show that <a id="eq1" class="anchor"></a>\[\begin{aligned} 
    \int_G \kappa(B) \, \mathrm{d}\mathbb{P} = \mathbb{P}(G \cap \{X \in B\}), \quad G \in \mathscr{G}, B \in \mathscr S.\end{aligned}\]  By the same Dynkin’s theorem-type argument as in 1. above, we need to show <span class="eqref">(<a href="#eq1">2</a>)</span> for \(B = [-\infty, t]\) with \(t \in \mathbb R.\) By the definition of \(C_q\) we get <div class="nonumber">\[\begin{aligned} \mathbb{P}(G \cap \{X \in B\}) = \mathbb{P}(G \cap \{X \le t\}) = \lim_{\stackrel{q \in \mathbb Q}{q > t}} \mathbb{P}(G \cap \{X \le q\}) = \lim_{\stackrel{q \in \mathbb Q}{q > t}} \int_G C_q \, \mathrm{d}\mathbb{P}.\end{aligned}\]</div>  Now note that for \(\omega \in \Omega_0,\) \(C_q(\omega) \to C_t(\omega) = \kappa_\omega(B)\) as \(q \downarrow t.\) Therefore using monotone convergence theorem and the fact that \(\Omega_0\) is of full measure, we can write <div class="nonumber">\[\begin{aligned} \lim_{\stackrel{q \in \mathbb Q}{q > t}} \int_G C_q \, \mathrm{d}\mathbb{P} = \lim_{\stackrel{q \in \mathbb Q}{q > t}} \int_{G \cap \Omega_0} C_q \, \mathrm{d}\mathbb{P} = \int_G \kappa(B) \, \mathrm{d}\mathbb{P}.\end{aligned}\]</div>  This implies <span class="eqref">(<a href="#eq1">2</a>)</span>.</p>
</li>
</ol>
<p>This completes the proof for the special case when \(\mathsf S = \overline{\mathbb R}.\) To extend the proof to arbitrary standard Borel spaces, suppose \(\iota \colon \mathsf S \to [0,1]\) is an isomorphism. The preceding argument applied to the real-valued random variable \(\iota \circ X\) shows the existence of the regular conditional distribution \(\kappa^\iota\) of \(\iota \circ X\) given \(\mathscr G.\) Now define <div class="nonumber">\[\begin{aligned} \kappa_\omega(B) = \kappa^\iota_\omega(\iota(B)), \quad \omega \in \Omega,\, B \in \mathscr S.\end{aligned}\]</div> Here \(\iota(B)\) is the image of \(B\) under \(\iota.\) It is easy to check that \(\kappa\) is a stochastic kernel from \((\Omega, \mathscr G)\) to \((\mathsf S, \mathscr S).\) To verify property <span class="eqref">(<a href="#r1">1</a>)</span> observe that, for \(G \in \mathscr G\) and \(B \in \mathscr S,\) <div class="nonumber">\[\begin{aligned} \mathbb{P}(G \cap \{X \in B\}) = \mathbb{P}(G \cap \{\iota \circ X \in \iota(B)\}) = \int_G \kappa^\iota(\iota(B)) \, \mathrm{d}\mathbb{P} = \int_G \kappa(B) \, \mathrm{d}\mathbb{P}.\end{aligned}\]</div></p></div>
<p>Regular conditional probabilities and regular conditional distributions are closely related. If \(\kappa\) is a regular version of the conditional probability \(\mathbb{P}(\cdot \mid \mathscr G),\) then \(\kappa^X\) defined as <div class="nonumber">\[\begin{aligned} \kappa_\omega^X(B) := \kappa_\omega(\{X \in B\})\end{aligned}\]</div> is easily seen to be a regular conditional distribution of \(X\) given \(\mathscr G.\) On the other hand, Theorem 1 follows from Theorem 2. Suppose that \((\Omega, \mathscr F)\) is a standard Borel space and let \((\mathsf S, \mathscr S) = (\Omega, \mathscr F).\) Define \(X(\omega) = \omega\) for all \(\omega \in \Omega.\) Then Theorem 2 gives a regular conditional distribution \(\kappa\) of \(X\) given \(\mathscr G\) which is precisely a regular version of the conditional probability \(\mathbb{P}(\cdot \mid \mathscr G).\)</p>
<p>Suppose in the setting of Definition 6, we also have a measurable space \((\mathsf T, \mathscr T)\) and a random element \(Y \colon \Omega \to \mathsf T.\) Then by the <em>conditional distribution of \(X\) given \(Y\)</em> we will mean the conditional distribution of \(X\) given \(\sigma(Y).\)</p>
<h3 id="disintegration"><a href="#disintegration" class="header-anchor">Disintegration</a></h3>
<p>Let us first recall the <strong>construction of measures on a product space</strong>, which is closely tied with disintegrations. Suppose \((\mathsf S, \mathscr{S})\) and \((\mathsf T, \mathscr{T})\) are measurable spaces, \(\mu\) is a measure on \((\mathsf S, \mathscr{S})\), and \(\kappa\) is a kernel from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T})\) that can be written as \(\kappa = \sum_{n=1}^\infty \kappa_n\) for kernels \(\kappa_n\) from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T})\) that satisfy \(\kappa_n(x, \mathsf{T}) < \infty\) for each \(x \in \mathsf{S}\) &#40;such kernels \(\kappa\) are called \(\Sigma-\)finite&#41;. If \(f \colon \mathsf{S} \times \mathsf{T} \to [-\infty, +\infty]\) is measurable, recall that \(x \mapsto f(x,y)\) is measurable for each \(y \in \mathsf T\) and \(y \mapsto f(x,y)\) is measurable for each \(x \in \mathsf S.\) If \(f \in (\mathscr S \otimes \mathscr T)_+\), then <div class="nonumber">\[\begin{aligned} \mathsf S \ni x \mapsto \kappa f(x) := \int_{\mathsf T} f(x,y) \, \kappa(x, \mathrm{d}y) \in [0, +\infty]\end{aligned}\]</div> defines a non-negative measurable function. The iterated integral <a id="d1" class="anchor"></a>\[\begin{aligned} 
\gamma f \equiv \int_{\mathsf S \times \mathsf T} f \, \mathrm{d}\gamma := \int_{\mathsf S} \left( \int_{\mathsf T} f(x,y) \, \kappa(x, \mathrm{d}y)\right)\,\mu(\mathrm{d}x)\end{aligned}\] for \(f \in (\mathscr S \otimes \mathscr T)_+\) defines a measure \(\gamma\) on the product space \((\mathsf{S} \times \mathsf T, \mathscr S \otimes \mathscr T).\) Moreover, if \(\mu\) is \(\sigma-\)finite and \(\kappa\) is \(\sigma-\)bounded &#40;i.e., there exists a measurable partition \(\{\mathsf T_n\}\) of \(\mathsf T\) such that \(x \mapsto \kappa(x, \mathsf T_n)\) is bounded for each \(n\)&#41;, then \(\gamma\) is \(\sigma-\)finite and is the unique measure on the product space satisfying <div class="nonumber">\[\begin{aligned} \gamma(A \times B) = \int_A\kappa(x,B) \, \mu(\mathrm{d} x), \quad A \in \mathscr{S}, B \in \mathscr{T}.\end{aligned}\]</div> We denote \(\gamma\) with \(\mu \otimes \kappa.\) For the special case when \(\kappa(x,B) = \nu(B)\) for some measure \(\nu\) on \(\mathscr T\), like in the <a href="/otium/mdp#examples">example above</a>, we denote \(\gamma\) with \(\mu \otimes \nu.\) In this special case we can interchange the order of integrals, i.e., <div class="nonumber">\[\begin{aligned} \int_{\mathsf S} \left( \int_{\mathsf T} f(x,y) \, \nu( \mathrm{d}y)\right)\,\mu(\mathrm{d}x) = \int_{\mathsf T} \left( \int_{\mathsf S} f(x,y) \, \mu( \mathrm{d}x)\right)\,\nu(\mathrm{d}y),\end{aligned}\]</div> for \(f \in (\mathscr S \otimes \mathscr T)_+\) &#40;this is known as the <em>Tonelli’s theorem</em>&#41;. Also if \(f \colon \mathsf{S} \times \mathsf{T} \to [-\infty, +\infty]\) is \(\mu \otimes \nu-\)integrable, then \(x \mapsto f(x,y)\) is \(\mu-\)integrable for \(\nu-\)a.e. \(y\), and \(y \mapsto f(x,y)\) is \(\nu-\)integrable for \(\mu-\)a.e. \(x\), and the interchange above holds again &#40;this is known as the <em>Fubini’s theorem</em>&#41;.</p>
<p>Disintegration asks the converse to the construction of measures on product spaces: Given a measure \(\gamma\) on the product space \((\mathsf{S} \times \mathsf T, \mathscr S \otimes \mathscr T)\) does there exist a measure \(\mu\) on \((\mathsf S, \mathscr{S})\) and a kernel \(\kappa\) from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T})\) such that <span class="eqref">(<a href="#d1">3</a>)</span> holds? We will answer this in the affirmative for the special case for probability measures and stochastic kernels. For a more thorough discussion see the paper <span class="bibref"><a href="#chang_pollard">(Chang and Pollard, 1997)</a></span>.</p>
<div class="thm"><strong>Theorem 3 &#40;Disintegration&#41;</strong>: Suppose \((\mathsf S, \mathscr{S})\) and \((\mathsf T, \mathscr{T})\) are measurable spaces with \((\mathsf T, \mathscr T)\) being a standard Borel space endowed with its Borel \(\sigma-\)algebra, and \(\gamma\) is a probability measure on the product space \((\mathsf{S} \times \mathsf T, \mathscr S \otimes \mathscr T).\) Then there exists a probability measure \(\mu\) on \((\mathsf S, \mathscr{S})\) and a stochastic kernel \(\kappa\) from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T})\) such that <span class="eqref">(<a href="#d1">3</a>)</span> holds for all \(f \in (\mathscr S \otimes \mathscr T)_+.\)</div>
<div class="proof"><p>&#40;From <span class="bibref"><a href="#cinlar">(Cinlar, 2011)</a></span> Theorem IV.2.18&#41;</p>
<p>We will use Theorem 2 by converting the theorem above to its special case. Define the probability space \((\Omega, \mathscr F, \mathbb P) = (\mathsf{S} \times \mathsf T, \mathscr S \otimes \mathscr T, \gamma).\) Define the random elements \(Y \colon \Omega \to \mathsf S\) and \(X \colon \Omega \to \mathsf T\) as the projections \((s,t) \mapsto s\) and \((s,t) \mapsto t\) respectively. Let \(\mu\) be the distribution of \(Y\), i.e., \(\mu(A) = \gamma(A \times \mathsf T)\) for every \(A \in \mathscr S.\) Applying Theorem 2 to \(\mathscr G = \sigma(Y)\) shows that there exists a version of the regular conditional distribution of \(X\) given \(\mathscr G,\) which we will denote by \(\overline\kappa.\) Recall that \(\overline\kappa\) is a stochastic kernel from \((\Omega, \mathscr G)\) to \((\mathsf T, \mathscr T)\) such that</p>
<a id="eq2" class="anchor"></a>\[\begin{aligned} 
    \mathbb{E}\left[ g\,\overline\kappa(B)\right] = \mathbb{E}\left[\mathbf{1}_{\{X \in B\}} g\right], \quad B \in \mathscr T, \, g \in \mathscr G_+.\end{aligned}\]
<p>By the structure of \(Y\), we see that \(\mathscr G\) consists of measurable rectangles of the form \(A \times \mathsf T\) for \(A \in \mathscr S.\) Therefore, the Doob-Dynkin factorization lemma implies that a function \(g \colon \Omega \to [0, \infty]\) is \(\mathscr G-\)measurable if and only if \(g((s,t)) = \overline{g}(s)\) for some measurable \(\overline{g} \colon \mathsf S \to [0, \infty].\) Now using equation <span class="eqref">(<a href="#eq2">4</a>)</span>, we conclude that \(\overline \kappa((s,t), B) = \kappa(s, B)\) for some stochastic kernel \(\kappa\) from \((\mathsf S, \mathscr S)\) to \((\mathsf T, \mathscr T).\)</p>
<p>If \(A \in \mathscr S\) and \(B \in \mathscr T\), equation <span class="eqref">(<a href="#eq2">4</a>)</span> allows to write <div class="nonumber">\[\begin{aligned} \gamma(A \times B) = \mathbb{E}\left[\mathbf{1}_{\{Y \in A\}} \mathbf{1}_{\{X \in B\}}\right] = \mathbb{E}\left[\mathbf{1}_{\{Y \in A\}}\kappa(B)\right] = \int_{\mathsf S}  \mathbf{1}_A(s) \kappa(s,B) \,\mu(\mathrm{d}s)\end{aligned}\]</div> which is equation <span class="eqref">(<a href="#d1">3</a>)</span> for \(f = \mathbf{1}_{A \times B}.\) Finally, a monotone class argument proves <span class="eqref">(<a href="#d1">3</a>)</span> for all \(f \in (\mathscr S \otimes \mathscr T)_+.\)</p></div>
<p>Now a natural question is if \(\mu\) is a probability measure, corresponding to the law of a random variable \(Y \colon \Omega \to \mathsf S,\) and \(\kappa\) is a stochastic kernel, such that the product measure \(\mu \otimes \kappa\) corresponds to the law of a random vector \((Y, X) \colon \Omega \to \mathsf{S} \times \mathsf T,\) with \(X \colon \Omega \to \mathsf T\) being some random variable, can we associate \(\kappa\) with the conditional distribution for \(X\) given \(Y?\) The next theorem answers this question.</p>
<div class="thm"><strong>Theorem 4</strong>: Suppose \((\Omega, \mathscr F, \mathbb P)\) is a probability space, \((\mathsf S, \mathscr{S})\) and \((\mathsf T, \mathscr{T})\) are measurable spaces, and \(Y \colon \Omega \to \mathsf S\) and \(X \colon \Omega \to \mathsf T\) are random elements such that the law of \(Y\) is \(\mu\) and the law of \((Y, X)\) is \(\mu \otimes \kappa\) for a stochastic kernel \(\kappa\) from \((\mathsf S, \mathscr{S})\) to \((\mathsf T, \mathscr{T}).\) Denote \(\mathscr G = \sigma(Y).\) Then the stochastic kernel \(\eta\) from \((\Omega, \mathscr{G})\) to \((\mathsf T, \mathscr T)\) defined by <div class="nonumber">\[\begin{aligned} \eta(\omega, B) := \kappa(Y(\omega), B), \quad \omega \in \Omega, B \in \mathscr T,\end{aligned}\]</div> is a version of the conditional distribution of \(X\) given \(Y.\) Moreover, for every measurable \(f \colon \mathsf S \times \mathsf T \to [0, \infty],\) <a id="d2" class="anchor"></a>\[\begin{aligned} 
\mathbb{E}\left[f(Y,X) \mid \mathscr G\right] = \int_{\mathsf T}f(Y, t) \, \kappa(Y, \mathrm{d}t).\end{aligned}\]</div>
<div class="proof"><p>The proof for the statement about \(\eta\) follows the same line of reasoning as the proof of Theorem 3.</p>
<p>To see <span class="eqref">(<a href="#d2">5</a>)</span>, note that for \(h \in \mathscr T_+\) and \(g \in \mathscr S,\) property <span class="eqref">(<a href="#d1">3</a>)</span> for \(\mu \otimes \kappa\) implies that <div class="nonumber">\[\begin{aligned}
    \mathbb{E}[g(Y)h(X)] = \int_{\mathsf S \times \mathsf T} g(s) h(t) \, \mathrm{d}(\mu \otimes \kappa)(s,t)
    = \int_{\mathsf S} \left( \int_{\mathsf T} g(s)h(t) \, \kappa(s, \mathrm{d}t)\right)\,\mu(\mathrm{d}s)
    = \mathbb{E}\left[g(Y) \int_{\mathsf T} h(t) \, \kappa(Y, \mathrm{d}t)\right].
    \end{aligned}\]</div></p>
<p>Since every function in \(\mathscr G_+\) is of the form \(g(Y),\) we get <div class="nonumber">\[\begin{aligned} \mathbb{E}[h(X) \mid \mathscr G] = \int_{\mathsf T} h(t) \, \kappa(Y, \mathrm{d}t).\end{aligned}\]</div></p>
<p>Now if \(f \in (\mathscr S \otimes \mathscr T)_+\) is such that it is the product \(f = g \cdot h,\) then since \(g(Y)\) is \(\mathscr G-\)measurable, we get <div class="nonumber">\[\begin{aligned} \mathbb{E}\left[f(Y,X) \mid \mathscr G\right] = g(Y) \mathbb{E}[h(X) \mid \mathscr G] = g(Y)\int_{\mathsf T} h(t) \, \kappa(Y, \mathrm{d}t) = \int_{\mathsf T}f(Y, t) \, \kappa(Y, \mathrm{d}t).\end{aligned}\]</div></p>
<p>Finally, a monotone class argument finishes the proof.</p></div>
<p>Although we won’t be needing it, let me briefly mention the notion of generalized disintegration.</p>
<div class="def"><p><strong>Definition 7 &#40;General disintegration&#41;</strong>: Let \((\mathsf E, \mathscr E)\) and \((\mathsf S, \mathscr{S})\) be measurable spaces, \(\psi \colon \mathsf E \to \mathsf S\) be a measurable mapping, \(\gamma\) be a measure on \((\mathsf E, \mathscr E),\) and \(\mu\) be a measure on \((\mathsf S, \mathscr S).\) We call a kernel \(\kappa\) from \((\mathsf S, \mathscr{S})\) to \((\mathsf E, \mathscr{E})\) a <em>\((\psi, \mu)-\)disintegration of \(\gamma\)</em> if</p>
<ol>
<li><p>\(\kappa_x\{\psi \neq x\} = 0\) for \(\mu-\)a.e. \(x\), and</p>
</li>
<li><p>we have the following iterated integral for each \(f \in \mathscr E_+,\)</p>
</li>
</ol>
<a id="d3" class="anchor"></a>\[\begin{aligned} 
\int_{\mathsf E} f \, \mathrm{d}\gamma = \int_{\mathsf S} \left( \int_{\mathsf E} f(y) \,\kappa_x(\mathrm{d}y) \right) \mu(\mathrm{d}x).\end{aligned}\]</div>
<p>Note that, because of property 1, we can write <span class="eqref">(<a href="#d3">6</a>)</span> as <div class="nonumber">\[\begin{aligned} \int_{\mathsf E} f \, \mathrm{d}\gamma = \int_{\mathsf S} \left( \int_{\{\psi=x\}} f(y) \,\kappa_x(\mathrm{d}y) \right) \mu(\mathrm{d}x).\end{aligned}\]</div></p>
<p>The disintegration discussed in Theorem 3 is a special case of the disintegration discussed in Definition 7. To see this, let \((\mathsf E, \mathscr E)\) be the product space \((\mathsf{S} \times \mathsf T, \mathscr S \otimes \mathscr T)\) and \(\psi \colon \mathsf S \times \mathsf T \to \mathsf S\) be the canonical projection. \(\{\psi = x\}\) is then simply \(\mathsf T\) and equation <span class="eqref">(<a href="#d3">6</a>)</span> becomes equation <span class="eqref">(<a href="#d1">3</a>)</span>.</p>
<p>The following existence theorem is taken from Pollard’s book <span class="bibref"><a href="#pollard">(Pollard, 2010)</a></span>.</p>
<div class="thm"><p><strong>Theorem 5</strong>: Under the setting of Definition 7, assume that</p>
<ol>
<li><p>\((\mathsf E, \mathscr E)\) is a metric space endowed with its Borel \(\sigma-\)algebra,</p>
</li>
<li><p>\(\gamma\) and \(\mu\) are \(\sigma-\)finite with \(\gamma\) being a Radon measure &#40;i.e., \(\gamma(K) < \infty\) for each compact \(K\) and \(\gamma(B) = \sup_{K \subseteq B} \gamma(K),\) the supremum being taken over compact sets, for each \(B \in\mathscr E\)&#41;,</p>
</li>
<li><p>the image measure \(\gamma \circ \psi^{-1}\) of \(\gamma\) under \(\psi\) is absolutely continuous with respect to \(\mu,\) and</p>
</li>
<li><p>the graph \(\llbracket \psi \rrbracket := \{(y,x) \in (\mathsf E, \mathsf S) : \psi(y) = x\}\) is contained in the product \(\sigma-\)algebra \(\mathscr E \otimes \mathscr S.\)</p>
</li>
</ol>
<p>Then \(\gamma\) has a \((\psi, \mu)-\)disintegration \(\kappa\), unique up to a \(\mu-\)equivalence, in the sense that if \(\overline \kappa\) is another \((\psi, \mu)-\)disintegration, then \(\mu\{x \in \mathsf S : \kappa_x \neq \overline\kappa_x\} = 0.\)</p></div>
<p>As an example of a Radon measure, every \(\sigma-\)finite measure on the Borel \(\sigma-\)algebra of a Polish space which assigns finite measure to compact sets is Radon.</p>
<p>As a curiosity check out <a href="https://en.wikipedia.org/wiki/Lifting_theory#Application:_disintegration_of_a_measure">lifting theory</a>.</p>
<h2 id="markov_decision_process"><a href="#markov_decision_process" class="header-anchor">MARKOV DECISION PROCESS</a></h2>
<h3 id="markov_decision_model"><a href="#markov_decision_model" class="header-anchor">Markov Decision Model</a></h3>
<p>Let us start by defining the moving parts of a Markov decision process. Our model evolves through time such that we can observe the state of the model and take admissible actions at discrete times \(n = 0, 1, 2, \ldots.\) We will say \(n \ge 0\) to mean \(n\) is a non-negative integer.</p>
<h4 id="state_space"><a href="#state_space" class="header-anchor">State Space</a></h4>
<p>We denote by \(\mathsf{S}_n\) the state space of the model at time \(n \ge 0.\) We will assume that for each \(n \ge 0\), \(\mathsf{S}_n\) is a standard Borel space endowed with its Borel \(\sigma-\)algebra.</p>
<h4 id="action_space"><a href="#action_space" class="header-anchor">Action Space</a></h4>
<p>We denote by \(\mathsf A_n\) the action space at time \(n \ge 0.\) This is the set from which possible actions can be chosen at time \(n.\) It is possible that the admissible actions at time \(n\) is a strict subset of \(\mathsf A_n\) depending on the state of the model. We will again assume that for each \(n \ge 0\), \(\mathsf A_n\) is a standard Borel space endowed with its Borel \(\sigma-\)algebra.</p>
<h4 id="admissible_actions"><a href="#admissible_actions" class="header-anchor">Admissible Actions</a></h4>
<p>For each \(n \ge 0\), we have a mapping <div class="nonumber">\[\begin{aligned} \alpha_n \colon \mathsf S_n \to \mathscr A_n\end{aligned}\]</div> from the state space \(\mathsf S_n\) to the measurable subsets of the action space \(\mathsf A_n\), which assigns to each \(x \in \mathsf S_n\) the set \(\alpha_n(x)\) of admissible actions. We will assume that <div class="nonumber">\[\begin{aligned} \llbracket\alpha_n\rrbracket \in \mathscr S_n \otimes \mathscr A_n, \quad \forall \; n \ge 0,\end{aligned}\]</div> and that <div class="nonumber">\[\begin{aligned} \text{there exists a measurable } f_n \colon \mathsf S_n \to \mathsf A_n \text{ such that } \llbracket f_n \rrbracket \subseteq 
\llbracket \alpha_n \rrbracket, \quad \forall \; n \ge 0.\end{aligned}\]</div> Here the notation</p>
<div class="nonumber">\[\begin{aligned}
\llbracket\alpha_n\rrbracket &:= \{(x,a) \in \mathsf S_n \times \mathsf A_n \mid a \in \alpha_n(x)\} \\
\llbracket f_n\rrbracket &:= \{(x,a) \in \mathsf S_n \times \mathsf A_n \mid a= f_n(x)\}
\end{aligned}\]</div>
<p>denotes the <em>graphs</em> of \(\alpha_n\) and \(f_n\) respectively. For a justification of these assumptions, see the section <a href="/otium/mdp#a_note_about_admissible_actions">“A Note About Admissible Actions”</a> below.</p>
<h4 id="transition_law"><a href="#transition_law" class="header-anchor">Transition Law</a></h4>
<p>For each time \(n \ge 0\), we have a stochastic kernel <div class="nonumber">\[\begin{aligned} \kappa_n \colon \llbracket\alpha_n\rrbracket \times \mathscr S_{n+1} \to [0,1]\end{aligned}\]</div> from the graph \(\llbracket\alpha_n\rrbracket\) &#40;endowed with the Borel \(\sigma-\)algebra on the subspace topology&#41; to \(\mathsf S_{n+1}\), called the transition law. Therefore, if at time \(n\) the model’s state is \(x_n\) and we took an admissible action \(a_n\), then the probability of finding the model in state \(B \in \mathscr S_{n+1}\) at time \(n+1\) is \(\kappa_n((x_n, a_n), B).\)</p>
<h4 id="reward_function"><a href="#reward_function" class="header-anchor">Reward Function</a></h4>
<p>For each time \(n \ge 0\), we have a measurable reward function <div class="nonumber">\[\begin{aligned} r_n \colon \llbracket \alpha_n \rrbracket \times \mathsf S_{n+1} \to [-\infty, \infty).\end{aligned}\]</div> For \((x_n, a_n) \in \llbracket \alpha_n \rrbracket\) and \(x_{n+1} \in \mathsf S_{n+1}\), \(r_n((x_n, a_n), x_{n+1})\) models the reward received at time \(n\) when at state \(x_n\) the admissible action \(a_n\) was taken and the system transitioned to state \(x_{n+1}.\) Under many performance criteria, it will suffice to model the rewards using \(\overline r_n \colon \llbracket \alpha_n \rrbracket \to [-\infty, \infty)\) which we can get from \(r_n\) &#40;assuming appropriate integrability&#41; using <div class="nonumber">\[\begin{aligned} \overline r_n(x,a) = \int_{\mathsf S_{n+1}} r_n((x,a),y)  \kappa_n((x,a), \mathrm{d}y),\end{aligned}\]</div> and, in fact, moving forward, we will assume that \(r_n\) has the form <div class="nonumber">\[\begin{aligned} r_n \colon \llbracket \alpha_n \rrbracket \to [-\infty, \infty).\end{aligned}\]</div></p>
<h4 id="stationary_markov_decision_model"><a href="#stationary_markov_decision_model" class="header-anchor">Stationary Markov Decision Model</a></h4>
<p>The sequence <div class="nonumber">\[\begin{aligned} \left\{\left(\mathsf S_n, \mathsf A_n, \alpha_n, \kappa_n, r_n\right)\right\}_{n \ge 0}\end{aligned}\]</div> of tuples defined above is called the <em>non-stationary Markov decision model</em>. We can find an equivalent <em>stationary</em> Markov decision model \((\mathsf S, \mathsf A, \alpha, \kappa, r)\) from a non-stationary Markov decision model by a standard augmentation procedure:</p>
<p>Define the tuple \((\mathsf S, \mathsf A, \alpha, \kappa, r)\) as follows:</p>
<div class="nonumber">\[\begin{aligned}
\mathsf S &:= \{(x,n) \mid x \in \mathsf S_n, n \ge 0\},\\
\mathsf A &:= \{(a,n) \mid x \in \mathsf A_n, n \ge 0\},\\
\mathsf S \ni (x,n) \mapsto \alpha((x,n)) &:= \{(a,t) \in \mathsf A \mid a \in \alpha_n(x)\}, \\
\kappa(((x,n), (a,n)), \{(b,n+1) \mid b \in B\}) &:= \kappa_n((x,a), B), \quad B \in \mathscr S_{n+1}, \\
\llbracket \alpha\rrbracket \ni ((x,n), (a,n)) \mapsto r((x,n), (a,n)) &:= r_n(x,a).
\end{aligned}\]</div>
<p>We will thus assume a stationary Markov decision model from now on.</p>
<div class="def"><p><strong>Definition 8</strong>: A <em>Markov decision model</em> is a tuple <div class="nonumber">\[\begin{aligned} (\mathsf S, \mathsf A, \alpha, \kappa, r)\end{aligned}\]</div> consisting of</p>
<ol>
<li><p>the <em>state space</em> \(\mathsf S\) which is a standard Borel space;</p>
</li>
<li><p>the <em>action space</em> or the <em>control space</em> \(\mathsf A\) which is a standard Borel space;</p>
</li>
<li><p>the mapping \(\alpha \colon \mathsf S \to \mathscr A\) from the state space to the measurable subsets of the action space, which assigns to each \(x \in \mathsf S\) the set \(\alpha(x)\) of <em>admissible actions</em> satisfying the assumptions: <a id="a1" class="anchor"></a>\[\begin{aligned} 
    \llbracket\alpha\rrbracket \in \mathscr S \otimes \mathscr A,\end{aligned}\] <a id="a2" class="anchor"></a>\[\begin{aligned} 
    \text{there exists a measurable } f \colon \mathsf S \to \mathsf A \text{ such that } \llbracket f \rrbracket \subseteq \llbracket \alpha \rrbracket;\end{aligned}\]</p>
</li>
<li><p>the stochastic kernel \(\kappa\) from the graph \(\llbracket \alpha \rrbracket\) of \(\alpha\) to \(\mathsf S\) called the <em>transition law</em>; and</p>
</li>
<li><p>the measurable <em>reward function</em> \(r \colon \llbracket \alpha \rrbracket \to [-\infty, \infty).\)</p>
</li>
</ol></div>
<h4 id="a_note_about_admissible_actions"><a href="#a_note_about_admissible_actions" class="header-anchor">A Note About Admissible Actions</a></h4>
<p>The raison d&#39;être for the assumptions <span class="eqref">(<a href="#a1">7</a>)</span> and <span class="eqref">(<a href="#a2">8</a>)</span> is to avoid measurability hell. This is the topic of measurable selection theorems. I discussed these theorems under special cases in <a href="https://makkar.github.io/otium/grlthprcs">two previous</a> <a href="https://makkar.github.io/otium/grlthprcs2">blog posts</a> for their applications to general theory of processes &#40;they are called section theorems there which means the same thing as selection theorems&#41;.</p>
<p>Let us very briefly discuss measurable selection in a general setting. Let \((\mathsf T, \mathscr T)\) be a measurable space, \(\mathsf X\) be a topological space, and \(\phi \colon \mathsf T \to \mathfrak{P}(\mathsf X)\) be a <em>set-valued function</em> &#40;also called a <em>multifunction</em> or a <em>correspondence</em>&#41; taking values in the class of all subsets of \(\mathsf X.\) We say that a multifunction \(\phi\) is closed if \(\phi(t)\) is closed for each \(t \in \mathsf T.\) Note that \(\phi\) can be equivalently identified with a subset of \(\mathsf T \times \mathsf X.\) A <em>selection</em> &#40;also called a <em>section</em>&#41; of \(\phi\) is a function \(f \colon \mathsf T \to \mathsf X\) such that \(f(t) \in \phi(t)\) for each \(t \in \mathsf T.\) If \(\phi(t) \neq \varnothing\) for each \(t \in \mathsf T\), then at least one selection exists by the axiom of choice. Note that writing \(\llbracket f \rrbracket \subseteq \llbracket \phi \rrbracket\) is same as saying \(f\) is a selection of \(\phi.\) If we denote <div class="nonumber">\[\begin{aligned} \mathfrak{S}(\phi) := \{f \colon \mathsf T \to \mathsf X \mid f \text{ is a measurable selection of }\phi\},\end{aligned}\]</div> then measurable selection theorems tell us when \(\mathfrak{S}(\phi)\) is nonempty. Note that if the \(\sigma-\)algebra \(\mathscr T\) is \(\mathfrak{P}(\mathsf T),\) for example if \(\mathsf T\) is countable, then any function \(f \colon \mathsf T \to \mathsf X\) satisfying \(\llbracket f \rrbracket \subseteq \llbracket \phi \rrbracket\) is measurable.</p>
<p>To be able to study measurability of selections it seems natural to first define measurability of multifunctions, and then relate the two notions. It turns out doing so isn’t straightforward. But before we get to measurability, we need a notion of inverse for multifunctions, of which there are two natural ones. The <em>upper inverse</em> \(\phi^{*}\) and the <em>lower inverse</em> \(\phi_*\) are defined by</p>
<div class="nonumber">\[\begin{aligned}
\phi^*(A) &:= \{t \in \mathsf T : \phi(t) \subseteq A\}, \quad A \subseteq \mathsf X, \\
\phi_*(A) &:= \{t \in \mathsf T : \phi(t) \cap A \neq \varnothing \}, \quad A \subseteq \mathsf X.
\end{aligned}\]</div>
<p>Note that \(\phi^*(A) = \mathsf X \setminus \phi_*(\mathsf X \setminus A)\), and therefore we can equivalently use either of the two inverses to define measurability notions. We say that \(\phi\) is</p>
<ol>
<li><p><em>weakly measurable</em>, if \(\phi_*(G) \in \mathscr T\) for each open \(G \subseteq \mathsf X\);</p>
</li>
<li><p><em>measurable</em>, if \(\phi_*(F) \in \mathscr T\) for each closed \(F \subseteq \mathsf X\);</p>
</li>
<li><p><em>Borel measurable</em>, if \(\phi_*(B) \in \mathscr T\) for each Borel subset \(B \subseteq \mathsf X.\)</p>
</li>
</ol>
<p>For the inverses we have that <div class="nonumber">\[\begin{aligned} \phi^*\left(\bigcap_{i \in I} A_i\right) = \bigcap_{i \in I}\phi^*(A_i) \text{ and } \phi_*\left(\bigcup_{i \in I} A_i\right) = \bigcup_{i \in I}\phi_*(A_i),\end{aligned}\]</div></p>
<p>but unlike functions, the inverses of multifunctions don’t behave well with complements, thereby necessitating three notions of measurability above.</p>
<p>The most well-known measurable selection theorem is the <strong>Kuratowski–Ryll-Nardzewski selection theorem</strong>, which states that a weakly measurable closed multifunction into a Polish space admits a measurable selection. It can be shown that a weakly measurable closed multifunction into a separable metrizable space has measurable graph &#40;in the product \(\sigma-\)algebra&#41;, and conversely if a closed multifunction into a separable metrizable space is compact-valued such that its graph is measurable, then it is weakly measurable. The converse allows usage of Kuratowski–Ryll-Nardzewski selection theorem to justify existence of a measurable selection. This explains why many papers and books in control, instead of having the lazy assumption <span class="eqref">(<a href="#a2">8</a>)</span>, assume that the mapping \(\alpha\) in definition 8 is compact-valued, an assumption which also has the advantage of being more explicit and avoiding inconsistencies.</p>
<p>Assumption <span class="eqref">(<a href="#a2">8</a>)</span> circumvents bringing conditions for measurable selection theorems in our discussion ahead, while assumption <span class="eqref">(<a href="#a1">7</a>)</span> allows us to define \(\kappa.\) For more details check the two review papers <span class="bibref"><a href="#wagner1">(Wagner, 1977)</a>, <a href="#wagner2">(Wagner, 1980)</a></span> or Chapter 18 of the book <span class="bibref"><a href="#aliprantis">(Aliprantis and Border, 2006)</a></span>.</p>
<h3 id="policy"><a href="#policy" class="header-anchor">Policy</a></h3>
<p>A policy \(\pi = \{\pi_n\}_{n \ge 0}\), also known as an admissible control, is a sequence of prescriptions \(\pi_n\) that at time \(n\) gives a rule for selecting an action. \(\pi_n\) can be a randomized procedure or a deterministic procedure; it can depend on the entire history of the model or only on the current state. Let us formalize this concept.</p>
<h4 id="history"><a href="#history" class="header-anchor">History</a></h4>
<div class="def"><strong>Definition 9</strong>: For each time \(n \ge 0\), define the space \(\mathsf H_n\) of admissible <em>histories</em> up to time \(n\) by \(\mathsf H_0 = \mathsf S\), and <div class="nonumber">\[\begin{aligned} \mathsf H_n = \mathsf S \times \llbracket \alpha \rrbracket  ^n =  \mathsf H_{n-1} \times \llbracket \alpha \rrbracket, \quad n \ge 1.\end{aligned}\]</div></div>
<p>An element \(h_n \in \mathsf H_n\) is of the form \(h_n = (x_0, a_0, x_1, a_1, \ldots, x_{n-1}, a_{n-1}, x_n).\)</p>
<div class="def"><strong>Definition 10</strong>: A <em>policy</em> \(\pi = \{\pi_n\}_{n \ge 0}\) is a sequence of stochastic kernels \(\pi_n\) from \(\mathsf H_n\) to \(\mathsf A\) subject to the constraint <a id="p1" class="anchor"></a>\[\begin{aligned} 
\pi_n(h_n, \alpha(x_n)) = 1, \quad \forall \; h_n \in \mathsf H_n, n \ge 0.\end{aligned}\] The set of all policies is denoted by \(\Pi.\)</div>
<h4 id="classes_of_policies"><a href="#classes_of_policies" class="header-anchor">Classes of Policies</a></h4>
<p>The largest class of policies we consider is \(\Pi\) defined above. If instead of using the entire history, the policy makes a decision using only the current state, then we get a Markov policy.</p>
<div class="def"><strong>Definition 11</strong>: If a policy \(\pi = \{\pi_n\}_{n \ge 0} \in \Pi\) is such that for each \(n \ge 0\), there exists a stochastic kernel \(\varphi_n\) from \(\mathsf S\) to \(\mathsf A\) such that <div class="nonumber">\[\begin{aligned} \pi_n(h_n, \cdot) = \varphi_n(x_n, \cdot), \quad \forall \; h_n = (x_0, a_0, \ldots, x_{n-1}, a_{n-1}, x_n) \in \mathsf H_n,\end{aligned}\]</div> then \(\pi\) is called a <em>Markov policy</em>. We denote the set of all Markov policies by \(\Pi_M.\)</div>
<p>We will sometimes abuse notation and write \(\pi = \{\varphi_n\}_{n \ge 0}\) for a Markov policy, where \(\varphi_n\) are the stochastic kernels as above. Even more simply, if our policy at each time doesn’t depend on the time but only on the current state, we get a stationary policy. A stationary policy is necessarily Markovian.</p>
<div class="def"><strong>Definition 12</strong>: If a policy \(\pi = \{\pi_n\}_{n \ge 0} \in \Pi\) is such that there exists a stochastic kernel \(\varphi\) from \(\mathsf S\) to \(\mathsf A\) such that for each \(n \ge 0\), <div class="nonumber">\[\begin{aligned} \pi_n(h_n, \cdot) = \varphi(x_n, \cdot), \quad \forall \; h_n = (x_0, a_0, \ldots, x_{n-1}, a_{n-1}, x_n) \in \mathsf H_n,\end{aligned}\]</div> then \(\pi\) is called a <em>stationary policy</em>. We denote the set of all stationary policies by \(\Pi_S.\)</div>
<p>What about deterministic policies?</p>
<div class="def"><strong>Definition 13</strong>: A policy \(\phi = \{f_n\}_{n \ge 0}\) is called a <em>deterministic policy</em> if it is a sequence of measurable functions \(f_n \colon \mathsf H_n \to \mathsf A\) such that \(f_n(h_n) \in \alpha(x_n)\) for all \(h_n \in \mathsf H_n\) and \(n \ge 0.\) We denote the set of deterministic policies by \(\Pi_D.\) Similar to definitions 11 and 12, we can define the class of <em>deterministic Markov policies</em> \(\Pi_{DM}\) and <em>deterministic stationary policies</em> \(\Pi_{DS}.\)</div>
<p>The set \(\Pi\) contains these deterministic policies, as can be easily seen by observing that for the deterministic policy \(\phi = \{f_n\}_{n \ge 0}\) the corresponding policy \(\pi = \{\pi_n\}_{n \ge 0}\) is given by <div class="nonumber">\[\begin{aligned} \pi_n(h_n, C) = \delta_{f_n(h_n)}(C), \quad h_n \in \mathsf H_n, C \in \mathscr A.\end{aligned}\]</div></p>
<p>We thus have the following inclusions: <div class="nonumber">\[\begin{aligned} \Pi_{DS} \subseteq\Pi_S \subseteq \Pi_{M} \subseteq \Pi, \; \Pi_{DS} \subseteq \Pi_{DM} \subseteq \Pi_{D} \subseteq \Pi, \; \Pi_{DM} \subseteq \Pi_{M}.\end{aligned}\]</div></p>
<h3 id="canonical_construction"><a href="#canonical_construction" class="header-anchor">Canonical Construction</a></h3>
<p>A natural question now arises to the suspicious reader about the existence of a probability space on which the random quantities discussed above are from. More precisely:</p>
<div class="important"><p>Given a Markov decision model \((\mathsf S, \mathsf A, \alpha, \kappa, r)\), a policy \(\pi \in \Pi,\) and a probability measure \(\nu\) on \(\mathsf S\), does there exist a probability space \((\Omega, \mathscr{F}, \mathbb{P}_{\nu}^{\pi}),\) and two stochastic processes \(\{X_n\}_{n \ge 0}\) and \(\{A_n\}_{n \ge 0}\) on this probability space, with the random variable \(X_n\) taking values in \(\mathsf S\) and the random variable \(A_n\) taking values in \(\mathsf A\) for each \(n \ge 0\), such that the following three properties hold true?</p>
<ol>
<li><p>The law of \(X_0\) equals \(\nu\), i.e.,</p>
<a id="c1" class="anchor"></a>\[\begin{aligned} 
    \mathbb{P}_\nu^\pi\{X_0 \in B\} = \nu(B), \quad B \in \mathscr S.\end{aligned}\]
</li>
<li><p>If \(H_n = (X_0, A_0, \ldots, X_{n-1}, A_{n-1}, X_n) \colon \Omega \to \mathsf{H}_n\) denotes the <em>history random variable</em>, the joint distribution of \((H_n,A_n)\) is given by \((\mathbb{P}_\nu^\pi \circ H_n^{-1}) \otimes \pi_n.\) More intuitively, by Theorem 4, this means that the stochastic kernel \(\overline{\pi}_n\) from \((\Omega, \sigma(H_n))\) to \((\mathsf{A}, \mathscr A)\) defined by <a id="c2" class="anchor"></a>\[\begin{aligned} 
    \overline\pi_n(\omega, C) = \pi_n(H_n(\omega), C), \quad \omega \in \Omega,\, C \in\mathscr A,\end{aligned}\]  is a version of the conditional distribution of \(A_n\) given \(H_n.\)</p>
</li>
<li><p>The joint distribution of \((H_n, A_n, X_{n+1})\) is given by \((\mathbb{P}^\pi_\nu \circ (H_n, A_n)^{-1}) \otimes \kappa.\) More intuitively, by Theorem 4, this means that the stochastic kernel \(\overline\kappa_n\) from \((\Omega, \sigma(H_{n}, A_n))\) to \((\mathsf S, \mathscr S)\) defined by <a id="c3" class="anchor"></a>\[\begin{aligned} 
    \overline\kappa_n(\omega, B) = \kappa((X_n(\omega), A_n(\omega)), B), \quad \omega \in \Omega,\, B \in \mathscr S,\end{aligned}\]  is a version of the conditional distribution of \(X_{n+1}\) given \((H_n, A_n).\)</p>
</li>
</ol></div>
<p>The answer to our question is, of course, yes. At this point recall the Ionescu-Tulcea theorem:</p>
<div class="thm"><strong>Theorem 6 &#91;Ionescu-Tuclea&#93;</strong>: Let \(\{\mathsf E_n, \mathscr{E}_n\}_{n \ge 0}\) be sequence of arbitrary measurable spaces. For each \(n \ge 0\), let \(\eta_{n+1}\) be a stochastic kernel from \((\mathsf E_0\times \cdots \times \mathsf E_n, \mathscr{E}_0 \otimes \cdots \otimes \mathscr{E}_n)\) to \((\mathsf E_{n+1}, \mathscr{E}_{n+1})\). Finally, let \(\mu\) be a probability measure on \((\mathsf E_{0}, \mathscr{E}_{0})\). Then there exists a unique probability measure \(\mathbb P\) on the measurable space \((\mathsf E, \mathscr{E}) = (\mathsf E_0\times \mathsf E_1\times \cdots, \mathscr{E}_0 \otimes \mathscr{E}_1 \otimes \cdots)\) whose value on every cylinder set \(B = B_0 \times \cdots \times B_m \times \mathsf E_{m+1} \times \mathsf E_{m+2} \times \cdots\) for all \(m \ge 0\), where \(B_i \in \mathscr E_i\) for \(i = 0, \ldots, m\), is given by <div class="nonumber">\[\begin{aligned} \mathbb P(B) = \int_{B_0} \mu(\mathrm{d}x_0) \int_{B_1} \eta_1(x_0, \mathrm{d}x_1)\cdots \int_{B_m} \eta_m((x_0, x_1, \ldots, x_{n-1}), \mathrm{d}x_n).\end{aligned}\]</div> More generally, for any non-negative random variable \(Z\) on \((\mathsf E, \mathscr{E})\) which only depends on the coordinates up to some finite index \(m \ge 0\), we have <div class="nonumber">\[\begin{aligned} \int_{\mathsf E} Z \, \mathrm{d}\mathbb{P} = \int_{B_0} \mu(\mathrm{d}x_0) \int_{B_1} \eta_1(x_0, \mathrm{d}x_1)\cdots \int_{B_m} \eta_m((x_0, x_1, \ldots, x_{n-1}), \mathrm{d}x_n) Z(x_0, \ldots, x_m).\end{aligned}\]</div></div>
<p>To this end, define <div class="nonumber">\[\begin{aligned} \Omega = (\mathsf S \times \mathsf A)^{\infty}\end{aligned}\]</div> and \(\mathscr{F}\) to be the product \(\sigma-\)algebra on \(\Omega\). The elements of \(\Omega\) are sequences of the form \(\omega = (x_0, a_0, x_1, a_1, \ldots).\) Comparing our formulation to the one in Theorem 6, we let \(\mathsf E_{2n}\) correspond to \(\mathsf S\) and \(\mathsf E_{2n+1}\) correspond to \(\mathsf A\) for each \(n \ge 0.\) We let \(\mu\) correspond to \(\nu.\) We let \(\eta_{2n}\) correspond to the stochastic kernel \(\pi_n\) from \((\mathsf S \times \mathsf A)^n \times \mathsf S\) to \(\mathsf A\), and we let \(\eta_{2n+1}\) correspond to the stochastic kernel from \((\mathsf S \times \mathsf A)^{n+1}\) to \(\mathsf S\) obtained from \(\kappa\), for each \(n \ge 0.\) More concretely, <div class="nonumber">\[\begin{aligned} \eta_{2n+1}((x_0, a_0, \ldots, x_n, a_n), B) := \kappa((x_n, a_n), B).\end{aligned}\]</div></p>
<p>Then Theorem 5 implies that there exists a probability space \((\Omega, \mathscr{F}, \mathbb{P}_{\nu}^{\pi})\) satisfying <span class="eqref">(<a href="#c1">10</a>)</span>, <span class="eqref">(<a href="#c2">11</a>)</span> and <span class="eqref">(<a href="#c3">12</a>)</span>, where we define the random variables \(\{X_n\}_{n \ge 0}\) and \(\{A_n\}_{n \ge 0}\) by the projection maps:</p>
<div class="nonumber">\[\begin{aligned}
\Omega \ni \omega = (x_0, a_0, x_1, a_1, \ldots)&\mapsto X_n(\omega) := x_n \in \mathsf S, \\
\Omega \ni \omega = (x_0, a_0, x_1, a_1, \ldots)&\mapsto A_n(\omega) := a_n \in \mathsf A.
\end{aligned}\]</div>
<p>We denote by \(\mathbb{E}_\nu^\pi\) the expectation operator with respect to the probability space \((\Omega, \mathscr{F}, \mathbb{P}_\nu^\pi).\) If \(\nu = \delta_x\) is a Dirac measure at \(x \in \mathsf S,\) we will simply write \(\mathbb{P}_x^\pi\) and \(\mathbb{E}_x^\pi\) for \(\mathbb{P}_{\delta_x}^\pi\) and \(\mathbb{E}_{\delta_x}^\pi.\)</p>
<p>Note that <span class="eqref">(<a href="#r1">1</a>)</span> implies that we can write <span class="eqref">(<a href="#c2">11</a>)</span> and <span class="eqref">(<a href="#c3">12</a>)</span> as <a id="c2a" class="anchor"></a>\[\begin{aligned} 
\mathbb{P}_\nu^\pi\left[\{A_n \in C\} \mid \sigma(H_n)\right] = \pi_n(H_n, C), \quad C \in \mathscr A,\end{aligned}\] <a id="c3a" class="anchor"></a>\[\begin{aligned} 
\mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(H_n, A_n)\right] = \kappa((X_n, A_n), B), \quad B \in \mathscr S.\end{aligned}\]</p>
<p>Also notice that because of the constraint <span class="eqref">(<a href="#p1">9</a>)</span> on a policy, the probability measure \(\mathbb{P}_\nu^\pi\) is supported on the closure of the set of all possible histories \(\mathsf H_\infty := \mathsf S \times \llbracket \alpha \rrbracket^\infty.\)</p>
<div class="def"><strong>Definition 14</strong>: Given a Markov decision model \((\mathsf S, \mathsf A, \alpha, \kappa, r)\), an initial distribution \(\nu,\) and a policy \(\pi \in \Pi,\) the associated stochastic process \((\Omega, \mathscr{F}, \mathbb{P}_\nu^\pi, \{X_n\}_{n \ge 0})\) is called a <em>Markov decision process</em>.</div>
<div class="def"><strong>Definition 15</strong>: We call \(\{X_n\}_{n \ge 0}\) the <em>state process</em> and \(\{A_n\}_{n \ge 0}\) the <em>action process</em>.</div>
<h3 id="markov_state_process"><a href="#markov_state_process" class="header-anchor">Markov State Process</a></h3>
<p>Although equation <span class="eqref">(<a href="#c3a">14</a>)</span> looks like a Markov condition, the state process \(\{X_n\}\) may not be a Markov process because of the dependence on history through the action process. It seems intuitively plausible that if the policy is Markov, then the state process should also be Markov. We prove this now.</p>
<p><strong>Notation</strong>: If \(\varphi\) is a stochastic kernel from \(\mathsf S\) to \(\mathsf A\), then for every \(x \in \mathsf S\), define</p>
<div class="nonumber">\[\begin{aligned}
r(x,\varphi) &:= \int_{\mathsf A} r(x,a) \varphi(x, \mathrm{d}a), \text{ and} \\
\kappa((x, \varphi), \cdot) &:= \int_{\mathsf A} \kappa((x,a), \cdot) \varphi(x, \mathrm{d}a).
\end{aligned}\]</div>
<p>Note that \(r(\cdot, \varphi)\) is a measurable function and \(\kappa((\cdot, \varphi), \cdot)\) is a stochastic kernel on \(\mathsf S.\)</p>
<div class="thm"><strong>Theorem 7</strong>: Under the setting of Definition 14, but where the policy \(\pi = \{\varphi_n\}_{n \ge 0} \in \Pi_M\), the state process \(\{X_n\}_{n \ge 0}\) is a non-homogeneous Markov process with transition kernels \(\{\kappa((\cdot,\varphi_n),\cdot)\}_{n \ge 0}\), i.e., for every \(B \in \mathscr S\) and \(n \ge 0\), almost surely <a id="m1" class="anchor"></a>\[\begin{aligned} 
\mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(X_0, X_1, \ldots, X_n)\right]= \kappa((X_n, \varphi_n), B)= \mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(X_n)\right].\end{aligned}\] In particular, if \(\pi \in \Pi_{S}\), then the state process is a homogeneous Markov process.</div>
<div class="proof"><p>Let us start by fixing <em>any</em> policy \(\pi = \{\pi_n\}_{n \ge 0} \in \Pi\) and any \(B \in \mathscr S.\) Then <div class="nonumber">\[\begin{aligned}
    \mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(H_n)\right]
    &=\mathbb{E}_\nu^\pi\left[\mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(H_n, A_n)\right] \mid \sigma(H_n)\right] \\
    &=\mathbb{E}_\nu^\pi\left[\kappa((X_n, A_n), B) \mid \sigma(H_n)\right] \\
    &= \int_{\mathsf A} \kappa((X_n, a), B) \,\pi_n(H_n, \mathrm{d}a),
    \end{aligned}\]</div></p>
<p>where the first equality follows from the tower rule for conditional expectation, the second equality follows from <span class="eqref">(<a href="#c3a">14</a>)</span>, and the third equality follows from <span class="eqref">(<a href="#c2">11</a>)</span> and <span class="eqref">(<a href="#d2">5</a>)</span>.</p>
<p>Now if \(\pi = \{\varphi_n\}_{n \ge 0} \in \Pi_M\), then the equation above becomes <a id="eq3" class="anchor"></a>\[\begin{aligned} 
    \mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(H_n)\right]
    =\int_{\mathsf A} \kappa((X_n, a), B) \,\varphi_n(X_n, \mathrm{d}a) = \kappa((X_n, \varphi_n), B).\end{aligned}\]</p>
<p>Then using the tower rule again, substituting equation <span class="eqref">(<a href="#eq3">16</a>)</span>, and using the fact that \(\kappa((X_n, \varphi_n), B)\) is \(\sigma(X_n)-\)measurable, the LHS of equation <span class="eqref">(<a href="#m1">15</a>)</span> can be written <div class="nonumber">\[\begin{aligned}
    \mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(X_0, X_1, \ldots, X_n)\right] &= \mathbb{E}_\nu^\pi\left[\mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(H_n)\right] \mid \sigma(X_0, X_1, \ldots, X_n)\right] \\
    &= \mathbb{E}_\nu^\pi\left[\kappa((X_n, \varphi_n), B) \mid \sigma(X_0, X_1, \ldots, X_n)\right] \\
    &= \kappa((X_n, \varphi_n), B),
    \end{aligned}\]</div>
showing the first equality in equation <span class="eqref">(<a href="#m1">15</a>)</span>.

Similarly we can write <div class="nonumber">\[\begin{aligned}
    \mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(X_n)\right] &= \mathbb{E}_\nu^\pi\left[\mathbb{P}_\nu^\pi\left[\{X_{n+1} \in B\} \mid \sigma(H_n)\right] \mid \sigma(X_n)\right] \\
    &= \mathbb{E}_\nu^\pi\left[\kappa((X_n, \varphi_n), B) \mid \sigma(X_n)\right] \\
    &= \kappa((X_n, \varphi_n), B),
    \end{aligned}\]</div>
showing the second equality in equation <span class="eqref">(<a href="#m1">15</a>)</span>.</p></div>
<h2 id="optimal_policies"><a href="#optimal_policies" class="header-anchor">OPTIMAL POLICIES</a></h2>
<p>Until now we have implicitly assumed that the total period of time over which the system is observed is infinite. In many situations we are interested in the finite horizon problem. So let us denote by \(T \in \mathbb N \cup \{+\infty\}\) the length of this planning or control horizon. This gives a way to classify Markov decision processes—finite horizon or infinite horizon problems.</p>
<p>Markov decision processes can also be classified according to the performance criterion used.</p>
<h3 id="performance_criteria"><a href="#performance_criteria" class="header-anchor">Performance Criteria</a></h3>
<p>Given a Markov decision model and an initial distribution, we have a choice over which policy to choose. We will need to define what performance criterion we are optimizing for to choose an optimal policy. Suppose we have a class \(\Pi_A\) of admissible policies, where \(\Pi_A\) could be \(\Pi\) or \(\Pi_{DM}\) or any other class we discussed above. The performance criterion measures how “good” a policy is. There are two performance criteria which are common in the literature: expected total reward and the long-run average expected reward per unit time.</p>
<h4 id="expected_total_reward"><a href="#expected_total_reward" class="header-anchor">Expected Total Reward</a></h4>
<div class="def"><strong>Definition 16</strong>: Given an initial state \(X_0 = x \in \mathsf S\), a policy \(\pi \in \Pi_A\), and a discount factor \(\beta \in (0,1]\), the <em>expected total reward</em> is given by <div class="nonumber">\[\begin{aligned} J(\pi, x) := \mathbb{E}^\pi_{x}\left[ \sum_{n=0}^T \beta^n r(X_n, A_n, X_{n+1}) \right]\end{aligned}\]</div> if \(T = +\infty,\) and <div class="nonumber">\[\begin{aligned} J(\pi, x) := \mathbb{E}_x^\pi \left[ \sum_{n=0}^{T-1} \beta^n r(X_n, A_n, X_{n+1}) + r_T(X_T) \right]\end{aligned}\]</div> if \(T \in \mathbb N,\)  with \(r_N \colon \mathsf S \to [-\infty, \infty)\), a measurable function, denoting the terminal reward.</div>
<p>Recall that \(\mathbb{E}_x^\pi\) simply means \(\mathbb{E}_{\delta_x}^\pi.\) In this setting it is usually assumed that the reward function \(r\) is bounded, so that \(J(\pi, x)\) is a bounded function.</p>
<p>We use \(J^*\) to denote the <em>value function</em> <div class="nonumber">\[\begin{aligned} J^*(x) := \sup_{\pi \in \Pi_A} J(\pi, x), \quad x \in \mathsf S.\end{aligned}\]</div></p>
<p>The problem is to find &#40;if it exists&#33;&#41; a policy \(\pi^* \in \Pi_A\) such that <div class="nonumber">\[\begin{aligned} J(\pi^*, x) = J^*(x), \quad \forall \; x \in \mathsf S.\end{aligned}\]</div></p>
<h4 id="long-run_average_expected_reward"><a href="#long-run_average_expected_reward" class="header-anchor">Long-run Average Expected Reward</a></h4>
<div class="def"><strong>Definition 17</strong>: Given an initial state \(x_0 = x \in \mathsf S\) and a policy \(\pi \in \Pi_A\), the <em>long-run average expected reward per unit time</em> is given by <div class="nonumber">\[\begin{aligned} J(\pi, x) := \liminf_{m \to \infty} 
\frac{1}{m} \mathbb{E}^\pi_{x}\left[ \sum_{n=0}^m r(X_n, A_n) \right].\end{aligned}\]</div></div>
<p>We could have taken \(\limsup\), and both give different results, but the \(\liminf\) case is easier to handle. Intuitively, the \(\liminf\) case gives a more pessimistic picture, while the \(\limsup\) gives a more optimistic picture. See the survey <span class="bibref"><a href="#reward_survey">(ABFGM, 1993)</a></span> for a detailed treatment of this performance criterion. We will be focusing on the expected total reward next.</p>
<p>The choice of the performance criterion is application dependent. For example, if the future rewards are to be valued less than immediate rewards, then using expected total reward with \(\beta < 1\) would make sense. On the other hand, if the long term behaviour is to be studied and initial transient period is to be ignored, then using the long-run average expected reward makes sense.</p>
<h3 id="deterministic_markov_policy"><a href="#deterministic_markov_policy" class="header-anchor">Deterministic Markov Policy</a></h3>
<p>Suppose that we are in the finite horizon with the expected total reward regime. Assume that the terminal reward is \(0\) for simplicity, and that the <div class="nonumber">\[\begin{aligned} \text{reward function } r \colon \mathsf S \times \mathsf A \to \mathbb R \text{ is bounded.}\end{aligned}\]</div></p>
<p>As the notation suggests, we have also implicitly assumed that \(\alpha(x) = \mathsf A\) for every \(x \in \mathsf S.\) In this setting we want to compare the classes \(\Pi_D\) and \(\Pi_{DM}\) of policies. We claim that there is no loss of optimality in restricting attention to the smaller class \(\Pi_{DM}\) of deterministic Markov policies. This is a surprising result&#33; Policies in \(\Pi_D\) can depend on the entire history in a very complicated manner, and it is not clear why a deterministic Markov policy should be just as good. I came across this result in a blog post by Maxim Raginsky <span class="bibref"><a href="#raginsky">(Raginsky, 2010)</a></span>. The key result which we will use is from a 1964 paper by David Blackwell <span class="bibref"><a href="#blackwell">(Blackwell, 1964)</a></span>. We state this result without proof.</p>
<div class="thm"><strong>Theorem 8 &#91;Blackwell&#93;</strong>: Let \(\mathsf S,\mathsf T,\) and \(\mathsf A\) be standard Borel spaces, let \(\mathbb Q\) be any probability measure on the product space \(\mathsf S \times \mathsf T\), and let \(R \colon \mathsf S \times \mathsf A \to \mathbb R\) be a bounded measurable reward function. Then for any measurable function \(g \colon \mathsf S \times \mathsf T \to \mathsf A\) there exists another measurable function \(f \colon \mathsf S \to \mathsf A\) such that <div class="nonumber">\[\begin{aligned} \int_{\mathsf S \times \mathsf T} R(x, f(x)) \, \mathbb{Q}(\mathrm{d}x, \mathrm{d}y) \ge \int_{\mathsf S \times \mathsf T} R(x, g(x,y))\, \mathbb{Q}(\mathrm{d}x, \mathrm{d}y).\end{aligned}\]</div></div>
<p>Let us now state and prove our theorem. Instead of fixing an initial state \(x\) as above in the total expected reward function, we let the initial distribution be any probability measure \(\nu\) on \(\mathscr S.\)</p>
<div class="thm"><strong>Theorem 9</strong>: For any policy \(\pi \in \Pi_D\), there exists a policy \(\tau \in \Pi_{DM}\) such that \(J(\tau) \ge J(\pi).\)</div>
<p>Here, \(J(\pi)\), of course, denotes <div class="nonumber">\[\begin{aligned} J(\pi) := \mathbb{E}_\nu^\pi \left[ \sum_{n=0}^{N-1} r(X_n, A_n) \right].\end{aligned}\]</div></p>
<p>Before we prove the theorem, let us establish two lemmas. The first lemma states that the theorem holds true for \(N=2.\)</p>
<div class="thm"><strong>Lemma 2</strong>: If \(N=2\), then for any policy \(\pi = (\pi_0, \pi_1) \in \Pi_D\) there exists a policy \(\tau = (\tau_0, \tau_1) \in \Pi_{DM}\) such that \(J(\tau) \ge J(\pi).\)</div>
<div class="proof">Writing \(J(\pi)\) explicitly, <div class="nonumber">\[\begin{aligned} J(\pi) = \mathbb{E}_\nu^\pi \left[ r(X_0, \pi_0(X_0)) \right] + \mathbb{E}_\nu^\pi \left[ r(X_1, \pi_1(X_0, \pi_0(X_0), X_1)) \right],\end{aligned}\]</div> we note that the first term of RHS does not depend on \(\pi_1.\) Using Blackwell’s theorem for the second term, with \(\mathsf S, \mathsf A\) being the same, \(\mathsf T=\mathsf A \times \mathsf S\), \(g = \pi_1\), \(\mathbb Q\) being the product of \(\nu\) with the kernel determined by the function \(\pi_0\) and the kernel \(\kappa\), as noted in the section of Policy, and \(R=r\), we conclude that there exists a measurable function \(\tau_1 \colon \mathsf S \to \mathsf A\) such that <div class="nonumber">\[\begin{aligned} \mathbb{E}_\nu^\tau \left[ r(X_1,\tau_1(X_1)) \right] \ge \mathbb{E}_\nu^\pi \left[ r(X_1, \pi_1(X_0, \pi_0(X_0), X_1)) \right].\end{aligned}\]</div> The claim follows after noting that we can take \(\tau_0 = \pi_0.\)</div>
<p>The next lemma states that when \(N=3\) and the last policy \(\pi_2\) is Markov, then we can choose even the second policy to be Markov. More precisely,</p>
<div class="thm"><strong>Lemma 3</strong>: If \(N=3\) and \(\pi = (\pi_0, \pi_1, \pi_2) \in \Pi_D\) is such that \(\pi_2 \colon \mathsf S \times \mathsf A \times \mathsf S \times \mathsf A \times \mathsf S \to \mathsf A\) is constant except in the last argument \(\mathsf S\), then there is a policy \(\tau \in \Pi_{DM}\) such that \(J(\tau) \ge J(\pi).\)</div>
<div class="proof">We let \(\tau = (\tau_0, \tau_1, \tau_2) \in \Pi_{DM}\) be such that \(\tau_0 = \pi_0\), \(\tau_2 = \pi_2\), and define \(\tau_1\) as follows. Writing \(J(\pi)\) explicitly, <a id="eq4" class="anchor"></a>\[\begin{aligned} 
J(\pi) = \mathbb{E}_\nu^\pi \left[ r(X_0, \pi_0(X_0)) \right] + \mathbb{E}_\nu^\pi \left[ r(X_1, \pi_1(X_0, \pi_0(X_0), X_1)) \right] + \mathbb{E}_\nu^\pi \left[ r(X_2, \pi_2(X_2)) \right],\end{aligned}\] where we ignored the irrelevant terms in \(\pi_2\), we note that the first term does not depend on \(\pi_1\) and \(\pi_2.\) Since \(X_2\) depends on the action \(\pi_1(X_0, \pi_0(X_0), X_1)\) taken at time \(1\), both the second and the third terms depend on \(\pi_1.\) Focusing on the third term, <div class="nonumber">\[\begin{aligned} \mathbb{E}_\nu^\pi \left[ r(X_2, \pi_2(X_2)) \right] = \mathbb{E}_\nu^\pi \left[\mathbb{E}_\nu^\pi \left[ r(X_2, \pi_2(X_2)) \mid X_1, A_1 \right]\right] =: \mathbb{E}_\nu^\pi \left[h(X_1, A_1)\right].\end{aligned}\]</div> Using <span class="eqref">(<a href="#d2">5</a>)</span> and <span class="eqref">(<a href="#c3a">14</a>)</span> we can write, <div class="nonumber">\[\begin{aligned} h(X_1, A_1) = \int_{\mathsf S} r(x_2, \pi_2(x_2)) \kappa((X_1, A_1), \mathrm{d}x_2).\end{aligned}\]</div> Then the function \(h\) is measurable and bounded. Now define <div class="nonumber">\[\begin{aligned} R(x,a) = r(x,a) + h(x,a),\end{aligned}\]</div> which is also measurable and bounded, and note that by combining the previous results we can write the last two two terms of RHS of <span class="eqref">(<a href="#eq4">17</a>)</span> as <div class="nonumber">\[\begin{aligned} \mathbb{E}_\nu^\pi \left[ r(X_1, \pi_1(X_0, \pi_0(X_0), X_1)) \right] + \mathbb{E}_\nu^\pi \left[ r(X_2, \pi_2(X_2)) \right] = \mathbb{E}_\nu^\pi \left[ R(X_1, \pi_1(X_0, \pi_0(X_0), X_1)) \right].\end{aligned}\]</div> Applying Blackwell’s theorem, with \(\mathsf S, \mathsf A, R\) being the same, \(\mathsf T=\mathsf A \times \mathsf S\), \(g = \pi_1\), and \(\mathbb Q\) being the product of \(\nu\) with the kernel determined by the function \(\pi_0\) and the kernel \(\kappa\), as noted in the section of Policy, we conclude that there exists a measurable function \(\tau_1 \colon \mathsf S \to \mathsf A\) such that <div class="nonumber">\[\begin{aligned} \mathbb{E}_\nu^\tau \left[ R(X_1,\tau_1(X_1)) \right] \ge \mathbb{E}_\nu^\pi \left[ R(X_1, \pi_1(X_0, \pi_0(X_0), X_1)) \right].\end{aligned}\]</div> But note that we can write <div class="nonumber">\[\begin{aligned} \mathbb{E}_\nu^\tau \left[ R(X_1,\tau_1(X_1)) \right] = \mathbb{E}_\nu^\tau \left[ r(X_1, \tau_1(X_0, \pi_0(X_0), X_1)) \right] + \mathbb{E}_\nu^\pi \left[ r(X_2, \tau_2(X_2)) \right],\end{aligned}\]</div> and thus \(J(\tau) \ge J(\pi)\), and the lemma is proved.</div>
<p>Finally, we come to the proof of Theorem 9.</p>
<div class="proof"><p>&#40;of Theorem 9&#41; Let \(\pi \in \Pi_D.\) The cases \(N=1\) and \(N=2\) follow from the lemmas 2 and 3. For \(N \ge 3\) we analyze as follows:</p>
<p>View \(\pi\) as a two-step policy \(((\pi_0, \ldots, \pi_{N-2}), \pi_{N-1})\) in an alternative two-step MDP. Then by Lemma 2, we can assume that \(\pi_{N-1}\) is a Markov policy.</p>
<p>Now view \(\pi\) as a three-step policy \(((\pi_0, \ldots, \pi_{N-3}), \pi_{N-2}, \pi_{N-1})\). The policy in the third time-step \(\pi_{N-1}\) is Markov, and so we can apply Lemma 3 to conclude that \(\pi_{N-2}\) is also Markov.</p>
<p>We continue in a similar manner for smaller indices \(k = N-3, N-4, \ldots\) to get our result.</p></div>
<p>This can be generalized to the class of <em>all</em>, and not necessarily deterministic, policies. I have not verified the proof, but it can be found in <span class="bibref"><a href="#derman">(Derman and Strauch, 1966)</a></span> or Section 3.8 of <span class="bibref"><a href="#dynkin">(Dynkin and Yushkevich, 1979)</a></span>.</p>
<h3 id="dynamic_programming"><a href="#dynamic_programming" class="header-anchor">Dynamic Programming</a></h3>
<p>Suppose, again, that we are in the finite horizon with the expected total reward regime. When can we say that there exists an optimal policy which is a deterministic Markov policy? Making some assumptions with regard to measurable selections, we can find, using dynamic programming, the optimal policy and the value function.</p>
<div class="thm"><strong>Theorem 10</strong>: Define the real-valued functions \(J_N, J_{N-1}, \ldots, J_0\) on \(\mathsf S\) inductively as follows: for each \(x \in \mathsf S,\) <div class="nonumber">\[\begin{aligned}
J_N(x) &:= r_N(x), \\
J_n(x) &:= \sup \left\{\left. r(x,a) + \int_{\mathsf S} J_{n+1}(y) \,\kappa((x,a), \mathrm{d}y) \; \right\vert \; a \in \alpha(x) \right\}, \quad n = N-1, N-2, \ldots, 0.
\end{aligned}\]</div> Suppose that these functions are measurable and that, for each \(n = 0, \ldots, N-1\), there exists a measurable selector \(f_n \colon \mathsf S \to \mathsf A\) satisfying \(f_n(x) \in \alpha(x)\) for all \(x \in \mathsf S\) &#40;or equivalently \(\llbracket f_n \rrbracket \subseteq \llbracket \alpha \rrbracket\)&#41;, and that \(f_n(x)\) attains the maximum in the supremum above, i.e., <div class="nonumber">\[\begin{aligned} J_n(x) = r(x,f_n(x)) + \int_{\mathsf S} J_{n+1}(y) \,\kappa((x, f_n(x)), \mathrm{d}y), \quad \forall \, x \in \mathsf S,\, n = 0, \ldots, N-1.\end{aligned}\]</div> Then the deterministic Markov policy \(\pi^* := (f_0, \ldots, f_{N-1})\) is optimal, and the value function \(J^*\) equals \(J_0.\)</div>
<div class="proof">Let \(\pi = (\pi_0, \ldots, \pi_{N-1}) \in \Pi\) be an arbitrary policy, and for \(n = 0, \ldots, N-1\) let \(R_n(\pi, x)\) be the corresponding expected total cost from time \(n\) to the terminal time \(N\), given that \(X_n = x\). That is, <a id="eq5" class="anchor"></a>\[\begin{aligned} 
R_n(\pi, x) := \mathbb{E}_x^\pi \left[ r(x, A_n) + \sum_{m=n+1}^{N-1} r(X_m, A_m) + r_N(X_N) \right].\end{aligned}\] Also let \(R_N(\pi, x) := r_N(x).\) Note that we have <div class="nonumber">\[\begin{aligned} R_0(\pi, x) = J(\pi, x).\end{aligned}\]</div> To prove the theorem it is sufficient to show that, for all \(x \in \mathsf S\) and \(n = 0, \ldots, N\), <a id="eq6" class="anchor"></a>\[\begin{aligned} 
R_n(\pi, x) \le J_n(x) \text{ and } R_n(\pi^*, x) = J_n(x).\end{aligned}\] <span class="eqref">(<a href="#eq6">19</a>)</span> holds for \(n = N\) by definition. We proceed by induction in the backward direction. Assume that for some \(k \in \{N-1, \ldots, 0\}\), <div class="nonumber">\[\begin{aligned} R_{k+1}(\pi, x) \le J_{k+1}(x), \quad \forall \, x \in \mathsf S.\end{aligned}\]</div> Then by <span class="eqref">(<a href="#eq5">18</a>)</span>, <span class="eqref">(<a href="#d2">5</a>)</span>, <span class="eqref">(<a href="#c2a">13</a>)</span> and <span class="eqref">(<a href="#c3a">14</a>)</span>, <a id="eq7" class="anchor"></a>\[\begin{aligned} 
    R_k(\pi, x) &= \mathbb{E}_x^\pi \left[ r(x, A_k) + \sum_{m=k+1}^{N-1} r(X_m, A_m) + r_N(X_N) \right] \\
    &= \int_{\mathsf A} \left[ r(x,a) + \int_{\mathsf S} R_{k+1}(\pi, y) \kappa((x,a), \mathrm{d}y) \right] \pi_k(x, \mathrm{d}a) \\
    &\le \int_{\mathsf A} \left[ r(x,a) + \int_{\mathsf S} J_{k+1}(y) \kappa((x,a), \mathrm{d}y) \right] \pi_k(x, \mathrm{d}a) \\
    &\le \sup \left\{\left. r(x,a) + \int_{\mathsf S} J_{k+1}(y) \kappa((x,a), \mathrm{d}y) \; \right\vert \; a \in \alpha(x) \right\} \\
    &= J_k(x).\end{aligned}\] This proves the first claim in <span class="eqref">(<a href="#eq6">19</a>)</span> for all \(n = 0, \ldots, N.\) Proceeding in a similar manner for \(\pi = \pi^*\), where the first inequality in <span class="eqref">(<a href="#eq7">20</a>)</span> becomes an equality by the induction hypothesis, and the second inequality in <span class="eqref">(<a href="#eq7">20</a>)</span> becomes an equality by the structure of \(\pi^*\), we conclude the second claim in <span class="eqref">(<a href="#eq6">19</a>)</span> also.</div>
<p>The equation defining \(J_n\)’s in the theorem statement is known as the <em>dynamic programming equation</em>.</p>
<h2 id="epilogue"><a href="#epilogue" class="header-anchor">EPILOGUE</a></h2>
<p>It would be remiss of me to not mention the encyclopaedic reference <span class="bibref"><a href="#shreve">(Bertsekas and Shreve, 1978)</a></span>. I also really liked the book <span class="bibref"><a href="#lerma">(Hernández-Lerma and Lasserre, 1996)</a></span>, which has the advantage of being short. Next, I would be interested in learning reinforcement learning, where MDPs play a foundational role.</p>
<h2 id="references"><a href="#references" class="header-anchor">REFERENCES</a></h2>
<ul>
<li><p><a id="cinlar" class="anchor"></a> Cinlar, E. &#40;2011&#41;. <em>Probability and Stochastics</em>, Graduate Texts in Mathematics 261, Springer.</p>
</li>
<li><p><a id="chang_pollard" class="anchor"></a> Chang, J. T. and Pollard, D. &#40;1997&#41;. <em>Conditioning as disintegration</em>. Statistica Neerlandica, Vol. 51, nr. 3, pp. 287-317.</p>
</li>
<li><p><a id="pollard" class="anchor"></a> Pollard, D. &#40;2010&#41;. <em>A User&#39;s Guide to Measure Theoretic Probability</em>. Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press.</p>
</li>
<li><p><a id="wagner1" class="anchor"></a> Wagner, D. H. &#40;1977&#41;. <em>Survey of Measurable Selection Theorems</em>, SIAM Journal of Control and Optimization Vol. 15, No. 5, August 1977.</p>
</li>
<li><p><a id="wagner2" class="anchor"></a> Wagner, D. H. &#40;1980&#41;. <em>Survey of Measurable Selection Theorems - An update</em>. Measure Theory Oberwolfach 1979, pp. 176-219, Lecture Notes in Mathematics, volume 794, Springer.</p>
</li>
<li><p><a id="aliprantis" class="anchor"></a> Aliprantis, R. and Border K. C. &#40;2006&#41;. <em>Infinite Dimensional Analysis - A Hitchhiker’s Guide</em>. Third Edition, Springer.</p>
</li>
<li><p><a id="reward_survey" class="anchor"></a> Arapostathis A. , Borkar V. S., Fernandez-Gaucherand E., Ghosh M. K. and Marcus S. I. &#40;1993&#41;. <em>Discrete-Time Controlled Markov Processes with Average Cost Criterion - A Survey</em>. SIAM Journal of Control and Optimization Vol. 31, No. 2, pp. 282-344, March 1993.</p>
</li>
<li><p><a id="raginsky" class="anchor"></a> Raginsky, M. &#40;2010&#41;. Blog post - <a href="https://infostructuralist.wordpress.com/2010/11/08/deadly-ninja-weapons-blackwells-principle-of-irrelevant-information/">Deadly ninja weapons: Blackwell’s principle of irrelevant information</a>.</p>
</li>
<li><p><a id="blackwell" class="anchor"></a> Blackwell, D. &#40;1964&#41;. <em>Memoryless Strategies in Finite-Stage Dynamic Programming</em>. Ann. Math. Statist. 35&#40;2&#41;: 863-865 &#40;June, 1964&#41;.</p>
</li>
<li><p><a id="derman" class="anchor"></a> Derman, C. and Strauch, R. E. &#40;1966&#41;. <em>A note on memoryless rules for controlling sequential control processes</em>, Ann. Math. Statist. 37&#40;1&#41;: 276-278 &#40;February, 1966&#41;.</p>
</li>
<li><p><a id="dynkin" class="anchor"></a> Dynkin, E. B. and Yushkevich, A. A. &#40;1979&#41;. <em>Controlled Markov Processes</em>, Springer-Verlag, New York.</p>
</li>
<li><p><a id="shreve" class="anchor"></a> Bertsekas, D. P. and Shreve, S. E. &#40;1978&#41;. <em>Stochastic Optimal Control: The Discrete-Time Case</em>, Academic Press.</p>
</li>
<li><p><a id="lerma" class="anchor"></a> Hernández-Lerma, O and Lasserre, J. B. &#40;1996&#41;. <em>Discrete-Time Markov Control Processes - Basic Optimality Criteria</em>, Springer.</p>
</li>
</ul>
<div class="page-foot">
    <!--<a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> {{ fill author }}. {{isnotpage /tag/*}}Last modified: {{ fill fd_mtime }}.{{end}}
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.-->
</div>
</div><!-- CONTENT ENDS HERE -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
  </body>
</html>
