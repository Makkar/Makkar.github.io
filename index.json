[{"authors":["admin"],"categories":null,"content":"I am a graduate student at Columbia University interested in probabilistic machine learning. I am affiliated with the Electrical Engineering Department and the Data Science Institute and am part of a thriving machine learning community here at Columbia. I am advised by Prof. John Paisley.\nPreviously, I was at Goldman Sachs working with Dr. Howard Karloff on machine learning applications for surveillance models. I received my Bachelors degree from Indian Institute of Technology (IIT) Delhi.\nIf you\u0026rsquo;d like to chat, feel free to write me at a \u0026lsquo;dot\u0026rsquo; makkar \u0026lsquo;at\u0026rsquo; columbia \u0026lsquo;dot\u0026rsquo; edu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://makkar.github.io/author/aditya-makkar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aditya-makkar/","section":"authors","summary":"I am a graduate student at Columbia University interested in probabilistic machine learning. I am affiliated with the Electrical Engineering Department and the Data Science Institute and am part of a thriving machine learning community here at Columbia.","tags":null,"title":"Aditya Makkar","type":"authors"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} $$\nIntroduction This is the second blog post in the series of blog posts on kernels. In the first part I introduced the functional analysis background for kernel theory. I highly recommend you read it before continuing. I will frequently refer to it and use the same notation. In this blog post I aim to introduce the fundamental theorems like Mercer\u0026rsquo;s theorem and Representer theorem.\nCharacterization of reproducing kernels We already defined the notion of reproducing kernels for an RKHS (Definition 19). We now turn our attention to obtaining necessary and sufficient conditions for a function $K : \\X \\times \\X \\to \\C$ to be the reproducing kernel for some RKHS. But first we must define kernel functions.\nKernel functions Let\u0026rsquo;s start by recalling a basic definition.\nDefinition 21: Let $A = (a_{i,j})$ be an $n \\times n$ complex matrix. Then $A$ is called positive if for every $\\alpha_1, \\ldots, \\alpha_n \\in \\C$ we have $$ \\sum_{i,j = 1}^{n} \\conj{\\alpha_i}\\alpha_j a_{i,j} \\geq 0 $$ We denote this by $A \\geq 0$.\nNote that if we define a vector $x \\in \\C^n$ to be such that its $i$th component is $\\alpha_i$, then the condition above can rewritten as $$ \\inner{Ax}{x} \\geq 0 $$\nAlso note that if $A \\geq 0$ then $A = A^* $, where $A^* $ denotes the Hermitian matrix (also known as the self-adjoint matrix), $\\conj{A^T}$. Therefore, positivity gives self-adjoint property for free if we are dealing with complex matrices. Things aren\u0026rsquo;t so elegant for real matrices. For the real case we need to explicitly state that the matrix $A$ is also symmetric apart from what\u0026rsquo;s stated in the definition above. Therefore, we often use the following as the definition of positive matrices:\nDefinition 21\u0026rsquo;: An $n \\times n$ matrix $A$ is positive, in symbols $A \\geq 0$, if it is self-adjoint and if $\\inner{Ax}{x} \\geq 0$ for all $x \\in \\C^n$.\nPositive matrices are also alternatively called  positive semidefinite or nonnegative matrices.\nThe following lemma connects the concept of positive matrices to its eigenvalues.\nLemma 2: A matrix $A \\geq 0$ if and only if $A = A^*$ and every eigenvalue of $A$ is nonnegative.\nProof: Let us first suppose $A \\geq 0$, then $A = A^*$ by definition. Now if $\\lambda$ is an eigenvalue of $A$ and $v$ is an eigenvector of $A$ corresponding to $\\lambda$, we have $$ 0 \\leq \\inner{Av}{v} = \\inner{\\lambda v}{v} = \\lambda \\inner{v}{v} $$ Thus, $\\lambda$ is a nonnegative number.\nFor the other side, find an orthonormal basis $v_1, \\ldots, v_n$ consisting of eigenvectors of $A$ (it exists by the Spectral theorem). Let $v_i$ be the eigenvector corresponding to the eigenvalue $\\lambda_i$. Then for any $x = \\sum_i \\alpha_i v_i$ we have $\\inner{Ax}{x} = \\sum_i \\lambda_i \\modu{\\alpha_i}^2$. Since $\\lambda_i \\geq 0$, $\\inner{Ax}{x} \\geq 0$ and $A$ must be positive. $\\square$\nWe are now ready to define kernel function.\nDefinition 22: Let $\\X$ be a set, then $K : \\X \\times \\X \\to \\C$ is called a kernel function if for every $n \\in \\N$ and for every choice of $\\{x_1, \\ldots, x_n\\} \\subseteq \\X$, the matrix $(K(x_i, x_j)) \\geq 0$. We will use the notation $K \\geq 0$ to denote that the function $K$ is a kernel function.\nKernel functions are alternatively called positive definite functions or positive semidefinite functions.\nDefinition 23: Given a kernel function $K : \\X \\times \\X \\to \\C$ and points $x_1, \\ldots, x_n \\in \\X$, the $n \\times n$ matrix $(K(x_i, x_j))$ is called the Gram matrix of $K$ with respect to $x_1, \\ldots, x_n$.\nSome examples follow:\nExamples  Linear kernels: When $\\X = \\R^d$, we can define the linear kernel function as $$K(x, y) = \\inner{x}{y} $$ It is clearly a symmetric function of its arguments, and hence self-adjoint. To prove positivity, let $x_1, \\ldots, x_n \\in \\R^d$ be an arbitrary collection of points, and consider its gram matrix $\\matr{K}$, i.e., $\\matr{K}_{i,j} = K(x_i, x_j) = \\inner{x_i}{x_j}$. Then for any $\\alpha \\in \\R^n$, we have  $$ \\inner{\\matr{K} \\alpha}{\\alpha} = \\alpha^T \\matr{K}^T \\alpha = \\alpha^T \\matr{K} \\alpha = \\sum_{i,j = 1}^n \\alpha_i \\alpha_j \\inner{x_i}{x_j} = \\left\\lVert \\sum_{i=1}^n \\alpha_i x_i \\right\\rVert^2 \\geq 0 $$\n Polynomial kernels: A natural generalization of the linear kernel on $\\R^d$ is the homogeneous polynomial kernel $$ K(x, y) = (\\inner{x}{y})^m $$ of degree $m \\geq 2$, also defined on $\\R^d$. It is clearly a symmetric function. To prove positivity, note that\n$$ K(x,y) = \\left( \\sum_{i=1}^d x_i y_i \\right)^m $$\nThis will have $D = \\binom{m+d-1}{m}$ monomials, so to simplify the analysis let\u0026rsquo;s take $m=2$. Then\n$$ K(x,y) = \\sum_{i=1}^d x_i^2 y_i^2 + 2 \\sum_{i \u0026lt; j} x_i x_j y_i y_j $$ In this case $D = \\binom{d+1}{d} = d + \\binom{d}{2}$. Define a mapping $\\Phi: \\R^d \\to \\R^D$ such that\n$$ \\Phi(x) = [x_1^2, \\ldots, x_d^2, \\sqrt{2}x_1 x_2, \\ldots, \\sqrt{2} x_{d-1} x_d ]^T $$\nThen\n$$ K(x,y) = \\inner{\\Phi(x)}{\\Phi(y)} $$\nFollowing the same argument as the first example, we can verify that the gram matrix thus formed is positive.\nThe mapping $x \\mapsto \\Phi(x)$ is often referred to as a feature map. We see that dealing with elements in the feature space, i.e. the range of $\\Phi$, is computationally expensive. The relation $K(x,y) = \\inner{\\Phi(x)}{\\Phi(y)}$ allows us compute the inner products using the kernel function instead of actually taking the inner product in a very high dimensional space. We will see that this \u0026ldquo;kernel trick\u0026rdquo; holds for very many kernel functions when we discuss Mercer\u0026rsquo;s theorem.\n  Gaussian kernels: Given some compact subset $\\X \\subseteq \\R^d$, consider the Gaussian kernel\n$$ K(x,y) = \\exp{\\left( -\\frac{1}{2 \\sigma^2} \\norm{x-y}^2 \\right)} $$\nIt is not obvious why this is a kernel function.\n  Equivalence between kernel function and reproducing kernel Let us return to the characterization of reproducing kernels. We will now prove that a function is a kernel function if and only if there is an RKHS for which it is the reproducing kernel. At this point recall Theorem-3 which states that an RKHS admits a unique reproducing kernel.\nTheorem 4: Let $\\X$ be a set and let $\\H$ be an RKHS on $\\X$ with reproducing kernel $K$. Then $K$ is a kernel function.\nProof: For some arbitrary $n \\in \\N$ fix some arbitrary collection $x_1, \\ldots, x_n \\in \\X$ and $\\alpha \\in \\C^n$. Then if we denote by $\\matr{K}$ the gram matrix of $K$ with respect to $x_1, \\ldots, x_n$, we have $$\\inner{\\matr{K} \\alpha}{\\alpha} = \\sum_{i,j = 1}^n \\conj{\\alpha_i}\\alpha_j K(x_i, x_j) = \\sum_{i,j = 1}^n \\conj{\\alpha_i}\\alpha_j \\inner{k_{x_j}}{k_{x_i}} = \\left\\lVert \\sum_{i=1}^n \\alpha_i k_{x_i} \\right\\rVert^2 \\geq 0 \\quad$$\nAnd thus $K$ is a kernel function. $\\square$\nWhat does it mean in the above proof if we have an equality? That is, if $\\inner{\\matr{K} \\alpha}{\\alpha} = 0$? This happens if and only if $\\left\\lVert \\sum_{i=1}^n \\alpha_i k_{x_i} \\right\\rVert = 0$. But this means that for every $f \\in \\H$ we have $\\sum_{i=1}^n \\conj{\\alpha_i} f(x_i) = \\inner{f}{\\sum_i \\alpha_i k_{x_i}} = 0$. Thus, in this case there is an equation of linear dependence between the values of every function in $\\H$ at this finite set of points.\nNow let us state the converse of Theorem-4. It is a deep result in RKHS theory known as the Moore–Aronszajn theorem.\nTheorem 5 [Moore–Aronszajn theorem]: Let $\\X$ be a set and let $K : \\X \\times \\X \\to \\C$ be a kernel function, then there exists a reproducing kernel Hilbert space $\\H$ of functions on $\\X$ such that $K$ is the reproducing kernel of $\\H$.\nFor a proof see here on Wikipedia.\nIn light of these two theorems we have the following notation.\nDefinition 24: Given a kernel function $K : \\X \\times \\X \\to \\C$, we let $\\H(K)$ denote the unique RKHS with the reproducing kernel $K$.\nIt is not an easy problem to start with a kernel function $K$ on some set $\\X$ and give a concrete description of $\\H(K)$. I will not be discussing this here, but there are plenty of resources available where you can see this problem getting discussed. I recommend Paulsen and Raghupathi\u0026rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces.\nMercer\u0026rsquo;s theorem Recall the Spectral theorem for finite-dimensional vector spaces: A linear operator $T: V \\to V$ for some finite dimensional vector space $V$ on $\\C$ is normal, i.e., $T T^* = T^* T$, if and only if $V$ has an orthonormal basis consisting of eigenvectors of $T$. This implies that if $\\matr{U} = [v_1, \\ldots, v_n]$ is a unitary matrix containing the $i$th eigenvector in its $i$th column and $\\matr{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)$ is a diagonal matrix containing the corresponding eigenvalues then if $\\matr{K}$ is a normal matrix then it can written as $$\\matr{K} = \\matr{U} \\matr{\\Lambda} \\matr{U}^T = \\sum_{i=1}^n \\lambda_i v_i v_i^T$$\nMercer\u0026rsquo;s theorem generalizes this decomposition to kernel functions. Let us start by defining a special type of kernel function.\nDefinition 25: Let $\\X$ be a compact metric space. A function $K : \\X \\times \\X \\to \\C$ is called a Mercer kernel if it is a continuous kernel function.\nRecall the space $L^2(\\mu)$ from Definition-8 where we now take $\\X$ (written as $X$ there) to be a compact metric space.\nDefinition 26: Given a Mercer kernel $K : \\X \\times \\X \\to \\C$, we define a linear operator $T_{K} : L^2(\\mu) \\to L^2(\\mu)$ as $$ T_K(f)(x) := \\int_{\\X} K(x, y) f(y) \\dmu(y), \\quad (x \\in \\X) $$\nWe assume that the Mercer kernel satisfies the Hilbert-Schmidt condition, stated as\n$$ \\int_{\\X \\times \\X} \\left\\lvert K(x,y) \\right\\rvert^2 \\dmu(x) \\dmu(y) \u0026lt; \\infty $$\nwhich ensures that $T_K$ is a bounded linear operator on $L^2(\\mu)$. Indeed, we have\n$$ \\lVert T_K(f) \\rVert^{2} = \\int_{\\X} \\left\\lvert \\int_{\\X} K(x, y) f(y) \\dmu(y) \\right\\rvert^2 \\dmu(x) \\leq \\norm{f}^2 \\int_{\\X \\times \\X} \\left\\lvert K(x,y) \\right\\rvert^2 \\dmu(x) \\dmu(y) $$\nwhere we have applied Schwarz inequality (Theorem-1) as follows $$ \\left\\lvert \\int_{\\X} K(x, y) f(y) \\dmu(y) \\right\\rvert^2 = \\left\\lvert T_K(f)(x) \\right\\rvert^2 = \\left\\lvert \\inner{K(x, \\cdot)}{f} \\right\\rvert^2 \\leq \\norm{K(x, \\cdot)}^2 \\norm{f}^2$$\nOperators of this type are known as Hilbert-Schmidt operators.\nWe are now ready to state the Mercer\u0026rsquo;s theorem.\nTheorem 5 [Mercer\u0026rsquo;s theorem]: Suppose that $\\X$ is a compact metric space, and $K : \\X \\times \\X \\to \\C$ is a Mercer\u0026rsquo;s kernel that satisfies the Hilbert-Schmidt condition. Then there exists an at most countable set of eigenfunctions $ (e_{i})_{i} $ for $ T_K $ that form an orthonormal basis of $L^2(\\mu)$, and a corresponding set of non-negative eigenvalues $ (\\lambda_{i})_{i} $ such that\n$$ T_K(e_i) = \\lambda_i e_i, \\quad (i \\in \\N) $$\nMoreover, $K$ has the expansion\n$$ K(x,y) = \\sum_{i} \\lambda_i e_i(x) e_i(y), \\quad (x,y \\in \\X) $$\nwhere the convergence of the series above holds absolutely and uniformly.\nI\u0026rsquo;ll skip the proof.\nAmong other things, Mercer\u0026rsquo;s theorem provides a framework for embedding an element of $\\X$ into an element of $\\ell^2(\\N)$ (for its definition, see Example-4 in the section on Hilbert spaces in the first part). More concretely, given the eigenfunctions and eigenvalues guaranteed by Mercer\u0026rsquo;s theorem, we may define a mapping $\\Phi : \\X \\to \\ell^2(\\N)$ as follows\n$$ x \\mapsto \\left( \\sqrt{\\lambda_i} e_i(x) \\right)_{i \\in \\N} $$\nTherefore, we have\n$$\\inner{\\Phi(x)}{\\Phi(y)} = \\sum_{i=1}^{\\infty} \\lambda_i e_i(x) e_i(y) = K(x,y) $$\nThis is the well-known \u0026ldquo;kernel trick\u0026rdquo;. Let us connect Mercer\u0026rsquo;s theorem to the Spectral theorem.\nLet $\\X = [d] := \\{1, 2, \\ldots, d\\}$ along with the Hamming metric be our compact metric space. Let $\\mu(\\{i\\}) = 1$ for all $i \\in [d]$ be the counting measure on $\\X$. Any function $f : \\X \\to \\C$ is equivalent to the $d$-dimensional vector $[f(1), \\ldots, f(d)]$, and any kernel function $K : \\X \\times \\X \\to \\C$ is continuous, satisfies the Hilbert-Schmidt condition, and is equivalent to a $d \\times d$ normal matrix $\\matr{K}$ where $\\matr{K}_{i,j} = K(i, j)$. The Hilbert-Schmidt operator reduces to\n$$ T_K(f)(x) = \\int_{\\X} K(x, y) f(y) \\dmu(y) = \\sum_{i=1}^{d} K(x,y) f(y) $$\nMercer\u0026rsquo;s theorem then states that there exists a set of eigenfunctions $v_1, \\ldots, v_d$ (for our $\\X$ they are equivalent to vectors) and the corresponding eigenvalues $\\lambda_1, \\ldots, \\lambda_d$ such that\n$$ \\matr{K} = \\sum_{i=1}^{d} \\lambda_i v_i v_i^T $$\nwhich is exactly the spectral theorem.\nOperations on kernels Let us now consider how various algebraic operations on kernels affect the corresponding Hilbert spaces. All this (and a lot more) can be found in the seminal paper by Aronszajn \u0026ldquo;Theory of Reproducing Kernels\u0026rdquo;.\nI state the following theorems without proof to illustrate how operations on kernels is done.\nSums of kernels Theorem 6: Suppose that $\\H_1$ and $\\H_2$ are both RKHSs with kernels $K_1$ and $K_2$, respectively. Then the space\n$$\\H = \\H_1 + \\H_2 := \\{f_1 + f_2 \\, : \\, f_1 \\in \\H_1 \\text{ and } f_2 \\in \\H_2 \\}$$\nwith the norm\n$$ \\norm{f}^{2}_{\\H} := \\inf \\left\\{ \\lVert f_1 \\rVert^{2} _ {\\H_1} + \\norm{f_2}^{2} _{\\H_2} \\, : \\, f = f_1 + f_2, f_1 \\in \\H_1, f_2 \\in \\H_2 \\right\\} $$\nis an RKHS with the kernel $K = K_1 + K_2$.\nProducts of kernels Let us first define the notion of tensor product of two (separable) Hilbert spaces $\\H_1$ and $\\H_2$ of functions, say with domains $\\X_1$ and $\\X_2$.\nDefinition 27: Consider the set of functions $h : \\X_1 \\times \\X_2 \\to \\C$ satisfying\n$$\\H = \\left\\{ h = \\sum_{i=1}^{n} u_i v_i \\, : \\, n \\in \\N \\text{ and } u_i \\in \\H_1, v_i \\in \\H_2 \\text{ for all } i \\in [n] \\right\\} $$\nWe define an inner product on $\\H$ as follows: for $h = \\sum_{i=1}^{n} u_i v_i$ and $g = \\sum_{j=1}^{m} w_j x_j$ in $\\H$ define\n$$ \\inner{h}{g} := \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\inner{u_i}{w_j}_{\\H_1} \\inner{v_i}{x_j}_{\\H_2} $$\nThen $\\H$ is a Hilbert space and is called the tensor product of $\\H_1$ and $\\H_2$. We denote it by $\\H = \\H_1 \\otimes \\H_2$.\nWe can now state the theorem for product of kernels.\nTheorem 7: Suppose that $\\H_1$ and $\\H_2$ are RKHSs of real-valued functions with domains $\\X_1$ and $\\X_2$, and equipped with kernels $K_1$ and $K_2$, respectively. Then the tensor product space $\\H = \\H_1 \\otimes \\H_2$ is an RKHS of functions with domain $\\X_1 \\times \\X_2$, and with kernel function $K : (\\X_1 \\times \\X_2) \\times (\\X_1 \\times \\X_2) \\to \\C$ defined by\n$$ K((x,s), (y,t)) := K_1(x,y) K_2(s,t) $$\n$K$ is called the tensor product of the kernels $K_1$ and $K_2$, and denoted by $K = K_1 \\otimes K_2$.\nOther operations We can similarly define more operations on kernels:\n If $K$ is a valid kernel and $\\alpha \\geq 0$, then $\\alpha K$ is a valid kernel. If $K$ is a valid kernel and $\\alpha \\geq 0$, then $K + \\alpha$ is a valid kernel. We can easily from all these results that a linear combination or more generally for any polynomial $P$ with positive coefficients, the composition $P \\circ K$ is a valid kernel if $K$ is a valid kernel. If $K$ is a valid kernel, then $\\exp(K)$ is a valid kernel.  Representer theorem We are now at a stage where we can put all this theory to use in machine learning. Specifically we will develop Representer theorem which allows many optimization problems over the RKHS to be reduced to relatively simple calculations involving the gram matrix.\nLet us start with a functional analytic viewpoint of supervised learning. Suppose we are given empirical data\n$$ (x_1, y_1), \\ldots, (x_n, y_n) \\in \\X \\times \\Y $$\nwhere $\\X$ is a nonempty set. For now let $\\Y = \\R$. They are from an unknown function, $g : \\X \\to \\R$, i.e., we assume\n$$ y_i = g(x_i), \\quad (i \\in [n]) $$\nWe need to find some function $f^*$ which \u0026ldquo;best\u0026rdquo; approximates $g$. A natural way to formalize the notion of \u0026ldquo;best\u0026rdquo; is to limit ourselves to an RKHS $\\H$ which contains functions of the form $f : \\X \\to \\R$ and choose\n$$ f^* = \\argmin_{f \\in \\H} \\norm{f} \\quad \\text{ such that } f^*(x_i) = y_i \\text{ for } i \\in [n]$$\nThis optimization problem is feasible whenever there exists at least one function $f \\in \\H$ that fits the data exactly. Denote by $y$ the vector $[y_1, \\ldots, y_n]^T$. It can be shown that if $\\matr{K}$ is the gram matrix of the kernel $K$ with respect to $x_1, \\ldots, x_n$ then the feasibility is equivalent to $y \\in \\text{range}(\\matr{K})$. This is a special of the representer theorem.\nIn a realistic setting we assume that we have noisy observations, i.e.,\n$$y_i = g(x_i) + \\e_i, \\quad (i \\in [n])$$\nwhere $\\e_i$'s denote the noise. Then the constraint of exact fitting is no longer desirable, and we model the \u0026ldquo;best\u0026rdquo; approximation by introducing a loss function which represents how close our approximation is to the observed outputs. More concretely, let $L_y : \\R^n \\to \\R$ be a continuous function. Then we can define our cost function as\n$$ J(f) = \\norm{f}^2 + L_y(f(x_1), \\ldots, f(x_n)) $$\nTheorem 8 [Representer theorem]: If $f^*$ is a function such that\n$$ J(f^*) = \\inf_{f \\in \\H} J(f) $$\nthen $f^*$ is in the span of the functions $k_{x_1}, \\ldots, k_{x_n}$, i.e.,\n$$ f^*(\\cdot) = \\sum_{i=1}^{n} \\alpha_i k_{x_i}(\\cdot) \\quad \\text{for some } \\alpha_1, \\ldots, \\alpha_n \\in \\C$$\nI\u0026rsquo;ll skip the proof which can be found in Paulsen and Raghupathi\u0026rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces or in Schölkopf, Herbrich and Smola\u0026rsquo;s A Generalized Representer Theorem.\nAs an example $L$ could be the squared loss $L_y(f(x_1), \\ldots, f(x_n)) = \\sum_{i=1}^n (y_i - f(x_i))^2$. If we assume $L$ to be convex then the solution exists and is unique. Recall that $E \\subseteq \\R^k$ is a convex set if $\\lambda x + (1-\\lambda) y \\in E$ whenever $x,y \\in E$ and $0 \u0026lt; \\lambda \u0026lt; 1$. A real-valued function $L$ defined on a convex set $E$ is a convex function if $L(\\lambda x + (1-\\lambda) y) \\leq \\lambda L(x) + (1-\\lambda) L(y)$ whenever $x, y \\in E$ and $0 \u0026lt; \\lambda \u0026lt; 1$. Note that a convex function is continuous.\nThe Representer theorem allows us to reduce the infinite-dimensional problem of optimizing over an RKHS to an $n$-dimensional problem.\nEpilogue I barely scratched the surface of reproducing kernel Hilbert space theory. Some resources I recommend that do a much better job than me in explaining this theory and which I used as reference are:\n Paulsen V and Raghupathi M: An Introduction to the Theory of Reproducing Kernel Hilbert Spaces Wainwright M: High-Dimensional Statistics Schölkopf B, Herbrich R and Smola A.J: A Generalized Representer Theorem (COLT 2001) Aronszajn N: Theory of Reproducing Kernels (1950)  In the next article (yet to be written) I will discuss kernel approximation methods. In particular I will focus on random Fourier Features developed by Rahimi and Recht which I am using in my work.\n","date":1594930823,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594930823,"objectID":"e6a68300e4479c6d14bd6375724ab9ea","permalink":"https://makkar.github.io/post/kernels1/","publishdate":"2020-07-16T16:20:23-04:00","relpermalink":"/post/kernels1/","section":"post","summary":"Some results in Reproducing Kernel Hilbert Spaces","tags":[],"title":"Kernels - Part 1","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\conj}[1]{\\overline{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} $$\nIntroduction The word \u0026ldquo;kernel\u0026rdquo; is heavily overloaded, but for our purposes it is, intuitively, a similarity measure that can be thought of as an inner product in some feature space. Kernel methods provide an elegant, theoretically well-founded, and powerful approach to solving many learning problems.\nWe usually have the following framework: The input space $\\X$ which contains our observations/inputs/features is either not rich enough (for example, if there is no linear boundary separating the two classes in a binary classification problem) or not convenient (for example, if our inputs are strings), and therefore we want to work is some other space we call feature space $\\H$. Suppose we have a map which takes our inputs from $\\X$ to $\\H$: $$ \\Phi : \\X \\to \\H $$ Then the class of kernels we are interested in are those for which is it possible to write $$ k(x,y) = \\langle \\Phi(x), \\Phi(y) \\rangle, \\quad x,y \\in \\X $$ What kind of functions, $k : \\H \\times \\H \\to \\R$ admit such a representation? We aim to be able to answer this question and many others in this series of articles on kernels.\nIn this article, I aim to introduce the necessary concepts from analysis, linear algebra, and functional analysis so as to understand Reproducing Kernel Hilbert Spaces (RKHS) and a few of their basic properties. In the next article I will discuss the kernel trick and some important theorems like Mercer\u0026rsquo;s theorem and Representer theorem.\nBackground A topic like this requires quite a bit of background before we can get to the interesting results like the one above. It\u0026rsquo;s always easy to get lazy and assume all the necessary background from the reader and get straight to the meat, but I want to write an article which I would have found useful had I had it when I started learning about kernel theory. With that being said, I am under no illusion and believe that a much better way to learn this background would be to read a functional analysis text if you have the time.\nLinear spaces Definition 1: A linear space, or alternatively a vector space, over a field $\\F$ ($\\F$ is $\\R$ or $\\C$ for our purposes) is a set $V$ of elements called vectors (the elements of $\\F$ are called scalars) satisfying:\n(A) To every pair, $x$ and $y$, of vectors in $V$ there corresponds a vector $x+y$, called the sum of $x$ and $y$, in such a way that\n addition is commutative, $x+y = y+x$, addition is associative, $x+(y+z) = (x+y)+z$, there exists in $V$ a unique vector $0$ such that $x+0=x$ for every vector $x$, and to every vector $x \\in V$ there corresponds a unique vector $-x$ such that $x+(-x)=0$.  (B) To every pair, $\\alpha \\in \\F$ and $x \\in V$, there corresponds a vector $\\alpha x \\in V$, called the product of $\\alpha$ and $x$, in such a way that\n multiplication by scalars is associative, $\\alpha(\\beta x) = (\\alpha \\beta)x$, and $1x = x$ for every vector $x$.  (C) Finally the distributive properties\n $\\alpha(x+y) = \\alpha x + \\alpha y$, and $(\\alpha + \\beta) x = \\alpha x + \\beta x$.  Examples  The Euclidean spaces $\\R^n$ are vector spaces over the real field. $\\C^n$ are vector spaces over $\\C$. The set of all polynomials, with complex coefficients, in a variable $t$ is a vector space over $\\C$. The set $C$ of all continuous complex functions on the unit interval $[0,1]$ is a vector space over $\\C$.  Definition 2: A linear transformation of a linear space $V$ into a linear space $W$ is a mapping $T: V \\to W$ such that $$ T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y), \\quad (x,y \\in V;\\; \\alpha, \\beta \\in \\F) $$\nDefinition 3: In the special case in which $W$ above is a field, $T$ is called a linear functional.\nNote that we often write $Tx$ instead of $T(x)$, if $T$ is linear.\nDefinition 4: Let $\\mu$ be a positive measure on an arbitrary measurable space $X$. We define $L^1(\\mu)$ to be the collection of all complex measurable functions $f$ on $X$ for which $$ \\int_X |f| \\dmu \u0026lt; \\infty $$\nIt can be shown that for every $f, g \\in L^1(\\mu)$ and for every $\\alpha, \\beta \\in \\C$, we have $\\alpha f + \\beta g \\in L^1(\\mu)$, and $$ \\int_X (\\alpha f + \\beta g) \\dmu = \\alpha \\int_X f \\dmu + \\beta \\int_X g \\dmu $$\nThus, $L^1(\\mu)$ is a vector space, and the mapping $F: L^1(\\mu) \\to \\R$ defined by $$ F(f) = \\int_X |f| \\dmu, \\quad f \\in L^1(\\mu) $$ is a linear functional.\nInner products and norms Definition 5: If $V$ be a linear space over $\\C$, an inner product on $V$ is a function $\\langle \\cdot , \\cdot \\rangle: V \\times V \\to \\C$ such that for all $\\alpha, \\beta \\in \\C$, and all $x,y,z \\in V$, the following are satisfied:\n Linearity in the first argument: $\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x,z \\rangle + \\beta \\langle y,z \\rangle$, Conjugate symmetry: $\\inner{x}{y} = \\conj{\\inner{y}{x}}$, Positivity: $\\inner{x}{x} \\geq 0$, If $\\inner{x}{x} = 0$, then $x=0$.  A function satisfying only the first three properties is called a semi-inner product on $V$.\nAn immediate consequences of this definition: For every $y \\in V$, the mapping $F: V \\to \\C$ defined by $$ F(x) = \\inner{x}{y}, \\quad (x \\in V) $$ is a linear functional on $V$.\nDefinition 6: If $V$ be a linear space over $\\C$, a norm on $V$ is a non-negative function $\\norm{\\cdot} : V \\to \\R$ such that for all $\\alpha \\in \\C$, and all $x,y \\in V$, the following are satisfied:\n Subadditivity: $\\norm{x+y} \\leq \\norm{x} + \\norm{y}$, Absolutely homogenous: $\\norm{\\alpha x} = |\\alpha| \\norm{x}$, Positive definite: $\\norm{x} = 0 \\implies x = 0$.  Given an inner product, we can define a norm as follows: $$ \\norm{x} = \\sqrt{\\inner{x}{x}} $$\nA classic result useful in many proofs:\nTheorem 1: Schwarz inequality $$|\\inner{x}{y}| \\leq \\norm{x} \\, \\lVert y \\rVert$$ Equality hold for $y = \\alpha x$ or $y=0$.\nThe proof is not too difficult.\nDefinition 7: The virtue of norm on a vector space $V$ is that $$ d(x,y) := \\norm{ x-y } $$ defines a metric on $V$ so that $V$ becomes a metric space.\nNote that technically the tuple $(V, d)$ defines a metric space, but I will often write just $V$ instead of $(V, d)$ when it\u0026rsquo;s clear from context what the metric $d$ is.\nDefinition 8: If $0 \u0026lt; p \u0026lt; \\infty$, $f$ is a complex measurable function on $X$, and $\\mu$ is a nonnegative measure on $X$, define $$ \\norm{f}_p := \\left( \\int_X |f|^p \\dmu \\right)^{1/p} $$ and let $L^p(\\mu)$ consist of all $f$ for which $\\norm{f}_p \u0026lt; \\infty$. We call $\\norm{f}_p$ the $L^p$-norm of $f$.\nHilbert spaces Definition 9: An inner product space, or alternatively a pre-Hilbert space, is a linear space with an inner product defined on it.\nWe need the concept of completeness to define Hilbert space. But before that let me define Cauchy sequences.\nDefinition 10: Given a metric space $(M, d)$, a sequence $(x_n)_{n \\in \\N}$ of elements in $M$ is called a Cauchy sequence if for every positive real number $\\e \u0026gt; 0$ there exists a positive integer $N \\in \\N$ such that $m, n \u0026gt; N$ implies that $d(x_m, x_n) \u0026lt; \\e$.\nRecall that we say a sequence $(x_n)_{n \\in \\N}$ in a metric space $(M, d)$ converges if there exists a point $x \\in M$ with the following property: for every $\\e \u0026gt; 0$ there exists a positive integer $N \\in \\N$ such that $n \u0026gt; N$ implies that $d(x_n, x) \u0026lt; \\e$.\nNow we are ready to define the notion of completeness.\nDefinition 11: A metric space $(M, d)$ is called complete if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.\nDefinition 12: A pre-Hilbert space $\\H$ is called a Hilbert space if it is complete in the metric $d$ (see Definition 7).\nExamples   The sets $\\R^n$ and $\\C^n$ are Hilbert spaces if we define $\\inner{x}{y} := \\sum_{i=1}^n x_i \\conj{y_i}$.\n  The set $C$ of all continuous complex functions on the unit interval $[0,1]$ defined above is an inner product space if we define $$ \\inner{f}{g} := \\int_0^1 f(x) \\conj{g(x)} \\dx $$ but is not a Hilbert space.\n  $L^2(\\mu)$ is a Hilbert space, with inner product $$ \\inner{f}{g} := \\int_X f \\, \\conj{g} \\dmu $$\n  The space of square-summable real-valued sequences, namely $$ \\ell^2(\\N) := \\left\\{ (x_n)_{n \\in \\N} \\; : \\; x_n \\in \\R,\\, \\sum_n x_n^2 \u0026lt; \\infty \\right\\} $$\nThis set, when endowed with the inner product $\\inner{x}{y} := \\sum_{n \\in \\N} x_n y_n$, defines a Hilbert space. It will play an important role in our discussion of eigenfunctions for Reproducing Kernel Hilbert spaces.\n  Definition 13: Consider a linear space $\\FF$ of functions each of which is a mapping from a set $X$ into $\\F$. For $x \\in X$, a linear evaluation functional is a linear functional $E_x$ that is defined as $$ E_x(f) = f(x), \\quad (f \\in \\FF) $$ In other words, a linear evaluation function with respect to $x \\in X$ evaluates each function at $x$.\nIn general, the evaluation functional is not continuous. This means we can have $f_n \\to f$ but $E_x(f_n)$ does not converge to $E_x(f)$. Intuitively, this is because Hilbert spaces can contain very unsmooth functions. We will later consider a special type of Hilbert space, Reproducing Kernel Hilbert Space where evaluation functional is continuous.\nA lemma that will be useful later on:\nLemma 1: Let $\\H$ be a Hilbert space and $L:\\H \\to \\F$ a linear functional. The following statements are equivalent:\n $L$ is continuous. $L$ is continuous at $0$. $L$ is continuous at some point. $L$ is bounded, i.e., there is a constant $c \u0026gt; 0$ such that $|L(f)| \\leq c \\norm{f}$ for every $f \\in \\H$.  Proof: It is clear that $(1) \\implies (2) \\implies (3)$, and $(4) \\implies (2)$. Let\u0026rsquo;s show that $(3) \\implies (1)$, and $(2) \\implies (4)$.\n$(3) \\implies (1)$: Suppose $L$ is continuous at $f$ and $g$ is any point in $\\H$. If $g_n \\to g$ in $\\H$, then $g_n - g + f \\to f$. By assumption $L(f) = \\limn L(g_n - g + f) = \\limn L(g_n) - L(g) + L(f)$. Hence $L(g) = \\limn L(g_n)$.\n$(2) \\implies (4)$: The definition of continuity at $0$ implies that $L^{-1}(\\{\\alpha \\in \\F : |\\alpha| \u0026lt; 1\\})$ contains an open ball centered at $0$. Let $\\delta \u0026gt; 0$ be the radius of that open ball centered at $0$. Then for $f \\in \\H$ and $\\norm{f} \u0026lt; \\delta$ we have $|L(f)| \u0026lt; 1$. If $f$ is an arbitrary element of $\\H$ and $\\e \u0026gt; 0$, then $$ \\left\\lVert \\frac{\\delta f}{\\norm{f} + \\e} \\right\\rVert \u0026lt; \\delta $$ Hence, $$ 1 \u0026gt; \\left\\lvert L\\left( \\frac{\\delta f}{\\norm{f} + \\e} \\right) \\right\\rvert = \\frac{\\delta }{\\norm{f} + \\e} |L(f)| $$ Letting $\\e \\to 0$ we see that $(4)$ holds with $c = 1/\\delta$. $\\square$\nOrthonormal bases We generalize the idea of orthonormal basis to infinite dimensional case. This will be needed when we discuss Mercer\u0026rsquo;s theorem.\nDefinition 14: A collection of vectors $\\{v_{\\alpha} \\, : \\, \\alpha \\in A \\}$ in a Hilbert space $\\H$ for some index set $A$ is called orthonormal if it satisfies $\\inner{v_{\\alpha}}{v_{\\beta}} = \\delta_{\\alpha \\beta}$ where $\\delta_{\\alpha \\beta}$ is the Kronecker delta, which equals $1$ if $\\alpha = \\beta$ and $0$ otherwise.\nDefinition 15: A collection of vectors $\\{v_{\\alpha} \\, : \\, \\alpha \\in A \\}$ in a Hilbert space $\\H$ is complete if for any $u \\in \\H$, $\\inner{u}{v_{\\alpha}} = 0$ for all $\\alpha \\in A$ implies that $u = 0$.\nDefinition 16: An orthonormal basis a complete orthonormal system.\nNote, we can also define an orthonormal basis as a maximal orthonormal set in $\\H$. To say $\\{v_{\\alpha}\\}$ is maximal means that no vector of $\\H$ can be added to $\\{v_{\\alpha}\\}$ in such a way that the resulting set is still orthonormal. This happens precisely when there is no $u \\neq 0$ in $\\H$ that is orthogonal to every $v_{\\alpha}$.\nSeparable Hilbert spaces Another idea we need is separability. Let us define that now.\nDefinition 17: A topological space is called separable if it contains a countable, dense subset; that is, there exists a sequence $(x_{n})_{n=1}^{\\infty }$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.\nDefinition 18: A Hilbert space is separable if and only if it has a countable orthonormal basis. It follows that any separable, infinite-dimensional Hilbert space is isometric to the space $\\ell^2(\\N)$ of square-summable sequences.\nWe will be dealing with separable Hilbert spaces in our discussion.\nRiesz representation theorem We now come to a very important theorem called the Riesz representation theorem. The name Riesz has many theorems attached to it, but the one relevant to us is the following:\nTheorem 2: For each continuous linear functional $L$ on a Hilbert space $\\H$, there exists a unique $g \\in \\H$ such that $$L(f) = \\inner{f}{g}\\, , \\quad (f \\in \\H) $$\nI\u0026rsquo;ll skip the proof as it\u0026rsquo;s not easy and will unnecessarily make this article abstruse.\nSide-note: In the mathematical treatment of quantum mechanics, this theorem can be seen as a justification for the popular bra–ket notation.\nReproducing Kernel Hilbert Spaces (RKHS) Definition 19: Let $\\X$ be a set. We will call a set $\\H$ of functions from $\\X$ to $\\F$ a Reproducing Kernel Hilbert Space (RKHS) on $\\X$ if\n $\\H$ is a vector space, $\\H$ is endowed with an inner product, $\\inner{\\cdot}{\\cdot}$, with respect to which $\\H$ is a Hilbert space, for every $x \\in \\X$, the linear evaluation functional $E_x : \\H \\to \\F$, is bounded (or continuous, as seen from Lemma-1).  If $\\H$ is an RKHS on $\\X$, then an application of the Riesz representation theorem shows that the linear evaluation functional is given by the inner product with a unique vector in $\\H$. Therefore, for each $x \\in \\X$, there exists a unique vector $k_x \\in \\H$, such that for every $f \\in \\H$, $$ f(x) = E_x(f) = \\inner{f}{k_x} $$\nDefinition 20: The function $k_x$ is called the reproducing kernel for the point $x$. The function $K: \\X \\times \\X \\to \\F$ defined by $$ K(x,y) = k_y(x) $$ is called the reproducing kernel for $\\H$.\nNote that we have $$ K(x,y) = k_y(x) = \\inner{k_y}{k_x} = \\conj{\\inner{k_x}{k_y}} = \\conj{K(y,x)}$$\nAlso, $$ \\norm{E_y}^2 = \\norm{k_y}^2 = \\inner{k_y}{k_y} = K(y,y) $$\nExample The first question that comes to mind is if any Reproducing Kernel Hilbert Spaces exist. The following example answers this question in the affirmative.\nWe saw before that $\\C^n$ is a Hilbert space. We can show that $\\C^n$ is in fact an RKHS. Let $\\X = \\{1, 2, \\ldots, n\\}$, then we can view $v \\in \\C$ as a function $V : \\X \\to \\C$, where $V(j) = v_j$. The linear evaluation functionals are of course bounded for every $x \\in \\X$ and we have $$ V(j) = v_j = \\inner{V}{e_j}\\,, \\quad (j \\in \\X) $$ where $e_j$ is a vector with $1$ at $j$th position and $0$ everywhere else. Therefore, the reproducing kernel for the point $x \\in \\X$ is $e_x$ and the reproducing kernel can be thought as the identity matrix.\nCan there be multiple reproducing kernels for an RKHS? The following theorem answers this question.\nTheorem 3: If an RKHS $\\H$ of functions on a set $\\X$ admits a reproducing kernel, $K$, then $K$ is uniquely determined by $\\H$.\nProof: Suppose that there exists another reproducing kernel $K'$ for $\\H$. Then $$ \\norm{k_y - k\u0026rsquo;_y} = \\inner{k_y - k\u0026rsquo;_y}{k_y - k\u0026rsquo;_y} = \\inner{k_y - k\u0026rsquo;_y}{k_y} - \\inner{k_y - k\u0026rsquo;_y}{k\u0026rsquo;_y} = (k_y - k\u0026rsquo;_y)(y) - (k_y - k\u0026rsquo;_y)(y) = 0 $$ for any $y \\in \\X$. In other words, $k_y(x) = k\u0026rsquo;_y(x)$ for every $x \\in \\X$ by the positive definite property of norms and hence the kernel is unique. $\\square$\nEpilogue We covered quite a lot of ground in this blog post but I didn\u0026rsquo;t even define a kernel as we commonly use in machine learning! In the next post I will do that and cover its fundamental properties.\nSome resources I recommend to go into more depth on what\u0026rsquo;s covered here are:\n Halmos, P: Finite-Dimensional Vector Spaces. Rudin, W: Real and Complex Analysis. (Chapter - 4) Rudin, W: Functional Analysis. Conway, J: A course in functional analysis.  ","date":1593826510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593826510,"objectID":"0947cda7e02dd4e13d4c0959ccd45212","permalink":"https://makkar.github.io/post/kernels/","publishdate":"2020-07-03T21:35:10-04:00","relpermalink":"/post/kernels/","section":"post","summary":"Functional analysis background for kernel theory.","tags":[],"title":"Kernels - Part 0","type":"post"},{"authors":[],"categories":[],"content":"I want to show how these two concepts are a single mathematical idea.\nPartition A partition of a non-empty set, $X$, is a disjoint class $\\{X_i\\}_{i \\in I}$ of non-empty subsets of $X$ whose union is $X$. The $X_i$'s are called the partition sets.\nFor example, if $X = \\mathbb{R}$, then $X$ can be partitioned as $$ X = \\bigcup_{n \\in \\mathbb{Z}}[n, n+1) $$\nBinary Relation A binary relation, or simply relation, $R$, in the set $X$ is a subset of $X \\times X$.\nFor $x, y \\in X$, we denote the fact $(x,y) \\in R$ by writing $x R y$. A function may be defined as a special kind of binary relation.\nEquivalence relation Let\u0026rsquo;s assume that a partition of our non-empty set $X$ is given, and we associate with this partition a relation, $\\sim$, in $X$ defined as follows: $x \\sim y$ if $x$ and $y$ belong to the same partition set. It can easily checked that the relation $\\sim$ satisfies:\n $x \\sim x$ for every $x \\in X$ (reflexivity); $x \\sim y \\implies y \\sim x$ (symmetry); $x \\sim y$ and $y \\sim z$ $\\implies$ $x \\sim z$ (transitivity).  Any relation in $X$ which possesses these three properties is called an equivalence relation in $X$.\nExamples  Let $X = \\mathbb{Z}$ and let $x \\sim y$ if $2 | x-y$ for $x,y \\in X$. Then clearly $\\sim$ is an equivalence relation in $X$. Let $X$, $Y$ be any non-empty sets and $f$ be a mapping from $X$ onto $Y$. Let $x \\sim y$ if $f(x) = f(y)$ for $x,y \\in A$. This defines an equivalence relation in $X$. Indeed, $f(x) = f(x)$, and so $x \\sim x$. If $f(x) = f(y)$ then $f(y) = f(x)$, and so $x \\sim y \\implies y \\sim x$. Finally, if $f(x) = f(y)$ and $f(y) = f(z)$ then $f(x) = f(z)$, and so $x \\sim y$ and $y \\sim z$ $\\implies$ $x \\sim z$. The first example is a special case of this one if we take $X = \\mathbb{Z}$, $Y = \\{0,1\\}$ and $f(x) = x \\mod 2$.  \u0026ldquo;Relation\u0026rdquo; to partition We have just seen that each partition of $X$ has associated with it a natural equivalence relation in $X$. Let us now reverse the situation and show that a given equivalence relation in $X$ determines a natural partition of $X$.\nLet $\\sim$ be an equivalence relation in $X$. For every $x \\in X$ define the set $$ [x] := \\{ y \\in X : y \\sim x\\} $$ called the equivalence set of $x$. We show that the class of all distinct equivalence sets forms a partition of $X$.\nBy reflexivity, $x \\in [x]$ for every $x \\in X$, and thus each equivalence set is non-empty and their union is $X$. We now need to show that any two equivalence sets $[x_1]$ and $[x_2]$ are either disjoint or identical. We prove this by showing that if $[x_1]$ and $[x_2]$ are not disjoint then they are identical. To this end, let $z$ be a common element of $[x_1]$ and $[x_2]$. Let $y$ be any element of $[x_1]$. Using transitivity, $$ y \\sim x_1 \\sim z \\sim x_2 $$ Therefore, $y \\in [x_2]$. Since $y$ was an arbitrary element of $[x_1]$, we get $[x_1] \\subseteq [x_2]$. We can similarly show that $[x_2] \\subseteq [x_1]$. In short, $[x_1] = [x_2]$.\nWe have shown that there is no real distinction between partitions of a set and equivalence relations in the set. They are two \u0026ldquo;equivalent\u0026rdquo; approaches for the same mathematical idea. The approach we choose in an application depends entirely on our own convenience.\n","date":1592695005,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592695005,"objectID":"d1bbc0efd74ed683d7616f54612bc205","permalink":"https://makkar.github.io/post/equivalence-relations/","publishdate":"2020-06-20T19:16:45-04:00","relpermalink":"/post/equivalence-relations/","section":"post","summary":"I want to show how these two concepts are a single mathematical idea.","tags":[],"title":"Partitions and Equivalence Relations","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limm}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sn}{\\{s_n\\}} \\newcommand{\\snk}{\\{s_{n_k}\\}} \\newcommand{\\supk}{\\sup_{k \\geq n}} \\newcommand{\\infk}{\\inf_{k \\geq n}} \\newcommand{\\sups}{\\sup_{k \\geq n} s_k} \\newcommand{\\infs}{\\inf_{k \\geq n} s_k} \\newcommand{\\cc}{\\mathsf{c}} $$\nMost of what follows is taken from Rudin\u0026rsquo;s Principles of Mathematical Analysis.\nIntroduction The concept of upper and lower limits (commonly denoted by $\\limsup$ and $\\liminf$ respectively) shows up routinely when discussing the limiting behaviour of a sequence. For example, consider the Big O notation, $\\mathcal{O}$, defined as $f(n) = \\mathcal{O}(g(n))$ iff there exists a positive real number $c$ and an integer $N$ such that for every $n \\in \\Z$, $n \u0026gt; N$, we have $|f(n)| \\leq c g(n)$. This can be succinctly written as $$ \\lims \\frac{|f(n)|}{g(n)} \u0026lt; \\infty $$\nIn this post, I aim to expound on the concept of upper and lower limits so that the second formulation of Big O notation above becomes easier to understand than the first one.\nDefinitions I start with some definitions.\nDefinition 1: A sequence is function defined on the set $\\N$ of positive integers. If $f(n) = x_n$, for $n \\in \\N$, we denote the sequence $f$ by the symbol $\\{x_n\\}$, or sometimes by $x_1, x_2, \\ldots$.\nDefinition 2: A sequence $\\{p_n\\}$ in a metric space $X$ is said to converge if there is a point $p \\in X$ with the following property: For every $\\e \u0026gt; 0$ there is an integer $N$ such that $n \\geq N$ implies that $d(p_n, p) \u0026lt; \\e$. We write this as $\\limm p_n = p$ or as $p_n \\to p$.\nDefinition 3: Let $\\{s_n\\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\geq M$. We then write $s_n \\to +\\infty$. Similarly for $s_n \\to -\\infty$.\nNote: The symbol $\\to$ is now used for certain type of divergent sequences as well.\nDefinition 4: Given a sequence $\\{p_n\\}$, consider a sequence $\\{n_k\\}$ of positive integers, such that $n_1 \u0026lt; n_2 \u0026lt; \\cdots$. Then the sequence $\\{p_{n_k}\\}$, which is a composition of the functions $\\{n_k\\}$ and $\\{p_n\\}$, is called a subsequence of ${p_n}$. If $\\{p_{n_k}\\}$ converges, its limit is called a subsequential limit of $\\{p_n\\}$.\nDefinition 5: Let $\\sn$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system, i.e., $x \\in \\RR := \\R \\cup \\{+\\infty, -\\infty\\}$) such that $s_{n_k} \\to x$ for some subsequence $\\{s_{n_k}\\}$. Therefore, this set contains all the subsequential limits of $\\sn$ plus possibly the numbers $+\\infty$ and $-\\infty$. We define $$ s^* = \\sup E $$ $$ s_* = \\inf E $$ The numbers $s^*$ and $s_*$ are called the upper and lower limits of $\\sn$. We use the notation $$ \\lims s_n = s^* $$ $$ \\limi s_n = s_* $$ It immediately follows that $s_* \\leq s^*$.\nThe fact that $E$ is non-empty (and thus taking $\\sup$ or $\\inf$ makes sense) follows from the observation that either $\\sn$ is bounded or unbounded. If it is bounded then it must contain a convergent subsequence (Bolzano–Weierstrass theorem) and thus at least one element, or if it is unbounded then it must contain either $+\\infty$ or $-\\infty$.\nSome useful lemmas Now let us prove some interesting lemmas that will be useful later.\nLemma 1: The subsequential limits of a sequence $\\{p_n\\}$ in a metric space $X$ form a closed subset of $X$.\nProof: Let $E$ be the set of all subsequential limits of $\\{p_n\\}$ and let $q$ be a limit point of $E$. We have to show that $q \\in E$. To show this we will construct a subsequence of $\\{p_n\\}$ which converges to $q$.\nChoose $n_1$ so that $p_{n_1} \\neq q$. If no such $n_1$ exists, then $E$ has only one element, $q = p_1 = p_2 = \\cdots$, and there is nothing to prove. Define $\\delta = d(q, p_{n_1})$. Suppose $n_1, \\ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E$, there is an $x \\in E$ with $d(q, x) \u0026lt; \\frac{\\delta}{2^i}$. Since $x \\in E$, there is an $n_i \u0026gt; n_{i-1}$ such that $d(x, p_{n_i}) \u0026lt; \\frac{\\delta}{2^i}$. Thus $$ d(q, p_{n_i}) \\leq d(q, x) + d(x, p_{n_i}) \u0026lt; \\frac{\\delta}{2^{i-1}} \\quad \\text{for } i = 1,2,\\ldots $$ This implies $p_{n_k} \\to q$, and thus $q \\in E$. $\\square$\nLemma 2: Let $F$ be a nonempty closed set of real numbers which is bounded above. Let $\\alpha = \\sup F$. Then $\\alpha \\in F$.\nProof: Assume for the sake of the contradiction that $\\alpha \\notin F$. Then since $F^\\cc$ is an open set (because $F$ is closed) there exists an $\\e \u0026gt; 0$ such that $(\\alpha - \\e, \\alpha + \\e) \\subset F^\\cc$. But this implies $\\alpha - \\frac{\\e}{2}$ is an upper bound for $F$ which is lower that $\\alpha$. This gives us our required contradiction. $\\square$\nProperties We now have all the tools to prove a very useful characterization of upper and lower limits and the highlight of this post.\nTheorem 1: Let $\\sn$ be a sequence of real numbers. Let $E$ and $s^*$ have the same meaning as in Definition 5. Then $s^*$ has the following two properties:\n $s^* \\in E$. If $x \u0026gt; s^*$, there is an integer $N$ such that $n \\geq N$ implies $s_n \u0026lt; x$.  Moreover, $s^*$ is the only number with these two properties.\nOf course, an analogous result is true for $s_*$.\nProof: We start by showing the two properties.\n  We divide it into three cases depending on what value $s^*$ takes:\nIf $s^* = +\\infty$, then $E$ is not bounded above; hence $\\sn$ is not bounded above, and thus there is a subsequence $\\{s_{n_k}\\}$ such that $s_{n_k} \\to +\\infty$. Therefore, $+\\infty \\in E$ and thus $s^* \\in E$.\nIf $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists by the definition of $\\sup$. Therefore, $s^* \\in E$ follows from the Lemmas 1 and 2, and the fact that $s^* = \\sup E$.\nIf $s^* = -\\infty$, then $E$ contains only one element, namely $-\\infty$, and there is no subsequential limit. Thus, $s^* \\in E$.\n  Suppose for the sake of contradiction that there is a number $x \u0026gt; s^*$ such that $s_n \\geq x$ for infinitely many values of $n$. Let\u0026rsquo;s denote this set of $n$'s with $\\mathcal{K}$ and let $\\{s_k\\}$$_{k \\in \\mathcal{K}}$ be this subsequence. If $\\{s_k\\}_{k \\in \\mathcal{K}}$ is unbounded then $s^* = +\\infty$ contradicting the fact that there exists an $x \u0026gt; s^*$. And if $\\{s_k\\}_{k \\in \\mathcal{K}}$ is bounded that it contains a convergent subsequence (Bolzano–Weierstrass theorem). Suppose this convergent subsequence converges to $y$. Then $y \\geq x \u0026gt; s^*$. This contradicts the definition of $s^*$.\n  To show the uniqueness, suppose there are two distinct numbers, $p$ and $q$, which satisfy the two properties, and suppose $p \u0026lt; q$. Choose $x$ such that $p \u0026lt; x \u0026lt; q$. Since $p$ satisfies the second property, we have $s_n \u0026lt; x$ for $n \\geq N$. But then $q$ cannot satisfy the second property. $\\square$\nAn intuitive theorem:\nTheorem 2: If $s_n \\leq t_n$ for $n \\geq N$, where $N \\in \\N$ is fixed, then $$\\limi s_n \\leq \\limi t_n,$$ $$\\lims s_n \\leq \\lims t_n$$\nProof: Let $s^* = \\lims s_n$ and $t^* = \\lims t_n$. Suppose for the sake of contradiction $t^* \u0026lt; s^*$. Choose $x$ such that $t^* \u0026lt; x \u0026lt; s^*$. Then by the second property of Theorem 1 there is an integer $N_1$ such that $n \\geq N_1$ implies $t_n \u0026lt; x$. Also by the first property there exists a subsequence $\\snk$ such that $s_{n_k} \\to s^*$. This implies that there exists an integer $N_2$ such that $n \\geq N_2$ implies $x \u0026lt; s_n$. But then for $n \\geq \\max\\{N_1, N_2\\}$ we have $t_n \u0026lt; x \u0026lt; s_n$. This gives us our required contradiction.\nA similar argument can be made for the $\\liminf$ case. $\\square$\nNext we give a necessary and sufficient condition for the convergence of a sequence in terms of its $\\liminf$ and $\\limsup$.\nTheorem 3: For a real-valued sequence $\\sn$, $\\limm s_n = s \\in \\RR$ if and only if $$\\lims s_n = \\limi s_n = s$$\nProof: We divide the analysis into three cases.\nFirst, let $s \\in \\R$. Then if $\\lims s_n = \\limi s_n = s$, Theorem 1 implies that for any $\\e \u0026gt; 0$ we have $s_n \\in (s-\\e, s+\\e)$ for all but finitely many $n$, which means $s_n \\to s$. On the other hand if $s_n \\to s$ then every subsequence $\\snk$ must converge to $s$ and hence $\\lims s_n = \\limi s_n = s$.\nNow let $s = +\\infty$. Then $s_n \\to s$, i.e., for every $M \\in \\R$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\geq M$ if and only $\\limi s_n = +\\infty$, and then $\\lims s_n = +\\infty$ since $\\limi s_n \\leq \\lims s_n$.\nLastly, let $s = -\\infty$. Then $s_n \\to s$, i.e., for every $M \\in \\R$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\leq M$ if and only $\\lims s_n = -\\infty$, and then $\\limi s_n = -\\infty$ since $\\limi s_n \\leq \\lims s_n$. $\\square$\nUpper and lower limits - a reprise There is an equivalent way to express upper and lower limits.\nDefinition 6: Let $\\sn$ be a sequence of real numbers. We define the notation $$\\sups := \\sup \\{ s_k : k \\geq n\\}$$ $$\\infs := \\inf \\{ s_k : k \\geq n\\}$$\nWe note that the sequence $\\{ \\sups \\}$$_{n \\in \\N}$ is monotonically decreasing and the sequence $\\{ \\infs \\}$$_{n \\in \\N}$ is monotonically increasing, and thus their limits exist in $\\RR$.\nWe now show the equivalence of the two ways of looking at upper and lower limits.\nTheorem 4: Let $\\sn$ be a sequence of real numbers. Then $$\\limm \\sups = \\lims s_n$$ $$\\limm \\infs = \\limi s_n$$\nProof: We will prove the first equation. The proof for the second is similar. We prove the equation in two steps.\nLet $S = \\{ s_n : n \\in \\N \\}$. Let\u0026rsquo;s show that $\\sup S = +\\infty$ if and only if $\\lims s_n = + \\infty$. Suppose first that $\\sup S = + \\infty$. Then we construct a subsequence $\\snk$ as follows. We let $n_1 = 1.$ Suppose $n_1, \\ldots, n_{k}$ are chosen and let $$S_k = \\{ n \\in \\N : s_n \\geq \\max\\{s_{n_1}, \\ldots, s_{n_k}, k\\} + 1 \\}$$ Notice that $S_k$ is infinite as otherwise we can find an $M \\in \\R$ such that $s_n \\leq M$ for all $n \\geq 1$, contradicting the fact that $\\sup S = + \\infty$. We pick $n_{k+1}$ to be the smallest element of $S_k$ which is bigger than $n_k$. The resulting subsequence satisfies the condition that $s_{n_k} \\geq k$ for $k \\geq 2$ and thus we conclude that $s_{n_k} \\to +\\infty$ which gives $\\lims s_n = +\\infty$. Now suppose that $\\lims s_n = +\\infty$. From Theorem 1 we can conclude that there exists a subsequence $\\snk$ such that $s_{n_k} \\to +\\infty$. This immediately implies $\\sup S = +\\infty$.\nFor the second step, suppose $\\sup S \u0026lt; + \\infty$. Let $$a_n = \\sups$$ Notice that $a_1 \u0026lt; +\\infty$ and $\\{a_n\\}$ is a monotonically decreasing sequence. Therefore, we have that either $\\{a_n\\}$ is lower bounded in which case it converges to, say, $a$ or it is not in which case $\\limm a_n = -\\infty$. Since $s_n \\leq a_n$, by Theorem 2 we can conclude $$ \\lims s_n \\leq \\lims a_n = \\limm a_n $$ where the last equality follows from Theorem 3. We will now show that $$ \\lims s_n \\geq \\limm a_n $$ which will give us our required equality. If $\\limm a_n = -\\infty$ there is nothing to prove and so we assume that $a \u0026gt; -\\infty$. Let $\\e \u0026gt; 0$ be given and let $$ B = \\{ n \\in \\N : s_n \\geq a - \\e \\} $$ We claim that $B$ is infinite. Indeed, if $B$ were finite we can find $N \\in \\N$ so that $N \\geq \\max(B)$. This will imply that $s_n \\leq a - \\e$ for all $n \\geq N$ and so $a_n \\leq a - \\e$ for $n \\geq N$. But then by Theorem 2 we would conclude $a = \\limm a_n \\leq a-\\e$, which is absurd. Thus $B$ is infinite and we let $\\snk$ be a subsequence of $\\sn$ with $n_k \\in B$. Notice that $\\lims s_{n_k} \\leq \\lims s_n$, since any subsequential limit of $\\snk$ is also a subsequential limit of $\\sn$. This along with Theorem 2 give $$ a - \\e \\leq \\lims s_{n_k} \\leq \\lims s_n $$ Since $\\e$ was arbitrary we conclude that $\\lims s_n \\geq a$, which is what we wanted. $\\square$\nMore properties These theorems open new avenues to discover more properties of upper and lower limits.\nTheorem 5: Let $\\sn$ be a sequence of real numbers. Then $$\\limi s_n = - \\lims (-s_n)$$\nProof: TODO\nSubadditivity of $\\limsup$:\nTheorem 6: For any two real sequences $\\{a_n\\}$ and $\\{b_n\\}$, $$ \\lims (a_n + b_n) \\leq \\lims a_n + \\lims b_n $$ provided the sum on the right is not of the form $\\infty - \\infty$.\nProof: If $\\lims a_n = \\infty$ then as by assumption the right side is not $\\infty - \\infty$, it is $\\infty$ and there is nothing to prove. Similarly for the case $\\lims b_n = \\infty$. We may thus assume that $$ \\lims a_n = A \u0026lt; \\infty \\text{ and } \\lims b_n = B \u0026lt; \\infty $$ We note that $\\sup_{k \\geq 1} a_k \u0026lt; \\infty$, $\\sup_{k \\geq 1} b_k \u0026lt; \\infty$, and so TODO\nSuperadditivity of $\\liminf$:\nTheorem 7: For any two real sequences $\\{a_n\\}$ and $\\{b_n\\}$, $$ \\limi (a_n + b_n) \\geq \\limi a_n + \\limi b_n $$ provided the sum on the right is not of the form $\\infty - \\infty$.\nProof: TODO\n","date":1592426033,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592426033,"objectID":"4fb8d43db098b4713ceef892d1d72774","permalink":"https://makkar.github.io/post/upper-and-lower-limits/","publishdate":"2020-06-17T16:33:53-04:00","relpermalink":"/post/upper-and-lower-limits/","section":"post","summary":"A reference for a useful concept.","tags":[],"title":"Upper and Lower Limits","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c7634463bc503571b81622dfbdaae48f","permalink":"https://makkar.github.io/hilbert/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/hilbert/","section":"","summary":"Blog","tags":null,"title":"Hilbert's Hotel","type":"widget_page"},{"authors":null,"categories":null,"content":"Title: Bayesian nonparametric ensemble Date: June 17, 2020 Tags: bayesian Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).\nIntroduction $$ \\mathbb{E}[X] = \\int_{\\Omega} X ;\\text{dP} $$\nProblem Setting BNE ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"79a589979f0bc90d50d8da1239fec4e1","permalink":"https://makkar.github.io/research/bayesian-ensemble/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/bayesian-ensemble/","section":"research","summary":"Title: Bayesian nonparametric ensemble Date: June 17, 2020 Tags: bayesian Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).","tags":null,"title":"","type":"research"}]