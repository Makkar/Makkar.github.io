[{"authors":["admin"],"categories":null,"content":"I am a graduate student at Columbia University interested in probabilistic machine learning. I am affiliated with the Electrical Engineering Department and the Data Science Institute and am part of a thriving machine learning community here at Columbia. I am advised by Prof. John Paisley.\nPreviously, I was at Goldman Sachs working with Dr. Howard Karloff on machine learning applications for surveillance models. I received my Bachelors degree from Indian Institute of Technology (IIT) Delhi.\nIf you\u0026rsquo;d like to chat, feel free to write me at a \u0026lsquo;dot\u0026rsquo; makkar \u0026lsquo;at\u0026rsquo; columbia \u0026lsquo;dot\u0026rsquo; edu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://makkar.github.io/author/aditya-makkar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aditya-makkar/","section":"authors","summary":"I am a graduate student at Columbia University interested in probabilistic machine learning. I am affiliated with the Electrical Engineering Department and the Data Science Institute and am part of a thriving machine learning community here at Columbia.","tags":null,"title":"Aditya Makkar","type":"authors"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\ZZ}{\\Z_{\\geq 0}^N} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\NN}{\\mathcal{N}} \\newcommand{\\LL}{\\mathcal{L}} \\newcommand{\\PP}{\\mathbb{P}} \\newcommand{\\OO}{\\mathcal{O}} \\newcommand{\\I}{\\mathcal{I}} \\newcommand{\\Prob}[1]{\\PP \\left( #1 \\right)} \\newcommand{\\eq}[1]{\\begin{align*}#1\\end{align*}} \\newcommand{\\eql}[1]{\\begin{align}#1\\end{align}} \\newcommand{\\ind}[1]{\\mathbf{1}_{#1}} \\newcommand{\\indo}[1]{\\mathbf{1}_{#1}(\\omega)} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\G}{\\mathcal{G}} \\newcommand{\\EE}{\\mathcal{E}} \\newcommand{\\probsp}{(\\Omega, \\F, \\PP)} \\newcommand{\\integ}[1]{\\int_{\\Omega} #1 \\dmu} \\newcommand{\\B}{\\mathcal{B}} \\newcommand{\\Bo}{\\B(\\R)} \\newcommand{\\Bon}[1]{\\B(\\R^{#1})} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sumn}{\\sum_{n=1}^{\\infty}} \\newcommand{\\trans}{\\intercal} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\ex}[1]{\\exp\\left{#1\\right}} \\newcommand{\\comp}{\\mathsf{c}} \\newcommand{\\emp}{\\varnothing} \\newcommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor} \\newcommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil} \\newcommand{\\se}[1]{\\left\\{ #1 \\right\\}} \\newcommand{\\set}[2]{\\left\\{ #1 ; : ; #2 \\right\\}} \\newcommand{\\sett}[2]{\\left\\{ #1 ; | ; #2 \\right\\}} \\newcommand{\\Ex}[1]{\\E\\left[#1\\right]} \\newcommand{\\pard}[2]{\\frac{\\partial #1}{\\partial #2}} \\newcommand{\\dd}{\\mathrm{d}} \\newcommand{\\ph}[1]{\\varphi^{-1}\\left(#1\\right)} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dy}{dy} \\DeclareMathOperator{\\du}{du} \\DeclareMathOperator{\\dz}{d\\matr{z}} \\DeclareMathOperator{\\dt}{dt} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator{\\dP}{d\\PP} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\Ord}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} \\DeclareMathOperator{\\V}{\\mathrm{Var}} \\DeclareMathOperator{\\Log}{\\mathrm{Log}} \\DeclareMathOperator{\\O}{\\mathcal{O}} $$\nIntroduction Finding a longest increasing subsequence is a well-known problem in computer science (note that I use the article \u0026ldquo;a\u0026rdquo; instead of \u0026ldquo;the\u0026rdquo; because there could be multiple longest subsequences): Given a sequence $\\se{a_1, \\ldots, a_n}$ of real numbers, we want to find a subsequence $\\se{a_{i_1}, \\ldots, a_{i_k}}$ such that $0 \\le i_1 \u0026lt; \\cdots \u0026lt; i_k \\le n$, $a_{i_1} \\le \\cdots \\le a_{i_k}$, and the subsequence is as long as possible. It has a very easy dynamic programming solution with a time complexity of $\\O(n^2)$ and a slightly more involved solution with a time complexity of $\\O(n \\log n)$. But we are not interested in these algorithms in this blog post.\nWe are interested in studying the asymptotics of the length of the longest increasing subsequences of a sequence whose elements are coming from a random permutation. This simple to state problem will take us on a beautiful journey touching topics from combinatorics and probability theory. In particular, we will get to see the very elegant technique of Poissonization.\nProblem Let us start by stating precisely what are trying to prove. To do that we first define some notation. For any integer $n \\ge 1$, let $S_n$ be the group of permutations of order $n$, i.e., it contains all permutations of $\\se{1, 2, \\ldots, n}$ and hence $S_n$ contains $n!$ elements. If $\\pi \\in S_n$ then a subsequence of $\\pi$ is a sequence $\\se{\\pi(i_1), \\ldots, \\pi(i_k)}$ such that $1 \\leq i_1 \u0026lt; \\cdots \u0026lt; i_k \\leq n$. It is an increasing subsequence if $\\pi(i_1) \u0026lt; \\cdots \u0026lt; \\pi(i_k)$ and similarly for the decreasing subsequence. Consider the uniform measure $\\mu_n$ on the discrete measurable space $(S_n, 2^{S_n})$, i.e., $\\mu_n(\\pi) = 1 / n!$ for any $\\pi \\in S_n$. For $\\pi \\in S_n$ define $l_n(\\pi)$ to be the maximal length of an increasing subsequence of $\\pi$, i.e., $l_n(\\pi)$ is the largest $k$ such that there are integers $1 \\leq i_1 \u0026lt; \\cdots \u0026lt; i_k \\leq n$ so that $\\pi(i_1) \u0026lt; \\cdots \u0026lt; \\pi(i_k)$. Similarly define $d_n(\\pi)$ to be the maximal length of a decreasing subsequence of $\\pi$.\nSince $l_n$ is a random variable we can consider its expectation $L_n = \\E[l_n]$ on the probability space $(S_n, 2^{S_n}, \\mu_n)$. It can be explicitly written as $$ L_n = \\frac{1}{n!} \\sum_{\\pi \\in S_n} l_n(\\pi) \\tag{1} \\label{eq1} $$ We want to study the limiting properties of the sequence $\\se{L_n}_{n \\in \\N}$. Specifically we will show that $$ \\frac{L_n}{\\sqrt{n}} \\to \\gamma \\text{ almost surely} \\tag{2} \\label{eq2} $$ for some constant $\\gamma$. It is known that $\\gamma = 2$. We will not be showing this, but we will show that $1 \\le \\gamma \\le e$.\nCombinatorial results We now prove some useful combinatorial results. The first result is called the Erdős–Szekeres theorem.\nTheorem 1 [Erdős–Szekeres theorem]:  In any sequence $\\se{a_1, a_2, \\ldots, a_{mn+1}}$ of $mn+1$ distinct real numbers, there exists either an increasing sequence $a_{i_1} \u0026lt; \\cdots \u0026lt; a_{i_{m+1}}$ $(i_1 \u0026lt; \\cdots \u0026lt; i_{m+1})$ of length $m+1$, or a decreasing sequence $a_{j_1} \u0026gt; \\cdots \u0026gt; a_{j_{n+1}}$ $(j_1 \u0026lt; \\cdots \u0026lt; j_{n+1})$ of length $n+1$.\nProof:  For $1 \\leq i \\leq mn+1$ define $t_i$ to be the length of a longest increasing subsequence starting at $a_i$, i.e., the first element of a longest increasing subsequence must be $a_i$ and the rest of the $a$'s must have indices greater than $i$. If $t_i \\geq m+1$ for some $i$ then we are done since we then have an increasing subsequence of length $m+1$, so assume $t_i \\leq m$ for all $i$. Since $t_i \\geq 1$, pigeonhole principle now implies that there is some integer $1 \\leq k \\leq m$ such that $t_i = k$ for at least $n+1$ $i$'s. Let $t_i = k$ for all $i \\in (j_1 \u0026lt; \\cdots \u0026lt; j_{n+1})$. Now note that if $a_{j_l} \u0026lt; a_{j_{l+1}}$ for some $1 \\leq l \\leq n$, then we would obtain an increasing subsequence of length $k+1$ starting at $a_{j_l}$ because there is an increasing subsequence of length $k$ starting at $a_{j_{l+1}}$. But this contradicts the fact that $t_{j_l} = k,$ and thus $a_{j_l} \u0026gt; a_{j_{l+1}}$ for all $1 \\leq l \\leq n$. But now this gives us a decreasing subsequence $a_{j_1} \u0026gt; \\cdots \u0026gt; a_{j_{n+1}}$ of length $n+1$. $\\square$\nThe next two theorems prove lower and upper bounds for $L_n / \\sqrt{n}$.\nTheorem 2:  $L_n / \\sqrt{n}$ is lower bounded as follows $$ L_n \\geq \\sqrt{n} \\text{ for all } n \\geq 1 \\tag{3} \\label{eq3} $$\nProof:  Theorem 1 implies $l_n(\\pi) d_n(\\pi) \\geq n$ for all $\\pi \\in S_n$. Note that for every permutation $\\pi \\in S_n$ there exists an inverse permutation $\\pi'$ such that $l_n(\\pi) = d_n(\\pi\u0026rsquo;)$ and thus we can write $L_n$ also as $$ L_n = \\frac{1}{n!} \\sum_{\\pi \\in S_n} d_n(\\pi) \\tag{4} \\label{eq4} $$ Therefore, averaging the two ways of computing the expectation $L_n$ (Equations \\eqref{eq1} and \\eqref{eq4}) and using the AM-GM inequality we have $$ L_n = \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\frac{l(\\pi) + d(\\pi)}{2} \\geq \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\sqrt{l(\\pi) d(\\pi)} \\geq \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\sqrt{n} = \\sqrt{n} \\qquad \\square $$\nTheorem 3:  Upper bound: $$ \\lims \\frac{L_n}{\\sqrt{n}} \\leq e \\tag{5} \\label{eq5} $$\nProof:  If $\\pi \\in S_n$ and $1 \\leq k \\leq n$ let $X_{n,k}(\\pi)$ be the number of increasing subsequences of $\\pi$ which are of length $k$. They are exactly those subsets $S \\subseteq \\se{1, \\ldots, n}$ such that $|S| = k$ and if $S = \\se{i_1 \u0026lt; \\cdots \u0026lt; i_k}$ then $\\pi(i_1) \u0026lt; \\cdots \u0026lt; \\pi(i_k)$. On the probability space $(S_n, 2^{S_n}, \\mu_n)$, by the linearity of expectation, the expected value of $X_{n,k}$ is given by the number of ways to select $k$ subsets of $\\se{1, \\ldots, n}$ times the probability that a selection has all the elements in the increasing order. The number of ways is simply $\\binom{n}{k}$ and the probability is $1/k!$ and thus $$ \\E[X_{n,k}] = \\frac{1}{k!} \\binom{n}{k} $$ The Taylor expansion of $e^x$ implies $e^x \\ge x^k / k!$. Substituting $x = k$ we get $k! \\geq (k/e)^k$. Therefore, we get $$ \\E[X_{n,k}] = \\frac{1}{k!} \\binom{n}{k} = \\frac{n(n-1) \\cdots (n-k-1)}{(k!)^2} \\leq \\frac{n^k}{(k/e)^{2k}} $$\nNow for a discrete random variable $X_{n,k}$ we can write $\\E[X_{n,k}] = \\sum_{i=0}^n \\mu_n(X_{n,k} \\geq i)$ and thus $\\mu_n(X_{n,k} \\geq 1) \\leq \\Ex{X_{n,k}}$. Also note that $l_n(\\pi) \\geq k$ if and only if $X_{n,k}(\\pi) \\geq 1$. We thus get $$ \\mu_n(l_n \\geq k) = \\mu_n(X_{n,k} \\geq 1) \\leq \\E[X_{n,k}] \\leq \\frac{n^k}{(k/e)^{2k}} $$ Fix an arbitrary $\\delta \u0026gt; 0$ and let $k = \\ceil{(1+\\delta)e\\sqrt{n}}$ in the inequality above to get $$ \\mu_n(l_n \\geq k) \\leq \\frac{n^k}{(k/e)^{2k}} \\leq \\left( \\frac{1}{1+\\delta} \\right)^{2k} \\leq \\left( \\frac{1}{1+\\delta} \\right)^{2(1+\\delta)e\\sqrt{n}} $$ Since $l_n \\leq n$, we have $$ L_n = \\Ex{l_n} \\leq \\mu_n(l_n \u0026lt; k) (1+\\delta)e\\sqrt{n} + \\mu_n(l_n \\geq k) n \\leq (1+\\delta)e\\sqrt{n} + O(e^{-c\\sqrt{n}}) $$ where $c$ is some positive constant that depends on $\\delta$. Since $\\delta$ was arbitrary, we can let $\\delta \\to 0$ and then take $\\limsup$ to get Equation \\eqref{eq5}. $\\square$\nPoissonization To be able to show \\eqref{eq2} we will draw a correspondence between this problem of longest increasing subsequences and a seemingly unrelated problem called the Poissonized version. This Poissonized version will allow us to use the powerful Subadditive Ergodic Theorem (Theorem 5 below) to show \\eqref{eq2}.\nTo this end, assume an underlying probability space $\\probsp$ and let $N$ be a Poisson random measure on $\\mathbb{R} _ + ^2$ with mean measure given by the Lebesgue measure on $\\mathbb{R} _ + ^2$. In other words $N:\\Omega \\times \\B(\\R _ + ^2) \\to \\bar{\\R} _ +$ is a transition kernel from $(\\Omega, \\F)$ into $(\\R_+^2, \\B(\\R_+^2))$ with $\\int_\\Omega \\PP(\\dd \\omega) N(\\omega, A) = \\text{Leb}(A)$ for any $A \\in \\B(\\R_+^2)$. We can view the random process as a sequence $\\se{(X_i, Y_i)} _ {i \\geq 1}$ of independent random variables taking values in $\\R _ + ^2$ and having a uniform probability measure (Lebesgue measure on $(\\R_+^2, \\B(\\R_+^2))$). If we let $R_{s,t}$ denote the rectangle with vertices $(s,s), (s,t), (t,t)$ and $(t,s)$, then for each outcome $\\omega \\in \\Omega$, we can think of having $\\text{Poisson}(\\text{Leb}(R_{s,t}(\\omega)))$ distributed number of such points inside the rectangle $R_{s,t}(\\omega).$\nFor $s \u0026lt; t \\in [0, \\infty)$ let $Z_{s,t}$ be the random variable denoting the length of the longest increasing path lying in the rectangle $R_{s,t}$, i.e., $Z_{s,t}$ is the largest integer $k$ for which there are points $(X_1,Y_1), \\dots, (X_k, Y_k)$ in the Poisson process with $s \u0026lt; X_1 \u0026lt; \\cdots \u0026lt; X_k \u0026lt; t$ and $s\u0026lt; Y_1 \u0026lt; \\cdots \u0026lt; Y_k \u0026lt; t$.\nLet $\\tau(n)$ be the smallest value of $t \\in [0, \\infty)$ for which there are $n$ points in $R_{0,t}$. Let the $n$ points in $R_{0, \\tau(n)}$ be written as $\\se{(X_i, Y_i)}_{1 \\leq i \\leq n}$ such that $0 \u0026lt; X_1 \u0026lt; \\cdots \u0026lt; X_n \\leq \\tau(n)$ (the inequalities are strict almost surely since they have continuous distributions). Let $\\pi_n \\in S_n$ be the unique permutation such that $Y_{\\pi_n(1)} \u0026lt; \\cdots \u0026lt; Y_{\\pi_{n}(n)}$ (It is not difficult to see the existence and the uniqueness). Then $$ \\pi_n \\text{ is a uniformly random sample of } S_n \\text{ , and } Z_{0, \\tau(n)} = l_n(\\pi_n) \\tag{6} \\label{eq6} $$ The second claim is obvious from the definition of $\\pi_n$. The first claim is equivalent to showing that if $U_1, \\ldots, U_n$ are independent random variables sampled from a uniform distribution on $[0,1]$ and if $U_{(1)}, \\ldots, U_{(n)}$ are the order statistics, i.e., $U_{(k)}$ is the $k$th smallest among $U_1, \\ldots, U_n$, then the probability that $U_{(1)}, \\ldots, U_{(n)}$ is same as $U_{\\pi(1)}, \\ldots, U_{\\pi(n)}$ for any $\\pi \\in S_n$ is $1/n!$. But this is obvious from the independence of $U_i$'s. The next theorem is from Durrett:\nTheorem 4:  $\\tau(n) / \\sqrt{n} \\to 1$ almost surely.\nProof:  Let $S_n$ be the number of points in $R_{0,\\sqrt{n}}$. Since $(R_{0, \\sqrt{n}} \\setminus R_{0, \\sqrt{n-1}}) \\cap (R_{0, \\sqrt{m}} \\setminus R_{0, \\sqrt{m-1}}) = \\emp$ for $n \\neq m$, the definition of a Poisson random measure implies $\\se{S_n - S_{n-1}}_{n \\geq 1}$ are independent Poisson random variables with mean $1$. The strong law of large numbers now implies $S_n / n \\to 1$ almost surely. For any $\\e \u0026gt; 0$ we can find an $n$ large enough such that $S_{n(1-\\e)} \u0026lt; n \u0026lt; S_{n(1+\\e)}$ but then this means $\\sqrt{n(1-\\e)} \\leq \\tau(n) \\leq \\sqrt{n(1+\\e)}$ which is same as the statement of the theorem since $\\e$ was arbitrary. $\\square$\nThe last theorem along with \\eqref{eq6} implies $Z_{0, \\sqrt{n}} \\to L_n$ almost surely and thus $$ \\frac{Z_{0,n}}{n} \\to \\frac{L_{n^2}}{n} \\text{ almost surely} \\tag{7} \\label{eq7} $$\nBack to longest increasing subsequences Having established the connection between the two ways of looking at the problem of longest increasing subsequences, we now freely jump between the two characterizations and use them to prove our results.\nDefine $W_{s,t} = - Z_{s,t}$. We now check that $W_{m,n}, 0 \\le m \u0026lt; n$ satisfies the conditions required for the Subadditive Ergodic Theorem. I state the theorem below for completeness. Check out Theorem 6.4.1 in Durrett for a proof.\nTheorem 5 [Subadditive Ergodic Theorem]:  Suppose $W_{m,n}, 0 \\le m \u0026lt; n$ satisfy:\n $W_{0,m} + W_{m,n} \\ge W_{0,n}$, $\\se{W_{nk, (n+1)k}, n \\ge 1}$ is a stationary sequence for each $k$, The distribution of $\\se{W_{m,m+k}, k \\ge 1}$ does not depend on $m$, $\\Ex{W_{0,1}^+} \u0026lt; \\infty$ and $\\inf_{n \\ge 1} \\frac{1}{n} \\Ex{W_{0,1}} = \\beta \u0026gt; - \\infty$.  Then\n $\\limn \\frac{1}{n} \\Ex{W_{0,1}} = \\beta$, $W = \\limn \\frac{1}{n} W_{0,1}$ exists almost surely and in $L^1$, and $\\Ex{X} = \\beta$, If all stationary sequences in 2. are ergodic, then $W = \\beta$ almost surely.  Coming back to the problem, let $0 \u0026lt; m \u0026lt; n$ then we claim $Z_{0,m} + Z_{m,n} \\leq Z_{0,n}$. To see this fix $\\omega \\in \\Omega$ and let $Z_{0,m}(\\omega) = a$ and $Z_{m,n}(\\omega) = b$. Then there exist $(X_1(\\omega), Y_1(\\omega)), \\ldots (X_a(\\omega), Y_a(\\omega)), (X_{a+1}(\\omega), Y_{a+1}(\\omega)), \\ldots, (X_{a+b}(\\omega), Y_{a+b}(\\omega))$ such that $0 \u0026lt; X_1(\\omega) \u0026lt; \\cdots \u0026lt; X_a(\\omega) \u0026lt; m$, $0 \u0026lt; Y_1(\\omega) \u0026lt; \\cdots \u0026lt; Y_a(\\omega) \u0026lt; m$ and $m \u0026lt; X_{a+1}(\\omega) \u0026lt; \\cdots \u0026lt; X_{a+b}(\\omega) \u0026lt; n$, $m \u0026lt; Y_{a+1}(\\omega) \u0026lt; \\cdots \u0026lt; Y_{a+b}(\\omega) \u0026lt; n$. But then it\u0026rsquo;s clear that $0 \u0026lt; X_1(\\omega) \u0026lt; \\cdots \u0026lt; X_{a+b}(\\omega) \u0026lt; n$, $0 \u0026lt; Y_1(\\omega) \u0026lt; \\cdots \u0026lt; Y_{a+b}(\\omega) \u0026lt; n$ and we have $Z_{0,m}(\\omega) + Z_{m,n}(\\omega) \\leq Z_{0,n}(\\omega)$. Since $\\omega$ was arbitrary equation our claim is true. Therefore, $W_{0,m} + W_{m,n} \\ge W_{0,n}$ and condition 1. is true.\nFor condition 2. we want to show that $\\se{W_{nk, (n+1)k, n \\geq 1}}$ is a stationary and ergodic sequence for all $k \\geq 1$. This is clear from the observation that $Z_{ik, (i+1)k} \\stackrel{d}{=} Y_{0,k}$ since $\\text{Leb}(R_{ik, (i+1)k}) = \\text{Leb}(R_{0,k})$ and thus by the definition of the Poisson random measure the number of points in each rectangle is an i.i.d. Poisson random variable. Checking the condition 3. is similar to condition 2..\nFor condition 4. note that $W_{0,1}^+ = Z_{0,1}$ and $Z_{0,1} \\leq \\text{Poisson}(1)$ since there are $\\text{Poisson}(1)$ number of points inside $[0,1]^2$ and at most all of them can be arranged in the increasing order. Thus, $\\Ex{W_{0,1}^+} \u0026lt; \\Ex{\\text{Poisson}(1)} = 1 \u0026lt; \\infty$. Now note that since $W_{0,n} = - Z_{0,n}$ $$ \\inf_{n \\geq 1} \\frac{1}{n} \\Ex{W_{0,n}} = - \\sup_{n \\geq 1} \\frac{1}{n} \\Ex{Z_{0,n}} $$ and thus to show the second part of condition 4. we need to show that $\\sup_{n \\geq 1} \\frac{1}{n} \\Ex{Z_{0,n}} \u0026lt; \\infty$. But this is immediate from Equation \\eqref{eq5} in Theorem 3 and Equation \\eqref{eq7}. Therefore, the subadditive ergodic theorem now implies $$ \\frac{Z_{0,n}}{n} \\to \\gamma \\text{ a.s.} $$ where $\\gamma$ from Equations \\eqref{eq3} and \\eqref{eq5} lies in $[1,e]$ and we are done since this implies \\eqref{eq2}.\nEpilogue I got introduced to this problem from an exam question in a math course I took recently. I recommend the book \u0026ldquo;The Surprising Mathematics of Longest Increasing Subsequences\u0026rdquo; by Dan Romik, which is freely available online, for a lot more content.\n","date":1608934628,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608934628,"objectID":"870f9e68745844b26ccf4093462dc6f7","permalink":"https://makkar.github.io/post/lisprob/","publishdate":"2020-12-25T17:17:08-05:00","relpermalink":"/post/lisprob/","section":"post","summary":"A probabilistic analysis","tags":[],"title":"Longest Increasing Subsequence","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\PP}{\\mathbb{P}} \\newcommand{\\eq}[1]{\\begin{align*}#1\\end{align*}} \\newcommand{\\eql}[1]{\\begin{align}#1\\end{align}} \\newcommand{\\ind}[1]{\\mathbf{1}_{#1}} \\newcommand{\\indo}[1]{\\mathbf{1}_{#1}(\\omega)} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\probsp}{(\\Omega, \\F, \\PP)} \\newcommand{\\integ}[1]{\\int_{\\Omega} #1 \\dmu} \\newcommand{\\B}{\\mathcal{B}} \\newcommand{\\Bo}{\\B(\\R)} \\newcommand{\\Bon}[1]{\\B(\\R^{#1})} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sumn}{\\sum_{n=1}^{\\infty}} \\newcommand{\\trans}{\\intercal} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\comp}{\\mathsf{c}} \\newcommand{\\emp}{\\varnothing} \\newcommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor} \\newcommand{\\se}[1]{\\left{ #1 \\right}} \\newcommand{\\set}[2]{\\left{ #1 ; : ; #2 \\right}} \\newcommand{\\sett}[2]{\\left{ #1 ; | ; #2 \\right}} \\newcommand{\\Ex}[1]{\\E\\left[#1\\right]} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dy}{dy} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator{\\dpr}{d\\PP} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\Ord}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} $$\nI recently came across this simple to state puzzle:\n Suppose you have a coin that has probabilities $p$ for heads and $1-p$ for tails. You play the following game with a friend. The first player picks one of the outcomes $HH$, $TH$, $HT$ and $TT$. The second player observes the choice of the first player and picks one of the remaining 3 outcomes. You then proceed to toss the coin infinitely many times independently constructing an infinite sequence of $H$’s and $T$’s. A player wins if their choice appears first in the sequence. For example, if player one chose $HH$ and player two chose $TT$ then $HTHTHTTHTHHH\u0026hellip;$ results in a victory for player two since $TT$ occurred before $HH$ in the sequence. When is it better to go first in this game?\n Let\u0026rsquo;s make the conditions under which a player wins more concrete.\nThe strategy:  It’s better to go as the first player in the game if for some choice $a \\in \\{HH,TT,HT,TH\\}$, the probability of winning the game is greater than $1/2$ no matter the choice of the second player. It’s better to go as the second player in the game if for every choice of the first player, we can find some element of the set $\\{HH,TT,HT,TH\\} \\setminus \\{a\\}$, where $a$ is the choice of the first player, such that the probability of winning the game is greater than $1/2$.\nAs an example, suppose $p = 3/4$, then if the first player chooses $HH$, then no matter the choice of the second player, the probability of the first player winning the game is greater than $1/2$. To see this note that the probability of getting two consecutive heads in the first two coin tosses itself is $9/16 \u0026gt; 1/2$.\nOn first impression it seems as if it must be better to go as the first player, no matter the value of $p$, since the first player has more choices, but as we will see, the fact that the second player has the advantage of choosing the outcome after observing the choice of the first player gives him an advantage for certain values of $p$.\nBut before we do that, we should prove an implicit assumption: the game ends with a winner in a finite number of moves with probability $1$. But to do that we first need to define a probability space on which the game is played. In particular, does it even exist? Indeed it does as we see below.\nConstructing the probability space To formalize our analysis we need to define a sequence of i.i.d. random variables, one for each coin toss. However, the existence of a probability space on which we can define this sequence is not obvious at all. For example, the following claim shows that we cannot always construct desired number of random variables on a measurable space.\nClaim:  There do not exist uncountably many independent, non-constant random variables on $([0,1], \\B([0,1]), \\lambda)$, where $\\lambda$ is the Lebesgue measure on the Borel $\\sigma$-algebra $\\B([0,1])$.\nProof:  Assume that $\\{X_i \\}_{i \\in I}$ is a collection of independent non-constant random variables. Define the collection $\\{Y_i \\} _{i \\in I}$ by letting $Y_i = X_i \\ind{|X_i| \u0026lt; C_i}$ where $C_i$ is large enough that $Y_i$ isn\u0026rsquo;t a constant. The collection formed by the random variables $Z_i = Y_i - \\Ex{Y_i}$ is a collection of independent random variables. Note that the random variables $Z_i$ are in the separable Hilbert space $L^2([0,1], \\lambda)$ and are orthogonal to each other. But a separable Hilbert can have only countably such elements. Thus, $I$ must be countable. $\\square$\nFortunately the situation isn\u0026rsquo;t so bleak for our case and Kolmogorov extension theorem allows us to claim the existence of a probability space on which there exists our required sequence of i.i.d. random variables if we can show the existence of probability spaces $(\\Omega_n, \\F_n, \\PP_n)$ for $n$ coin tosses that satisfy the consistency conditions. This is very easy: let $\\Omega_n = \\{H, T\\}^n$, $\\F_n = 2^{\\Omega_n}$, $\\PP({\\omega}) = p^{m} (1-p)^{n-m}$ where $m = $ number of $H$ in $\\omega \\in \\Omega_n$, and finally $\\PP(A) = \\sum_{\\omega \\in A} \\PP({\\omega})$ for any $A \\in \\F_n$. It is easy to see that this construction satisfies the consistency conditions and thus there exists a unique probability measure $\\PP$ defined on the measurable space $(\\Omega, \\F)$ where $\\Omega = \\{H, T\\}^{\\N}$ and $\\F$ is the $\\sigma$-algebra generated by the cylinder sets.\nThe game has a winner with probability $1$ We can now safely say the following statement: Let $\\{X_n\\}$ be a sequence of i.i.d. Bernoulli random variables such that $X_n = H$ or $T$ depending on the result of the $n$th coin toss. To show that the game has a winner with probability $1$ we need to prove that in the sequence $X_1, X_2, \\ldots$ all of the four choices $HH$, $TH$, $HT$ and $TT$ appear in a finite number of coin tosses with probability $1$.\nTo that end, fix any of the four choices $HH$, $TH$, $HT$ and $TT$, and call it $XY$. Let $E_n$ be the event that $X_{2n-1} = X$ and $X_{2n} = Y$. Then $\\PP(E_n) = \\e \u0026gt; 0$ independent of $n$, where $\\e$ is some positive real number dependent on $XY$ (for example, if $XY = HT$ then $\\e = p(1-p)$). Now the events $\\{E_n\\}$ are independent and $\\sum_{n=1}^\\infty \\PP(E_n) = \\infty$, and thus we can apply the second Borel-Cantelli lemma to get $\\PP(E_n \\text{ i.o.}) = 1$. But this immediately implies that each of the four outcomes appear in that sequence infinitely many times with probability $1$.\nBack to the game Recall the strategy outlined above. Without loss of generality we may assume $p \\geq 1/2$ because otherwise we can just flip the tags $H$ and $T$. From a first player perspective we just want to find one of the choices $HH$, $TH$, $HT$ or $TT$. Let\u0026rsquo;s calculate the minimum $p$ we get for each choice.\nIf player $1$ chooses $HH$ and player $2$ chooses $TH$, then it\u0026rsquo;s better to be player $1$ if $p \u0026gt; 1/\\sqrt{2}$. Checking for other choices of player $2$ we see that $TH$ is the optimal choice.\nIf player $1$ chooses $TT$ and player $2$ chooses $HT$, then for no $p \\geq 1/2$ it is better to be player $1$.\nIf player $1$ chooses $HT$ and player $2$ chooses $HH$, then for no $p \\geq 1/2$ it is better to be player $1$.\nIf player $1$ chooses $TH$ and player $2$ chooses $HT$, then for no $p \\geq 1/2$ it is better to be player $1$.\nOverall it is better to be player $1$ if $p \u0026gt; 1/\\sqrt{2}$ or if $p \u0026lt; 1 - 1/\\sqrt{2}$. And on the other hand it is better to be player $2$ if $1 - 1/\\sqrt{2} \u0026lt; p \u0026lt; 1/\\sqrt{2}$.\n","date":1602449945,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602449945,"objectID":"0b70a585f13f500ad0fdb86e23aa126b","permalink":"https://makkar.github.io/post/infcoingame/","publishdate":"2020-10-11T16:59:05-04:00","relpermalink":"/post/infcoingame/","section":"post","summary":"A discussion of a counter-intuitive game played with coin tosses","tags":[],"title":"A coin tossing game","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\P}{\\mathcal{P}} \\newcommand{\\CC}{\\mathcal{C}} \\newcommand{\\M}{\\mathcal{M}} \\newcommand{\\NN}{\\mathcal{N}} \\newcommand{\\L}{\\mathcal{L}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\trans}{\\intercal} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\O}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} $$\nIntroduction We discussed some functional analysis and the basics of Reproducing Kernel Hilbert Space theory in the previous two articles. The aim of this article is to discuss the need for approximating the kernel matrix and one method in particular to do that, Random Fourier Features.\nLet\u0026rsquo;s start by discussing Kernel Ridge Regression, a simple application of kernel methods.\nKernel Ridge Regression Ridge Regression Let\u0026rsquo;s recall the ridge regression model. We have some training data of the form:\n$$ (x_1, y_1), \\ldots, (x_n, y_n) \\in (\\X, \\Y) $$ where $\\X = \\R^d$ and $\\Y = \\R$. We represent the training data more succinctly by letting $\\matr{y} = [y_1, \\ldots, y_n]^\\trans$ and $\\matr{X}$ be an $n \\times d$ matrix whose $i$th row is $x_i^\\trans$. Then in ridge regression the likelihood model is\n$$ \\matr{y} \\sim \\NN(\\matr{X} \\matr{w}, \\sigma^2 \\matr{I}_n) $$ where $\\matr{w}$ is a $d$-dimensional column vector representing the weights to learned, and $\\sigma^2 \u0026gt; 0$. We also assume a Gaussian prior on $\\matr{w}$\n$$ \\matr{w} \\sim \\NN\\left(0, \\frac{1}{\\lambda} \\matr{I}_d\\right) $$\nThen the maximum a posteriori (MAP) estimate is given by\n\\begin{align} \\matr{w}_{\\text{MAP}} \u0026amp;= \\argmax _{\\matr{w}} \\; \\ln p(\\matr{w} | \\matr{y}, \\matr{X}) \\\\ \u0026amp;= \\argmax _{\\matr{w}} \\; \\ln p(\\matr{y} | \\matr{w}, \\matr{X}) + \\ln p(\\matr{w}) \\\\ \u0026amp;= \\argmax _{\\matr{w}} \\; \\underbrace{-\\frac{1}{2\\sigma^2} (\\matr{y} - \\matr{X} \\matr{w})^\\trans (\\matr{y} - \\matr{X} \\matr{w}) - \\frac{\\lambda}{2} \\matr{w}^\\trans \\matr{w}} _{\\L} \\end{align}\nIf we call this objective $\\L$, then $\\matr{w}_{\\text{MAP}}$ is given by the solution of the equation formed by equating the gradient of $\\L$ with respect to $\\matr{w}$ to $0$. Thus,\n$$ 0 = \\nabla_{\\matr{w}} \\L = \\frac{1}{\\sigma^2} \\matr{X}^\\trans \\matr{y} - \\frac{1}{\\sigma^2} \\matr{X}^\\trans \\matr{X} \\matr{w} - \\lambda \\matr{w} $$\nand we get\n$$ \\matr{w}_{\\text{MAP}} = (\\lambda \\sigma^2 \\matr{I} + \\matr{X}^\\trans \\matr{X})^{-1} \\matr{X}^\\trans \\matr{y} $$\nwhich is called the ridge regression solution, and we denote it by $\\matr{w}_{\\text{RR}}$. We are inverting a $d \\times d$ matrix taking $\\O(d^3)$ time, and matrix multiplication of a $d \\times d$ matrix with a $d \\times n$ matrix taking $\\O(d^2 n)$ time.\nKernel Ridge Regression The previous model suffers from limited expressiveness. As we have seen before, the idea of kernel methods is to define a map which takes our inputs from $\\X$ to an RKHS $\\H$:\n$$ \\phi : \\X \\to \\H $$\nand then apply the linear model of ridge regression above to $\\phi(x_i)$ instead of $x_i$. $\\H$ could be an infinite-dimensional vector space, but for now suppose it is $D$-dimensional. Let $\\matr{\\Phi}$ represent the $n \\times D$ matrix whose $i$th row is $\\phi(x_i)^\\trans$. Then the ridge regression solution is given by\n\\begin{equation} \\matr{w}_{\\text{RR}} = (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans \\matr{y} \\end{equation}\nWe immediately realize the problem here: we are inverting a $D \\times D$ matrix and $D$ can be huge! Fortunately, we have a matrix trick that we can exploit, the push-though identity. It looks like this\n\\begin{align} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) \u0026amp;= (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi}) \\matr{\\Phi}^\\trans \\quad \\text{(can be seen by expanding)} \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) \u0026amp;= (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi}) \\matr{\\Phi}^\\trans \\quad \\text{(left multiplying by } (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\text{ )} \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) \u0026amp;= \\matr{\\Phi}^\\trans \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \u0026amp;= \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\quad \\text{(right multiplying by } (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\text{ )} \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans \u0026amp;= \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\end{align}\nAnd thus we get $$ \\matr{w}_{\\text{RR}} = \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\matr{y} $$\nWe are now inverting an $n \\times n$ matrix taking $\\O(n^3)$ time, and matrix multiplication of a $D \\times n$ matrix with a $n \\times n$ matrix taking $\\O(D n^2)$ time.\nNotice that $\\matr{\\Phi} \\matr{\\Phi}^\\trans$ is the gram matrix, and let\u0026rsquo;s denote it by $\\matr{K}$. It\u0026rsquo;s $(i,j)$th entry is $\\matr{K}_{i,j} = \\inner{\\phi(x_i)}{\\phi(x_j)} = K(x_i, x_j)$, where $K : \\R^d \\times \\R^d \\to \\C$ is the kernel function corresponding to the RKHS $\\H$.\nIf we denote\n$$ \\matr{\\alpha} = (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\matr{y} = (\\lambda \\sigma^2 \\matr{I}_n + \\matr{K})^{-1} \\matr{y} $$\nthen we have\n$$ \\matr{w}_{\\text{RR}} = \\matr{\\Phi}^\\trans \\matr{\\alpha} = \\sum _{i=1}^n \\alpha_i \\phi(x_i) $$\nand prediction for a test data point, $x$, is\n$$ \\matr{w}_{\\text{RR}}^\\trans \\phi(x) = \\sum _{i=1}^n \\alpha_i \\phi(x_i)^\\trans \\phi(x) = \\sum _{i=1}^n \\alpha_i k _{x_i}(x) $$\nwhere $k_{x_i}$ denotes the reproducing kernel for the point $x_i$.\nIf this looks suspiciously similar to the Representer theorem (Theorem-8), then that\u0026rsquo;s because it is an instance of it! So prediction takes $\\O(n)$ time.\nThe point of this discussion was to show that Kernel Ridge Regression is computationally expensive. Simply storing the gram matrix, $\\matr{K}$, takes $\\O(n^2)$ space. We need to find methods which can circumvent using $\\matr{K}$ if we are to apply kernel methods to anything other than the smallest of the data sets.\nKernel approximation The idea behind kernel approximation methods is to replace the gram matrix, $\\matr{K}$, with a low rank approximation, i.e., find an $n \\times S$ matrix, $\\matr{Z}$, such that\n$$ \\matr{K} \\approx \\matr{Z} \\matr{Z}^\\trans $$\nWe want $S \\ll n$, and thus we significantly reduce our time and space complexity. $\\matr{Z}$ takes $\\O(nS)$ space. Inversion of $(\\lambda \\sigma^2 \\matr{I}_n + \\matr{Z} \\matr{Z}^\\trans)$ takes $\\O(n S^2)$ time.\nOne way to think about this is we want to find a mapping of $x_i$'s to some latent space such that the inner product in this latent space approximates the kernel\n$$ \\matr{K}_{i,j} = K(x_i, x_j) \\approx z_i^\\trans z_j $$\nwhere $z_i$ denotes the $i$th row of $\\matr{Z}$. One way to do that is Nyström approximation, which I won\u0026rsquo;t be discussing at all. The other method is to use Random Fourier Features. This approach was introduced by Rahimi and Recht in their seminal 2007 paper Random Features for Large-Scale Kernel Machines. It relies on a result from functional analysis called Bochner\u0026rsquo;s theorem, so let\u0026rsquo;s digress and discuss that first.\nBochner\u0026rsquo;s theorem Definition 28: A function $K : \\R^n \\to \\C$, is said to be positive-definite if\n$$ \\sum _{i,j = 1}^n \\alpha_i \\conj{\\alpha_j} K(x_i - x_j) \\geq 0 $$\nfor every choice of $x_1, \\ldots, x_n \\in \\R^n$ and for every choice of complex numbers $\\alpha_1, \\ldots, \\alpha_n$.\nNotice how similar this definition is to the definition of a kernel function. Positive-definite functions have some nice properties:\nLemma 3: If $K: \\R^n \\to \\C$ is a positive-definite function, then $K(-x) = \\conj{K(x)}$ for every $x \\in \\R^n$.\nProof: It is easy to see that $K(0) \\geq 0$. In the definition above, let $n = 2$, $x_1 = x$, $x_2 = 0$, $\\alpha_1$ be an arbitrary complex number, and $\\alpha_2 = 1$. Then applying the definition of a positive-definite function, we get\n$$ (1 + |\\alpha_1|^2)K(0) + \\alpha_1 K(x) + \\conj{\\alpha_1}K(-x) \\geq 0 $$\nLet $\\alpha_1 = 1$, then $$ 2K(0) + K(x) + K(-x) \\geq 0 $$\nIn particular, $K(x) + K(-x)$ is real, which implies\n$$ K(x) + K(-x) = \\conj{K(x)} +\\conj{K(-x)} $$\nSimilarly, letting $\\alpha_1 = i$, we get $i(K(x) - K(-x))$ is real, which implies\n$$ K(x) - K(-x) = -\\conj{K(x)} + \\conj{K(-x)} $$\nAdding these two equations, we get $K(-x) = \\conj{K(x)}$. $\\square$\nLemma 4: If $K: \\R^n \\to \\C$ is a positive-definite function, then $K$ is bounded. In particular, $|K(x)| \\leq K(0)$ for every $x \\in \\R^n$.\nProof: From lemma-3, $K(0)$ must be real. In the definition above, let $n = 2$, $x_1 = 0$, $x_2 = x$, $\\alpha_1 = |K(x)|$, and $\\alpha_2 = -\\conj{K(x)}$. Then applying the definition of a positive-definite function, we get\n$$ 2K(0) |K(x)|^2 - |K(x)| K(x) K(-x) - \\conj{K(x)} |K(x)| K(x) \\geq 0 $$\nNow use lemma-3 to substitute $\\conj{K(x)}$ for $K(-x)$ in the middle term to get\n$$ 2K(0) |K(x)|^2 - 2 |K(x)|^3 \\geq 0 $$\nIf $|K(x)| = 0$, then we obviously have our result, since we can easily show $K(0) \\geq 0$, otherwise we can divide by $2|K(x)|^2$ to get\n$$ K(0) - |K(x)| \\geq 0 $$\nwhich is our desired result. $\\square$\nThe following theorem is the converse of Bochner\u0026rsquo;s theorem. We state it first since it is easier to prove.\nTheorem 9 [Converse of Bochner\u0026rsquo;s theorem]: The Fourier transform of every finite Borel measure on $\\R^n$ is positive-definite.\nProof: Let $\\mu$ be a finite Borel measure on $\\R^n$. Let $K : \\R^n \\to \\C$ be the Fourier transform of $\\mu$, i.e.,\n$$ K(x) = \\int_{\\R^n} \\exp(-i \\omega^\\trans x) \\dmu(\\omega) $$\nLet $x_1, \\ldots, x_n \\in \\R^n$ and $\\alpha_1, \\ldots, \\alpha_n \\in \\C$ be arbitrary. Then\n\\begin{align} \\sum _{i,j = 1}^n \\alpha_i \\conj{\\alpha_j} K(x_i - x_j) \u0026amp;= \\sum _{i,j = 1}^n \\alpha_i \\conj{\\alpha_j} \\int _{\\R^n} \\exp\\{-i \\omega^\\trans (x_i - x_j)\\} \\dmu(\\omega) \\\\\n\u0026amp;= \\int _{\\R^n} \\left| \\sum _{i = 1}^n \\alpha_i \\exp(-i \\omega^\\trans x_i) \\right|^2 \\dmu(\\omega) \\\\\n\u0026amp;\\geq 0 \\end{align}\nThus, $K$ is positive-definite. $\\square$\nTheorem 10 [Bochner\u0026rsquo;s theorem]: If $K$ is continuous and positive-definite, then $K$ is the Fourier transform of a finite positive Borel measure.\nI\u0026rsquo;ll skip the proof since it is beyond my understanding. The proof can be easily found on the internet, see here or here for example.\nThis finite positive Borel measure is often called as spectral measure. It is easy to see that $K(0) = \\mu(\\R^n)$, and thus if we assume $K(0) = 1$ then the spectral measure is a probability measure.\nRandom Fourier Features Picking up where we left: We want to find a low-dimensional mapping of $x_i$'s into a latent space such that we can approximate the kernel computation with an inner product in this latent space. We will use Bochner\u0026rsquo;s theorem for this. To apply this theorem we need to limit ourselves to a special class of kernels.\nDefinition 29: A kernel function $K : \\R^d \\times \\R^d \\to \\C$ is called shift-invariant if $K(x, y) = K(x-y, 0)$.\nGaussian and Laplacian kernels are examples of shift-invariant kernels. Note that the function $K\u0026rsquo;: \\R^d \\to \\C$ defined by $K\u0026rsquo;(x) = K(x, 0)$ is positive-definite if $K$ is a shift-invariant kernel. Bochner\u0026rsquo;s theorem then implies the existence of a spectral measure, $\\mu$, such that\n$$ K(x, y) = K(x-y, 0) = K\u0026rsquo;(x-y) = \\int _{\\R^d} \\exp\\{-i\\omega^\\trans (x-y)\\} \\dmu(\\omega) $$\nLet $p$ denote the Radon-Nikodym derivative of $\\mu$ with respect to the Lebesgue measure on $\\R^d$, then we can write the equation above as\n\\begin{equation} K(x, y) = \\int _{\\R^d} \\exp\\{-i\\omega^\\trans (x-y)\\} p(\\omega) \\dom \\tag{$\\star$} \\end{equation}\nFor example, if we take the Gaussian kernel\n$$ K(x,y) = \\exp\\left(-\\frac{\\norm{x-y}^2_2}{2 \\sigma^2}\\right) $$\nthen since $K(0,0) = 1$, $p$ is a probability density, and can be easily shown to be Gaussian\n$$ p \\sim \\NN\\left(0, \\frac{1}{\\sigma^2} \\matr{I}_d\\right) $$\nWe now see an obvious way to approximate the kernel from equation $(\\star)$: use Monte Carlo approximation. If we take $S$ samples, $\\omega_1, \\ldots, \\omega_S \\sim p(\\omega)$,\n\\begin{align} K(x, y) \u0026amp;= \\int _{\\R^d} \\exp\\{-i\\omega^\\trans (x-y)\\} p(\\omega) \\dom \\\\\n\u0026amp;\\approx \\frac{1}{S} \\sum _{s=1}^S \\exp\\{-i\\omega^\\trans_s (x-y)\\} \\\\\n\u0026amp;= \\inner{z(x)}{z(y)} \\end{align}\nwhere $z : \\R^d \\to \\R^S$ is the latent mapping given by\n$$ z(x) = \\frac{1}{\\sqrt{S}}[\\exp(-i \\omega ^\\trans _1 x), \\ldots, \\exp(-i \\omega ^\\trans _S x)]^\\trans $$\nWe can get another mapping by noting that if the kernel is real, then from $(\\star)$ we can see that the RHS must also be real and we can thus write it as\n\\begin{equation} K(x, y) = \\int _{\\R^d} \\cos\\{\\omega^\\trans (x-y)\\} p(\\omega) \\dom \\end{equation}\nUsing the fact that $\\cos(a - b) = \\cos(a) \\cos(b) + \\sin(a) \\sin(b)$, we can write $\\cos\\{\\omega^\\trans (x-y)\\} = \\cos(\\omega^\\trans x) \\cos(\\omega^\\trans y) + \\sin(\\omega^\\trans x) \\sin(\\omega^\\trans y)$\nThus, if we define $z_1 : \\R^d \\to \\R^{2S}$ by\n$$ z_1(x) = \\frac{1}{\\sqrt{S}}[\\cos(\\omega^\\trans_1 x), \\ldots, \\cos(\\omega^\\trans_S x), \\sin(\\omega^\\trans_1 x), \\ldots, \\sin(\\omega^\\trans_S x)]^\\trans $$\nwe have $\\inner{z_1(x)}{z_1(y)} \\approx K(x, y)$.\nAnother mapping that is used is $z_2 : \\R^d \\to \\R^{S}$ defined by\n$$ z_2(x) = \\sqrt{\\frac{2}{S}}[\\cos(\\omega^\\trans_1 x + b_1), \\ldots, \\cos(\\omega^\\trans_S x + b_S)]^\\trans $$\nwhere $b_1, \\ldots, b_S \\sim \\text{Unif}[0, 2\\pi]$. It is not too difficult to show that\n$$ \\E_{\\omega, b}[z_2(x)^\\trans z_2(y)] = \\E_{\\omega}[z_1(x)^\\trans z_1(y)] $$\nThere exist many modifications to the basic RFF approach described here. For example, we can do Quasi-Monte Carlo approximations instead of Monte Carlo approximations. See the paper Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels by Avron, Sindhwani, Yang and Mahoney (2016). Or we could sample from a modified distribution in Fourier space, given by the leverage function of the kernel. See the paper Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees by Avron et al (2017).\nExtension to a more general class of kernels I have often seen kernels being represented as an integral of a product of two functions neatly separating the dependence on $x$ and $y$:\n$$ K(x,y) = \\int_{\\Omega} \\varphi(\\omega, x) \\varphi(\\omega, y) \\dmu(\\omega) $$\nfor some function $\\varphi(\\cdot, \\cdot)$ and measure $\\mu$ on $\\Omega$. I found a good explanation of when this is possible in the papers by Bach (2017) Breaking the Curse of Dimensionality with Convex Neural Networks and Li et al (2019) Towards a Unified Analysis of Random Fourier Features.\nLet $\\mu$ be a Borel probability measure on the compact space $\\Omega$, and $\\varphi: \\Omega \\times \\X \\to \\R$ be a function such that the functions $\\varphi(\\cdot, x) : \\Omega \\to \\R$ are measurable for all $x \\in \\X$, i.e., they are random variables. Now define the set $\\H$ to consist of all functions $f$ that can written as\n$$ f(x) = \\int_{\\Omega} h(\\omega) \\varphi(\\omega, x) \\dmu(\\omega) \\quad \\text{for all } x \\in \\X $$\nfor some $h: \\Omega \\to \\R$ such that $\\int_{\\Omega} h^2 \\dmu \u0026lt; \\infty$. Let us define the squared norm, $\\norm{f}_{\\H}^2$, as the infimum of $\\int_{\\Omega} h^2 \\dmu$ over all functions $h$ for which $f$ can be decomposed as above. Then it can be shown that $\\H$ is an RKHS with the kernel\n$$ K(x,y) = \\int_{\\Omega} \\varphi(\\omega, x) \\varphi(\\omega, y) \\dmu(\\omega) $$\nTherefore, we can again approximate this kernel with a Monte Carlo approximation:\n\\begin{align} K(x,y) \u0026amp;= \\int_{\\Omega} \\varphi(\\omega, x) \\varphi(\\omega, y) \\dmu(\\omega) \\\\\n\u0026amp;\\approx \\frac{1}{S} \\sum_{s=1}^S \\varphi(\\omega_s, x) \\varphi(\\omega_s, y) \\end{align}\nEpilogue Random Fourier Features is an easy to implement approach that allows us to apply kernel methods on large data sets. I haven\u0026rsquo;t discussed the number of samples, $S$, needed to approximate the kernel function within an error bound. You can find such results in the papers linked above.\nWith this article I conclude the series on kernels. I had initially set out to write a short piece on Random Fourier Features but the subject of kernel theory is vast and beautiful, and that short piece metastasised to three articles.\n","date":1599509135,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599509135,"objectID":"25a7b6390f505311277bb15ca56943dd","permalink":"https://makkar.github.io/post/kernels2/","publishdate":"2020-09-07T16:05:35-04:00","relpermalink":"/post/kernels2/","section":"post","summary":"Random Fourier Features","tags":[],"title":"Kernels - Part 2","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\P}{\\mathcal{P}} \\newcommand{\\Ch}{\\mathcal{C}} \\newcommand{\\M}{\\mathcal{M}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} $$\nIntroduction I have been self-studying some functional analysis recently, and Zorn\u0026rsquo;s lemma comes up often in proofs. Since I have no formal background in mathematics (my undergrad was in Mechanical engineering), it was my first time hearing about Zorn\u0026rsquo;s lemma. I read its statement from the Wikipedia article, naively assuming I understood this extremely powerful tool. But every usage of Zorn\u0026rsquo;s lemma seemed contrived to me, and it was only in retrospect that I could see how Zorn\u0026rsquo;s lemma seems like an obvious tool to apply. An excellent article which helped me understand Zorn\u0026rsquo;s lemma is How to use Zorn\u0026rsquo;s lemma by Timothy Gowers. If you didn\u0026rsquo;t know about Gowers\u0026rsquo;s article, then mentioning it is the biggest contribution of my article and I recommend you read it.\nIn this article, I want to show how to use Zorn\u0026rsquo;s lemma by stating a theorem and discussing its proof. Things \u0026ldquo;clicked\u0026rdquo; for me when I proved that theorem. But before we do that let me define some important concepts.\nPreliminaries Recall the concept of relation I defined here.\nDefinition 1: Let $P$ be a nonempty set. A partial order relation in $P$ is a relation which is symbolized by $\\leq$ and that satisfies the following properties for all $x,y,z \\in P$:\n Reflexivity: $x \\leq x$; Antisymmetry: $x \\leq y$ and $y \\leq x$ implies $x = y$; Transitivity: $x \\leq y$ and $y \\leq z$ implies $x \\leq z$.  The set $P$ is then called a partially ordered set. If in addition to these three properties, the set $P$ also satisfies\nany two elements are comparable, i.e., either $x \\leq y$ or $y \\leq x$.  then the relation is called a total order relation and the set $P$ is called a totally ordered set or a chain.\nExamples  Let $P$ be the set of all positive integers, and let $m \\leq n$ mean that $m$ divides $n$. Then $P$ is a partially ordered set. It is not a chain as $2$ and $3$ are not comparable, for example. Let $P$ be the set of all real numbers, and let $x \\leq y$ mean that $y-x$ is nonnegative. Then $P$ is a chain. Let $P$ be the class of all subsets of some universal set $U$, and let $A \\leq B$ for $A,B \\in P$ mean that $A \\subseteq B$. Then $P$ is a partially ordered set. It is not a chain because if $U$ contains at least two elements, then we can find two subsets of $U$ neither of which is a subset of the other.  Definition 2: If $P$ is a partially ordered set, an element $x \\in P$ is said to be maximal if for any $y \\in P$ for which $x \\leq y$, we must have $x = y$.\nNote that a maximal element does not have to be bigger than everything else: it just must not be smaller than anything else. A maximal element may not exist, and if it exists it may not be unique. Examples 1 and 2 above have no maximal elements. Example 3 has one maximal element: $U$.\nDefinition 3: Let $Q$ be a nonempty subset of a partially ordered set $P$. An element $x \\in P$ is called an upper bound of $Q$ if $y \\leq x$ for every $y \\in Q$. An upper bound of $Q$ is called a least upper bound of $Q$ if it is less than or equal to every upper bound of $Q$.\nSimilarly for lower bound and greatest lower bound.\nNote how similar these definitions are to the definitions of supremum and infimum for real numbers. That\u0026rsquo;s because in that case they are the same. In Example 1 above, if we take $Q$ to be any finite subset of $P$, then the greatest lower bound is the greatest common divisor of all the elements of $Q$ and the least upper bound is the least common multiple. In Example 3 above, let $Q$ be any nonempty subset of $P$. Then the least upper bound is the union of all the sets in $Q$, and the greatest lower bound is the intersection of all the sets in $Q$.\nWe are now ready to state the Zorn\u0026rsquo;s lemma.\nZorn\u0026rsquo;s lemma If $P$ is a partially ordered set in which every chain has an upper bound, then $P$ possesses a maximal element.\nExample application Theorem: Any infinite set $X$ can be represented as a union of a disjoint class of countably infinite subsets.\nProof: The theorem is trivial if $X$ is countably infinite, so assume that it is uncountably infinite.\nThe first thing that we need to do is realize Zorn\u0026rsquo;s lemma can be applied here. I use the following description taken from Gowers\u0026rsquo;s article as a guiding principle:\n If you are building a mathematical object in stages and find that (i) you have not finished even after infinitely many stages, and (ii) there seems to be nothing to stop you continuing to build, then Zorn’s lemma may well be able to help you.\n The fact that we need to build $X$ by taking a union of some sets, and it could be an uncountable union and therefore we might not finish even in countably infinite number of stages, suggests Zorn\u0026rsquo;s lemma might be helpful here. Zorn\u0026rsquo;s lemma will prove the existence of a maximal element in a partially ordered set, therefore, we want to construct a partially ordered set such that we can prove that its maximal element is $X$.\nWe want to build our partially ordered set to be consisting of elements of this form: disjoint class of countably infinite subsets of $X$. The reason we want to define our partially ordered set this way is because, first, the subset operation makes it a partially ordered set, and second, it seems intuitively true that the maximal element of this partially ordered set, if it exists, will be such that the union of its elements is $X$.\nTo this end, let $\\P$ be the set of all disjoint classes of countably infinite subsets of $X$. $\\P$ is partially ordered by $\\subseteq$ as we saw in Example 3 above.\nTo be able to apply Zorn\u0026rsquo;s lemma we need to show that every chain in $\\P$ has an upper bound. Let $\\Ch$ be a chain in $\\P$. A natural guess for the upper bound of $\\Ch$ is $U = \\bigcup _{S \\in \\Ch} S$. To show that $U$ is an upper bound of $\\Ch$ we need to show that $U \\in \\P$ and $S \\subseteq U$ for every $S \\in \\Ch$. The second claim is trivial from $U$'s definition. It is easy to see that $U$ is a class of countably infinite subsets of $X$ because this is true for each $S \\in \\Ch$. To show that $U$ is a disjoint class, let $A, B \\in U$ be distinct elements of $U$. There exist elements $S_A, S_B \\in \\Ch$ such that $A \\in S_A$ and $B \\in S_B$. Since $\\Ch$ is a chain, either $S_A \\subseteq S_B$ or $S_B \\subseteq S_A$. Without loss of generality, $S_A \\subseteq S_B$. Then $A, B \\in S_B$ and this implies $A$ and $B$ are disjoint because this is true for the elements of our chain. Thus $U$ is a disjoint class of countably infinite subsets and hence belongs to $\\P$.\nBy the Zorn\u0026rsquo;s lemma, $\\P$ possesses a maximal element, $M$. If $\\bigcup _{E \\in M} E = X$ then we are done because $M$ is the required disjoint class of countably infinite subsets whose union is $X$. But it can still happen that some elements of $X$ are not present in this union. In this case we can show that these leftover elements form a finite set and we can get our required disjoint class by adding these leftover elements to any element of $M$. To see that the leftover elements must be finite, denote the set of leftover elements by $Y = X \\setminus \\bigcup _{E \\in M} E$. If $Y$ is infinite it must contain a countably infinite subset $Z \\subseteq Y$. Consider the class $M \\cup \\{Z\\}$. It is an element of $\\P$ because it is a disjoint class of countably infinite subsets of $X$. But $M$ is a strict subset of $M \\cup \\{Z\\}$ which contradicts the fact that $M$ is a maximal element of $\\P$.\nWe are done!\n","date":1596841152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596841152,"objectID":"8a22e14e2982adf5a8ada11e4fae9c55","permalink":"https://makkar.github.io/post/zorn/","publishdate":"2020-08-07T18:59:12-04:00","relpermalink":"/post/zorn/","section":"post","summary":"An example on how to use Zorn's lemma","tags":[],"title":"Zorn's lemma","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} $$\nIntroduction This is the second blog post in the series of blog posts on kernels. In the first part I introduced the functional analysis background for kernel theory. I highly recommend you read it before continuing. I will frequently refer to it and use the same notation. In this blog post I aim to introduce the fundamental theorems like Mercer\u0026rsquo;s theorem and Representer theorem.\nCharacterization of reproducing kernels We already defined the notion of reproducing kernels for an RKHS (Definition 20). We now turn our attention to obtaining necessary and sufficient conditions for a function $K : \\X \\times \\X \\to \\C$ to be the reproducing kernel for some RKHS. But first we must define kernel functions.\nKernel functions Let\u0026rsquo;s start by recalling a basic definition.\nDefinition 21: Let $A = (a_{i,j})$ be an $n \\times n$ complex matrix. Then $A$ is called positive if for every $\\alpha_1, \\ldots, \\alpha_n \\in \\C$ we have $$ \\sum_{i,j = 1}^{n} \\conj{\\alpha_i}\\alpha_j a_{i,j} \\geq 0 $$ We denote this by $A \\geq 0$.\nNote that if we define a vector $x \\in \\C^n$ to be such that its $i$th component is $\\alpha_i$, then the condition above can rewritten as $$ \\inner{Ax}{x} \\geq 0 $$\nAlso note that if $A \\geq 0$ then $A = A^* $, where $A^* $ denotes the Hermitian matrix (also known as the self-adjoint matrix), $\\conj{A^T}$. Therefore, positivity gives self-adjoint property for free if we are dealing with complex matrices. Things aren\u0026rsquo;t so elegant for real matrices. For the real case we need to explicitly state that the matrix $A$ is also symmetric apart from what\u0026rsquo;s stated in the definition above. Therefore, we often use the following as the definition of positive matrices:\nDefinition 21\u0026rsquo;: An $n \\times n$ matrix $A$ is positive, in symbols $A \\geq 0$, if it is self-adjoint and if $\\inner{Ax}{x} \\geq 0$ for all $x \\in \\C^n$.\nPositive matrices are also alternatively called  positive semidefinite or nonnegative matrices.\nThe following lemma connects the concept of positive matrices to its eigenvalues.\nLemma 2: A matrix $A \\geq 0$ if and only if $A = A^*$ and every eigenvalue of $A$ is nonnegative.\nProof: Let us first suppose $A \\geq 0$, then $A = A^*$ by definition. Now if $\\lambda$ is an eigenvalue of $A$ and $v$ is an eigenvector of $A$ corresponding to $\\lambda$, we have $$ 0 \\leq \\inner{Av}{v} = \\inner{\\lambda v}{v} = \\lambda \\inner{v}{v} $$ Thus, $\\lambda$ is a nonnegative number.\nFor the other side, find an orthonormal basis $v_1, \\ldots, v_n$ consisting of eigenvectors of $A$ (it exists by the Spectral theorem). Let $v_i$ be the eigenvector corresponding to the eigenvalue $\\lambda_i$. Then for any $x = \\sum_i \\alpha_i v_i$ we have $\\inner{Ax}{x} = \\sum_i \\lambda_i \\modu{\\alpha_i}^2$. Since $\\lambda_i \\geq 0$, $\\inner{Ax}{x} \\geq 0$ and $A$ must be positive. $\\square$\nWe are now ready to define kernel function.\nDefinition 22: Let $\\X$ be a set, then $K : \\X \\times \\X \\to \\C$ is called a kernel function if for every $n \\in \\N$ and for every choice of $\\{x_1, \\ldots, x_n\\} \\subseteq \\X$, the matrix $(K(x_i, x_j)) \\geq 0$. We will use the notation $K \\geq 0$ to denote that the function $K$ is a kernel function.\nKernel functions are alternatively called positive definite functions or positive semidefinite functions.\nDefinition 23: Given a kernel function $K : \\X \\times \\X \\to \\C$ and points $x_1, \\ldots, x_n \\in \\X$, the $n \\times n$ matrix $(K(x_i, x_j))$ is called the Gram matrix of $K$ with respect to $x_1, \\ldots, x_n$.\nSome examples follow:\nExamples  Linear kernels: When $\\X = \\R^d$, we can define the linear kernel function as $$K(x, y) = \\inner{x}{y} $$ It is clearly a symmetric function of its arguments, and hence self-adjoint. To prove positivity, let $x_1, \\ldots, x_n \\in \\R^d$ be an arbitrary collection of points, and consider its gram matrix $\\matr{K}$, i.e., $\\matr{K}_{i,j} = K(x_i, x_j) = \\inner{x_i}{x_j}$. Then for any $\\alpha \\in \\R^n$, we have  $$ \\inner{\\matr{K} \\alpha}{\\alpha} = \\alpha^T \\matr{K}^T \\alpha = \\alpha^T \\matr{K} \\alpha = \\sum_{i,j = 1}^n \\alpha_i \\alpha_j \\inner{x_i}{x_j} = \\left\\lVert \\sum_{i=1}^n \\alpha_i x_i \\right\\rVert^2 \\geq 0 $$\n Polynomial kernels: A natural generalization of the linear kernel on $\\R^d$ is the homogeneous polynomial kernel $$ K(x, y) = (\\inner{x}{y})^m $$ of degree $m \\geq 2$, also defined on $\\R^d$. It is clearly a symmetric function. To prove positivity, note that\n$$ K(x,y) = \\left( \\sum_{i=1}^d x_i y_i \\right)^m $$\nThis will have $D = \\binom{m+d-1}{m}$ monomials, so to simplify the analysis let\u0026rsquo;s take $m=2$. Then\n$$ K(x,y) = \\sum_{i=1}^d x_i^2 y_i^2 + 2 \\sum_{i \u0026lt; j} x_i x_j y_i y_j $$ In this case $D = \\binom{d+1}{d} = d + \\binom{d}{2}$. Define a mapping $\\Phi: \\R^d \\to \\R^D$ such that\n$$ \\Phi(x) = [x_1^2, \\ldots, x_d^2, \\sqrt{2}x_1 x_2, \\ldots, \\sqrt{2} x_{d-1} x_d ]^T $$\nThen\n$$ K(x,y) = \\inner{\\Phi(x)}{\\Phi(y)} $$\nFollowing the same argument as the first example, we can verify that the gram matrix thus formed is positive.\nThe mapping $x \\mapsto \\Phi(x)$ is often referred to as a feature map. We see that dealing with elements in the feature space, i.e. the range of $\\Phi$, is computationally expensive. The relation $K(x,y) = \\inner{\\Phi(x)}{\\Phi(y)}$ allows us compute the inner products using the kernel function instead of actually taking the inner product in a very high dimensional space. We will see that this \u0026ldquo;kernel trick\u0026rdquo; holds for very many kernel functions when we discuss Mercer\u0026rsquo;s theorem.\n  Gaussian kernels: Given some compact subset $\\X \\subseteq \\R^d$, consider the Gaussian kernel\n$$ K(x,y) = \\exp{\\left( -\\frac{1}{2 \\sigma^2} \\norm{x-y}^2 \\right)} $$\nIt is not obvious why this is a kernel function.\n  Equivalence between kernel function and reproducing kernel Let us return to the characterization of reproducing kernels. We will now prove that a function is a kernel function if and only if there is an RKHS for which it is the reproducing kernel. At this point recall Theorem-3 which states that an RKHS admits a unique reproducing kernel.\nTheorem 4: Let $\\X$ be a set and let $\\H$ be an RKHS on $\\X$ with reproducing kernel $K$. Then $K$ is a kernel function.\nProof: For some arbitrary $n \\in \\N$ fix some arbitrary collection $x_1, \\ldots, x_n \\in \\X$ and $\\alpha \\in \\C^n$. Then if we denote by $\\matr{K}$ the gram matrix of $K$ with respect to $x_1, \\ldots, x_n$, we have $$\\inner{\\matr{K} \\alpha}{\\alpha} = \\sum_{i,j = 1}^n \\conj{\\alpha_i}\\alpha_j K(x_i, x_j) = \\sum_{i,j = 1}^n \\conj{\\alpha_i}\\alpha_j \\inner{k_{x_j}}{k_{x_i}} = \\left\\lVert \\sum_{i=1}^n \\alpha_i k_{x_i} \\right\\rVert^2 \\geq 0 \\quad$$\nAnd thus $K$ is a kernel function. $\\square$\nWhat does it mean in the above proof if we have an equality? That is, if $\\inner{\\matr{K} \\alpha}{\\alpha} = 0$? This happens if and only if $\\left\\lVert \\sum_{i=1}^n \\alpha_i k_{x_i} \\right\\rVert = 0$. But this means that for every $f \\in \\H$ we have $\\sum_{i=1}^n \\conj{\\alpha_i} f(x_i) = \\inner{f}{\\sum_i \\alpha_i k_{x_i}} = 0$. Thus, in this case there is an equation of linear dependence between the values of every function in $\\H$ at this finite set of points.\nNow let us state the converse of Theorem-4. It is a deep result in RKHS theory known as the Moore–Aronszajn theorem.\nTheorem 5 [Moore–Aronszajn theorem]: Let $\\X$ be a set and let $K : \\X \\times \\X \\to \\C$ be a kernel function, then there exists a reproducing kernel Hilbert space $\\H$ of functions on $\\X$ such that $K$ is the reproducing kernel of $\\H$.\nFor a proof see here on Wikipedia.\nIn light of these two theorems we have the following notation.\nDefinition 24: Given a kernel function $K : \\X \\times \\X \\to \\C$, we let $\\H(K)$ denote the unique RKHS with the reproducing kernel $K$.\nIt is not an easy problem to start with a kernel function $K$ on some set $\\X$ and give a concrete description of $\\H(K)$. I will not be discussing this here, but there are plenty of resources available where you can see this problem getting discussed. I recommend Paulsen and Raghupathi\u0026rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces.\nMercer\u0026rsquo;s theorem Recall the Spectral theorem for finite-dimensional vector spaces: A linear operator $T: V \\to V$ for some finite dimensional vector space $V$ on $\\C$ is normal, i.e., $T T^* = T^* T$, if and only if $V$ has an orthonormal basis consisting of eigenvectors of $T$. This implies that if $\\matr{U} = [v_1, \\ldots, v_n]$ is a unitary matrix containing the $i$th eigenvector in its $i$th column and $\\matr{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)$ is a diagonal matrix containing the corresponding eigenvalues then if $\\matr{K}$ is a normal matrix then it can written as $$\\matr{K} = \\matr{U} \\matr{\\Lambda} \\matr{U}^T = \\sum_{i=1}^n \\lambda_i v_i v_i^T$$\nMercer\u0026rsquo;s theorem generalizes this decomposition to kernel functions. Let us start by defining a special type of kernel function.\nDefinition 25: Let $\\X$ be a compact metric space. A function $K : \\X \\times \\X \\to \\C$ is called a Mercer kernel if it is a continuous kernel function.\nRecall the space $L^2(\\mu)$ from Definition-8 where we now take $\\X$ (written as $X$ there) to be a compact metric space.\nDefinition 26: Given a Mercer kernel $K : \\X \\times \\X \\to \\C$, we define a linear operator $T_{K} : L^2(\\mu) \\to L^2(\\mu)$ as $$ T_K(f)(x) := \\int_{\\X} K(x, y) f(y) \\dmu(y), \\quad (x \\in \\X) $$\nWe assume that the Mercer kernel satisfies the Hilbert-Schmidt condition, stated as\n$$ \\int_{\\X \\times \\X} \\left\\lvert K(x,y) \\right\\rvert^2 \\dmu(x) \\dmu(y) \u0026lt; \\infty $$\nwhich ensures that $T_K$ is a bounded linear operator on $L^2(\\mu)$. Indeed, we have\n$$ \\lVert T_K(f) \\rVert^{2} = \\int_{\\X} \\left\\lvert \\int_{\\X} K(x, y) f(y) \\dmu(y) \\right\\rvert^2 \\dmu(x) \\leq \\norm{f}^2 \\int_{\\X \\times \\X} \\left\\lvert K(x,y) \\right\\rvert^2 \\dmu(x) \\dmu(y) $$\nwhere we have applied Schwarz inequality (Theorem-1) as follows $$ \\left\\lvert \\int_{\\X} K(x, y) f(y) \\dmu(y) \\right\\rvert^2 = \\left\\lvert T_K(f)(x) \\right\\rvert^2 = \\left\\lvert \\inner{K(x, \\cdot)}{f} \\right\\rvert^2 \\leq \\norm{K(x, \\cdot)}^2 \\norm{f}^2$$\nOperators of this type are known as Hilbert-Schmidt operators.\nWe are now ready to state the Mercer\u0026rsquo;s theorem.\nTheorem 5 [Mercer\u0026rsquo;s theorem]: Suppose that $\\X$ is a compact metric space, and $K : \\X \\times \\X \\to \\C$ is a Mercer\u0026rsquo;s kernel that satisfies the Hilbert-Schmidt condition. Then there exists an at most countable set of eigenfunctions $ (e_{i})_{i} $ for $ T_K $ that form an orthonormal basis of $L^2(\\mu)$, and a corresponding set of non-negative eigenvalues $ (\\lambda_{i})_{i} $ such that\n$$ T_K(e_i) = \\lambda_i e_i, \\quad (i \\in \\N) $$\nMoreover, $K$ has the expansion\n$$ K(x,y) = \\sum_{i} \\lambda_i e_i(x) e_i(y), \\quad (x,y \\in \\X) $$\nwhere the convergence of the series above holds absolutely and uniformly.\nI\u0026rsquo;ll skip the proof.\nAmong other things, Mercer\u0026rsquo;s theorem provides a framework for embedding an element of $\\X$ into an element of $\\ell^2(\\N)$ (for its definition, see Example-4 in the section on Hilbert spaces in the first part). More concretely, given the eigenfunctions and eigenvalues guaranteed by Mercer\u0026rsquo;s theorem, we may define a mapping $\\Phi : \\X \\to \\ell^2(\\N)$ as follows\n$$ x \\mapsto \\left( \\sqrt{\\lambda_i} e_i(x) \\right)_{i \\in \\N} $$\nTherefore, we have\n$$\\inner{\\Phi(x)}{\\Phi(y)} = \\sum_{i=1}^{\\infty} \\lambda_i e_i(x) e_i(y) = K(x,y) $$\nThis is the well-known \u0026ldquo;kernel trick\u0026rdquo;. Let us connect Mercer\u0026rsquo;s theorem to the Spectral theorem.\nLet $\\X = [d] := \\{1, 2, \\ldots, d\\}$ along with the Hamming metric be our compact metric space. Let $\\mu(\\{i\\}) = 1$ for all $i \\in [d]$ be the counting measure on $\\X$. Any function $f : \\X \\to \\C$ is equivalent to the $d$-dimensional vector $[f(1), \\ldots, f(d)]$, and any kernel function $K : \\X \\times \\X \\to \\C$ is continuous, satisfies the Hilbert-Schmidt condition, and is equivalent to a $d \\times d$ normal matrix $\\matr{K}$ where $\\matr{K}_{i,j} = K(i, j)$. The Hilbert-Schmidt operator reduces to\n$$ T_K(f)(x) = \\int_{\\X} K(x, y) f(y) \\dmu(y) = \\sum_{i=1}^{d} K(x,y) f(y) $$\nMercer\u0026rsquo;s theorem then states that there exists a set of eigenfunctions $v_1, \\ldots, v_d$ (for our $\\X$ they are equivalent to vectors) and the corresponding eigenvalues $\\lambda_1, \\ldots, \\lambda_d$ such that\n$$ \\matr{K} = \\sum_{i=1}^{d} \\lambda_i v_i v_i^T $$\nwhich is exactly the spectral theorem.\nOperations on kernels Let us now consider how various algebraic operations on kernels affect the corresponding Hilbert spaces. All this (and a lot more) can be found in the seminal paper by Aronszajn \u0026ldquo;Theory of Reproducing Kernels\u0026rdquo;.\nI state the following theorems without proof to illustrate how operations on kernels are done.\nSums of kernels Theorem 6: Suppose that $\\H_1$ and $\\H_2$ are both RKHSs with kernels $K_1$ and $K_2$, respectively. Then the space\n$$\\H = \\H_1 + \\H_2 := \\{f_1 + f_2 \\, : \\, f_1 \\in \\H_1 \\text{ and } f_2 \\in \\H_2 \\}$$\nwith the norm\n$$ \\norm{f}^{2}_{\\H} := \\inf \\left\\{ \\lVert f_1 \\rVert^{2} _ {\\H_1} + \\norm{f_2}^{2} _{\\H_2} \\, : \\, f = f_1 + f_2, f_1 \\in \\H_1, f_2 \\in \\H_2 \\right\\} $$\nis an RKHS with the kernel $K = K_1 + K_2$.\nProducts of kernels Let us first define the notion of tensor product of two (separable) Hilbert spaces $\\H_1$ and $\\H_2$ of functions, say with domains $\\X_1$ and $\\X_2$.\nDefinition 27: Consider the set of functions $h : \\X_1 \\times \\X_2 \\to \\C$ satisfying\n$$\\H = \\left\\{ h = \\sum_{i=1}^{n} u_i v_i \\, : \\, n \\in \\N \\text{ and } u_i \\in \\H_1, v_i \\in \\H_2 \\text{ for all } i \\in [n] \\right\\} $$\nWe define an inner product on $\\H$ as follows: for $h = \\sum_{i=1}^{n} u_i v_i$ and $g = \\sum_{j=1}^{m} w_j x_j$ in $\\H$ define\n$$ \\inner{h}{g} := \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\inner{u_i}{w_j}_{\\H_1} \\inner{v_i}{x_j}_{\\H_2} $$\nThen $\\H$ is a Hilbert space and is called the tensor product of $\\H_1$ and $\\H_2$. We denote it by $\\H = \\H_1 \\otimes \\H_2$.\nWe can now state the theorem for product of kernels.\nTheorem 7: Suppose that $\\H_1$ and $\\H_2$ are RKHSs of real-valued functions with domains $\\X_1$ and $\\X_2$, and equipped with kernels $K_1$ and $K_2$, respectively. Then the tensor product space $\\H = \\H_1 \\otimes \\H_2$ is an RKHS of functions with domain $\\X_1 \\times \\X_2$, and with kernel function $K : (\\X_1 \\times \\X_2) \\times (\\X_1 \\times \\X_2) \\to \\C$ defined by\n$$ K((x,s), (y,t)) := K_1(x,y) K_2(s,t) $$\n$K$ is called the tensor product of the kernels $K_1$ and $K_2$, and denoted by $K = K_1 \\otimes K_2$.\nOther operations We can similarly define more operations on kernels:\n If $K$ is a valid kernel and $\\alpha \\geq 0$, then $\\alpha K$ is a valid kernel. If $K$ is a valid kernel and $\\alpha \\geq 0$, then $K + \\alpha$ is a valid kernel. We can easily see from all these results that a linear combination or more generally for any polynomial $P$ with positive coefficients, the composition $P \\circ K$ is a valid kernel if $K$ is a valid kernel. If $K$ is a valid kernel, then $\\exp(K)$ is a valid kernel.  Representer theorem We are now at a stage where we can put all this theory to use in machine learning. Specifically we will develop Representer theorem which allows many optimization problems over the RKHS to be reduced to relatively simple calculations involving the gram matrix.\nLet us start with a functional analytic viewpoint of supervised learning. Suppose we are given empirical data\n$$ (x_1, y_1), \\ldots, (x_n, y_n) \\in \\X \\times \\Y $$\nwhere $\\X$ is a nonempty set. For now let $\\Y = \\R$. They are from an unknown function, $g : \\X \\to \\R$, i.e., we assume\n$$ y_i = g(x_i), \\quad (i \\in [n]) $$\nWe need to find some function $f^*$ which \u0026ldquo;best\u0026rdquo; approximates $g$. A natural way to formalize the notion of \u0026ldquo;best\u0026rdquo; is to limit ourselves to an RKHS $\\H$ which contains functions of the form $f : \\X \\to \\R$ and choose\n$$ f^* = \\argmin_{f \\in \\H} \\norm{f} \\quad \\text{ such that } f^*(x_i) = y_i \\text{ for } i \\in [n]$$\nThis optimization problem is feasible whenever there exists at least one function $f \\in \\H$ that fits the data exactly. Denote by $y$ the vector $[y_1, \\ldots, y_n]^T$. It can be shown that if $\\matr{K}$ is the gram matrix of the kernel $K$ with respect to $x_1, \\ldots, x_n$, then the feasibility is equivalent to $y \\in \\text{range}(\\matr{K})$. This is a special of the representer theorem.\nIn a realistic setting we assume that we have noisy observations, i.e.,\n$$y_i = g(x_i) + \\e_i, \\quad (i \\in [n])$$\nwhere $\\e_i$'s denote the noise. Then the constraint of exact fitting is no longer desirable, and we model the \u0026ldquo;best\u0026rdquo; approximation by introducing a loss function which represents how close our approximation is to the observed outputs. More concretely, let $L_y : \\R^n \\to \\R$ be a continuous function. Then we can define our cost function as\n$$ J(f) = \\norm{f}^2 + L_y(f(x_1), \\ldots, f(x_n)) $$\nTheorem 8 [Representer theorem]: If $f^*$ is a function such that\n$$ J(f^*) = \\inf_{f \\in \\H} J(f) $$\nthen $f^*$ is in the span of the functions $k_{x_1}, \\ldots, k_{x_n}$, i.e.,\n$$ f^*(\\cdot) = \\sum_{i=1}^{n} \\alpha_i k_{x_i}(\\cdot) \\quad \\text{for some } \\alpha_1, \\ldots, \\alpha_n \\in \\C$$\nI\u0026rsquo;ll skip the proof which can be found in Paulsen and Raghupathi\u0026rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces or in Schölkopf, Herbrich and Smola\u0026rsquo;s A Generalized Representer Theorem.\nAs an example $L$ could be the squared loss $L_y(f(x_1), \\ldots, f(x_n)) = \\sum_{i=1}^n (y_i - f(x_i))^2$. If we assume $L$ to be convex then the solution exists and is unique. Recall that $E \\subseteq \\R^k$ is a convex set if $\\lambda x + (1-\\lambda) y \\in E$ whenever $x,y \\in E$ and $0 \u0026lt; \\lambda \u0026lt; 1$. A real-valued function $L$ defined on a convex set $E$ is a convex function if $L(\\lambda x + (1-\\lambda) y) \\leq \\lambda L(x) + (1-\\lambda) L(y)$ whenever $x, y \\in E$ and $0 \u0026lt; \\lambda \u0026lt; 1$. Note that a convex function is continuous.\nThe Representer theorem allows us to reduce the infinite-dimensional problem of optimizing over an RKHS to an $n$-dimensional problem.\nEpilogue I barely scratched the surface of reproducing kernel Hilbert space theory. Some resources I recommend that do a much better job than me in explaining this theory and which I used as reference are:\n Paulsen V and Raghupathi M: An Introduction to the Theory of Reproducing Kernel Hilbert Spaces Wainwright M: High-Dimensional Statistics Schölkopf B, Herbrich R and Smola A.J: A Generalized Representer Theorem (COLT 2001) Aronszajn N: Theory of Reproducing Kernels (1950)  In the next article I will discuss kernel approximation methods. In particular I will focus on random Fourier Features developed by Rahimi and Recht which I am using in my work.\n","date":1594930823,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594930823,"objectID":"e6a68300e4479c6d14bd6375724ab9ea","permalink":"https://makkar.github.io/post/kernels1/","publishdate":"2020-07-16T16:20:23-04:00","relpermalink":"/post/kernels1/","section":"post","summary":"Some results in Reproducing Kernel Hilbert Spaces","tags":[],"title":"Kernels - Part 1","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\conj}[1]{\\overline{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} $$\nIntroduction The word \u0026ldquo;kernel\u0026rdquo; is heavily overloaded, but for our purposes it is, intuitively, a similarity measure that can be thought of as an inner product in some feature space. Kernel methods provide an elegant, theoretically well-founded, and powerful approach to solving many learning problems.\nWe usually have the following framework: The input space $\\X$ which contains our observations/inputs/features is either not rich enough (for example, if there is no linear boundary separating the two classes in a binary classification problem) or not convenient (for example, if our inputs are strings), and therefore we want to work is some other space we call feature space $\\H$. Suppose we have a map which takes our inputs from $\\X$ to $\\H$: $$ \\Phi : \\X \\to \\H $$ Then the class of kernels we are interested in are those for which is it possible to write $$ K(x,y) = \\langle \\Phi(x), \\Phi(y) \\rangle, \\quad x,y \\in \\X $$ What kind of functions, $K : \\H \\times \\H \\to \\C$ admit such a representation? We aim to be able to answer this question and many others in this series of articles on kernels.\nIn this article, I aim to introduce the necessary concepts from analysis, linear algebra, and functional analysis so as to understand Reproducing Kernel Hilbert Spaces (RKHS) and a few of their basic properties. In the next article I will discuss the kernel trick and some important theorems like Mercer\u0026rsquo;s theorem and Representer theorem.\nBackground A topic like this requires quite a bit of background before we can get to the interesting results like the one above. It\u0026rsquo;s always easy to get lazy and assume all the necessary background from the reader and get straight to the meat, but I want to write an article which I would have found useful had I had it when I started learning about kernel theory. With that being said, I am under no illusion and believe that a much better way to learn this background would be to read a functional analysis text if you have the time.\nLinear spaces Definition 1: A linear space, or alternatively a vector space, over a field $\\F$ ($\\F$ is $\\R$ or $\\C$ for our purposes) is a set $V$ of elements called vectors (the elements of $\\F$ are called scalars) satisfying:\n(A) To every pair, $x$ and $y$, of vectors in $V$ there corresponds a vector $x+y$, called the sum of $x$ and $y$, in such a way that\n addition is commutative, $x+y = y+x$, addition is associative, $x+(y+z) = (x+y)+z$, there exists in $V$ a unique vector $0$ such that $x+0=x$ for every vector $x$, and to every vector $x \\in V$ there corresponds a unique vector $-x$ such that $x+(-x)=0$.  (B) To every pair, $\\alpha \\in \\F$ and $x \\in V$, there corresponds a vector $\\alpha x \\in V$, called the product of $\\alpha$ and $x$, in such a way that\n multiplication by scalars is associative, $\\alpha(\\beta x) = (\\alpha \\beta)x$, and $1x = x$ for every vector $x$.  (C) Finally the distributive properties\n $\\alpha(x+y) = \\alpha x + \\alpha y$, and $(\\alpha + \\beta) x = \\alpha x + \\beta x$.  Examples  The Euclidean spaces $\\R^n$ are vector spaces over the real field. $\\C^n$ are vector spaces over $\\C$. The set of all polynomials, with complex coefficients, in a variable $t$ is a vector space over $\\C$. The set $C$ of all continuous complex functions on the unit interval $[0,1]$ is a vector space over $\\C$.  Definition 2: A linear transformation of a linear space $V$ into a linear space $W$ is a mapping $T: V \\to W$ such that $$ T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y), \\quad (x,y \\in V;\\; \\alpha, \\beta \\in \\F) $$\nDefinition 3: In the special case in which $W$ above is a field, $T$ is called a linear functional.\nNote that we often write $Tx$ instead of $T(x)$, if $T$ is linear, hinting that a linear transformation mapping a finite dimensional vector to another finite dimensional vector space is equivalent to a matrix vector product.\nDefinition 4: Let $\\mu$ be a positive measure on an arbitrary measurable space $X$. We define $L^1(\\mu)$ to be the collection of all complex measurable functions $f$ on $X$ for which $$ \\int_X |f| \\dmu \u0026lt; \\infty $$\nIt can be shown that for every $f, g \\in L^1(\\mu)$ and for every $\\alpha, \\beta \\in \\C$, we have $\\alpha f + \\beta g \\in L^1(\\mu)$, and $$ \\int_X (\\alpha f + \\beta g) \\dmu = \\alpha \\int_X f \\dmu + \\beta \\int_X g \\dmu $$\nThus, $L^1(\\mu)$ is a vector space, and the mapping $F: L^1(\\mu) \\to \\R$ defined by $$ F(f) = \\int_X |f| \\dmu, \\quad f \\in L^1(\\mu) $$ is a linear functional.\nInner products and norms Definition 5: If $V$ be a linear space over $\\C$, an inner product on $V$ is a function $\\langle \\cdot , \\cdot \\rangle: V \\times V \\to \\C$ such that for all $\\alpha, \\beta \\in \\C$, and all $x,y,z \\in V$, the following are satisfied:\n Linearity in the first argument: $\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x,z \\rangle + \\beta \\langle y,z \\rangle$, Conjugate symmetry: $\\inner{x}{y} = \\conj{\\inner{y}{x}}$, Positivity: $\\inner{x}{x} \\geq 0$, If $\\inner{x}{x} = 0$, then $x=0$.  A function satisfying only the first three properties is called a semi-inner product on $V$.\nAn immediate consequences of this definition: For every $y \\in V$, the mapping $F: V \\to \\C$ defined by $$ F(x) = \\inner{x}{y}, \\quad (x \\in V) $$ is a linear functional on $V$.\nDefinition 6: If $V$ be a linear space over $\\C$, a norm on $V$ is a non-negative function $\\norm{\\cdot} : V \\to \\R$ such that for all $\\alpha \\in \\C$, and all $x,y \\in V$, the following are satisfied:\n Subadditivity: $\\norm{x+y} \\leq \\norm{x} + \\norm{y}$, Absolutely homogenous: $\\norm{\\alpha x} = |\\alpha| \\norm{x}$, Positive definite: $\\norm{x} = 0 \\implies x = 0$.  Given an inner product, we can define a norm as follows: $$ \\norm{x} = \\sqrt{\\inner{x}{x}} $$\nA classic result useful in many proofs:\nTheorem 1: Schwarz inequality $$|\\inner{x}{y}| \\leq \\norm{x} \\, \\lVert y \\rVert$$ Equality hold for $y = \\alpha x$ or $y=0$.\nThe proof is not too difficult.\nDefinition 7: The virtue of norm on a vector space $V$ is that $$ d(x,y) := \\norm{ x-y } $$ defines a metric on $V$ so that $V$ becomes a metric space.\nNote that technically the tuple $(V, d)$ defines a metric space, but I will often write just $V$ instead of $(V, d)$ when it\u0026rsquo;s clear from context what the metric $d$ is.\nDefinition 8: If $0 \u0026lt; p \u0026lt; \\infty$, $f$ is a complex measurable function on $X$, and $\\mu$ is a nonnegative measure on $X$, define $$ \\norm{f}_p := \\left( \\int_X |f|^p \\dmu \\right)^{1/p} $$ and let $L^p(\\mu)$ consist of all $f$ for which $\\norm{f}_p \u0026lt; \\infty$. We call $\\norm{f}_p$ the $L^p$-norm of $f$.\nHilbert spaces Definition 9: An inner product space, or alternatively a pre-Hilbert space, is a linear space with an inner product defined on it.\nWe need the concept of completeness to define Hilbert space. But before that let me define Cauchy sequences.\nDefinition 10: Given a metric space $(M, d)$, a sequence $(x_n)_{n \\in \\N}$ of elements in $M$ is called a Cauchy sequence if for every positive real number $\\e \u0026gt; 0$ there exists a positive integer $N \\in \\N$ such that $m, n \u0026gt; N$ implies that $d(x_m, x_n) \u0026lt; \\e$.\nRecall that we say a sequence $(x_n)_{n \\in \\N}$ in a metric space $(M, d)$ converges if there exists a point $x \\in M$ with the following property: for every $\\e \u0026gt; 0$ there exists a positive integer $N \\in \\N$ such that $n \u0026gt; N$ implies that $d(x_n, x) \u0026lt; \\e$.\nIt can be shown that every convergent sequence is a Cauchy sequence: Let the sequence $(x_n)_{n \\in \\N}$ in a metric space $(M, d)$ converge to $x \\in M$. If $\\e \u0026gt; 0$, there is an integer $N \\in \\N$ such that $d(x_n, x) \u0026lt; \\e$ for all $n \u0026gt; N$. Hence\n$$d(x_n, x_m) \\leq d(x_n, x) + d(x, x_m) \u0026lt; 2\\e$$\nif $n,m \u0026gt; N$. Thus $(x_n)_{n \\in \\N}$ is a Cauchy sequence.\nThe converse is not necessarily true. But if it holds in some space, we anoint the space with a special name.\nDefinition 11: A metric space $(M, d)$ is called complete if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.\nDefinition 12: A pre-Hilbert space $\\H$ is called a Hilbert space if it is complete in the metric $d$ (see Definition 7).\nExamples   The sets $\\R^n$ and $\\C^n$ are Hilbert spaces if we define $\\inner{x}{y} := \\sum_{i=1}^n x_i \\conj{y_i}$.\n  The set $C$ of all continuous complex functions on the unit interval $[0,1]$ defined above is an inner product space if we define $$ \\inner{f}{g} := \\int_0^1 f(x) \\conj{g(x)} \\dx $$ but is not a Hilbert space.\n  $L^2(\\mu)$ is a Hilbert space, with inner product $$ \\inner{f}{g} := \\int_X f \\, \\conj{g} \\dmu $$\n  The space of square-summable real-valued sequences, namely $$ \\ell^2(\\N) := \\left\\{ (x_n)_{n \\in \\N} \\; : \\; x_n \\in \\R,\\, \\sum_n x_n^2 \u0026lt; \\infty \\right\\} $$\nThis set, when endowed with the inner product $\\inner{x}{y} := \\sum_{n \\in \\N} x_n y_n$, defines a Hilbert space. It will play an important role in our discussion of eigenfunctions for Reproducing Kernel Hilbert spaces.\n  Definition 13: Consider a linear space $\\FF$ of functions each of which is a mapping from a set $X$ into $\\F$. For $x \\in X$, a linear evaluation functional is a linear functional $E_x$ that is defined as $$ E_x(f) = f(x), \\quad (f \\in \\FF) $$ In other words, a linear evaluation functional with respect to $x \\in X$ evaluates each function at $x$.\nIn general, the evaluation functional is not continuous. This means we can have $f_n \\to f$ but $E_x(f_n)$ does not converge to $E_x(f)$. Intuitively, this is because Hilbert spaces can contain very unsmooth functions. We will later consider a special type of Hilbert space, Reproducing Kernel Hilbert Space where evaluation functional is continuous.\nA lemma that will be useful later on:\nLemma 1: Let $\\H$ be a Hilbert space and $L:\\H \\to \\F$ a linear functional. The following statements are equivalent:\n $L$ is continuous. $L$ is continuous at $0$. $L$ is continuous at some point. $L$ is bounded, i.e., there is a constant $c \u0026gt; 0$ such that $|L(f)| \\leq c \\norm{f}$ for every $f \\in \\H$.  Proof: It is clear that $(1) \\implies (2) \\implies (3)$, and $(4) \\implies (2)$. Let\u0026rsquo;s show that $(3) \\implies (1)$, and $(2) \\implies (4)$.\n$(3) \\implies (1)$: Suppose $L$ is continuous at $f$ and $g$ is any point in $\\H$. If $g_n \\to g$ in $\\H$, then $g_n - g + f \\to f$. By assumption $L(f) = \\limn L(g_n - g + f) = \\limn L(g_n) - L(g) + L(f)$. Hence $L(g) = \\limn L(g_n)$.\n$(2) \\implies (4)$: The definition of continuity at $0$ implies that $L^{-1}(\\{\\alpha \\in \\F : |\\alpha| \u0026lt; 1\\})$ contains an open ball centered at $0$. Let $\\delta \u0026gt; 0$ be the radius of that open ball centered at $0$. Then for $f \\in \\H$ and $\\norm{f} \u0026lt; \\delta$ we have $|L(f)| \u0026lt; 1$. If $f$ is an arbitrary element of $\\H$ and $\\e \u0026gt; 0$, then $$ \\left\\lVert \\frac{\\delta f}{\\norm{f} + \\e} \\right\\rVert \u0026lt; \\delta $$ Hence, $$ 1 \u0026gt; \\left\\lvert L\\left( \\frac{\\delta f}{\\norm{f} + \\e} \\right) \\right\\rvert = \\frac{\\delta }{\\norm{f} + \\e} |L(f)| $$ Letting $\\e \\to 0$ we see that $(4)$ holds with $c = 1/\\delta$. $\\square$\nOrthonormal bases We generalize the idea of orthonormal basis to infinite dimensional case. This will be needed when we discuss Mercer\u0026rsquo;s theorem.\nDefinition 14: A collection of vectors $\\{v_{\\alpha} \\, : \\, \\alpha \\in A \\}$ in a Hilbert space $\\H$ for some index set $A$ is called orthonormal if it satisfies $\\inner{v_{\\alpha}}{v_{\\beta}} = \\delta_{\\alpha \\beta}$ where $\\delta_{\\alpha \\beta}$ is the Kronecker delta, which equals $1$ if $\\alpha = \\beta$ and $0$ otherwise.\nDefinition 15: A collection of vectors $\\{v_{\\alpha} \\, : \\, \\alpha \\in A \\}$ in a Hilbert space $\\H$ is complete if for any $u \\in \\H$, $\\inner{u}{v_{\\alpha}} = 0$ for all $\\alpha \\in A$ implies that $u = 0$.\nDefinition 16: An orthonormal basis a complete orthonormal system.\nNote, we can also define an orthonormal basis as a maximal orthonormal set in $\\H$. To say $\\{v_{\\alpha}\\}$ is maximal means that no vector of $\\H$ can be added to $\\{v_{\\alpha}\\}$ in such a way that the resulting set is still orthonormal. This happens precisely when there is no $u \\neq 0$ in $\\H$ that is orthogonal to every $v_{\\alpha}$.\nSeparable Hilbert spaces Another idea we need is separability. Let us define that now.\nDefinition 17: A topological space is called separable if it contains a countable, dense subset; that is, there exists a sequence $(x_{n})_{n=1}^{\\infty }$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.\nDefinition 18: A Hilbert space is separable if and only if it has a countable orthonormal basis. It follows that any separable, infinite-dimensional Hilbert space is isometric to the space $\\ell^2(\\N)$ of square-summable sequences.\nWe will be dealing with separable Hilbert spaces in our discussion.\nRiesz representation theorem We now come to a very important theorem called the Riesz representation theorem. The name Riesz has many theorems attached to it, but the one relevant to us is the following:\nTheorem 2: For each continuous linear functional $L$ on a Hilbert space $\\H$, there exists a unique $g \\in \\H$ such that $$L(f) = \\inner{f}{g}\\, , \\quad (f \\in \\H) $$\nI\u0026rsquo;ll skip the proof as it\u0026rsquo;s not easy and will unnecessarily make this article abstruse.\nSide-note: In the mathematical treatment of quantum mechanics, this theorem can be seen as a justification for the popular bra–ket notation.\nReproducing Kernel Hilbert Spaces (RKHS) Definition 19: Let $\\X$ be a set. We will call a set $\\H$ of functions from $\\X$ to $\\F$ a Reproducing Kernel Hilbert Space (RKHS) on $\\X$ if\n $\\H$ is a vector space, $\\H$ is endowed with an inner product, $\\inner{\\cdot}{\\cdot}$, with respect to which $\\H$ is a Hilbert space, for every $x \\in \\X$, the linear evaluation functional $E_x : \\H \\to \\F$, is bounded (or continuous, as seen from Lemma-1).  If $\\H$ is an RKHS on $\\X$, then an application of the Riesz representation theorem shows that the linear evaluation functional is given by the inner product with a unique vector in $\\H$. Therefore, for each $x \\in \\X$, there exists a unique vector $k_x \\in \\H$, such that for every $f \\in \\H$, $$ f(x) = E_x(f) = \\inner{f}{k_x} $$\nDefinition 20: The function $k_x$ is called the reproducing kernel for the point $x$. The function $K: \\X \\times \\X \\to \\F$ defined by $$ K(x,y) = k_y(x) $$ is called the reproducing kernel for $\\H$.\nNote that we have $$ K(x,y) = k_y(x) = \\inner{k_y}{k_x} = \\conj{\\inner{k_x}{k_y}} = \\conj{K(y,x)}$$\nAlso, $$ \\norm{E_y}^2 = \\norm{k_y}^2 = \\inner{k_y}{k_y} = K(y,y) $$\nExample The first question that comes to mind is if any Reproducing Kernel Hilbert Spaces exist. The following example answers this question in the affirmative.\nWe saw before that $\\C^n$ is a Hilbert space. We can show that $\\C^n$ is in fact an RKHS. Let $\\X = \\{1, 2, \\ldots, n\\}$, then we can view $v \\in \\C$ as a function $V : \\X \\to \\C$, where $V(j) = v_j$. The linear evaluation functionals are of course bounded for every $x \\in \\X$ and we have $$ V(j) = v_j = \\inner{V}{e_j}\\,, \\quad (j \\in \\X) $$ where $e_j$ is a vector with $1$ at $j$th position and $0$ everywhere else. Therefore, the reproducing kernel for the point $x \\in \\X$ is $e_x$ and the reproducing kernel can be thought as the identity matrix.\nCan there be multiple reproducing kernels for an RKHS? The following theorem answers this question.\nTheorem 3: If an RKHS $\\H$ of functions on a set $\\X$ admits a reproducing kernel, $K$, then $K$ is uniquely determined by $\\H$.\nProof: Suppose that there exists another reproducing kernel $K'$ for $\\H$. Then $$ \\norm{k_y - k\u0026rsquo;_y} = \\inner{k_y - k\u0026rsquo;_y}{k_y - k\u0026rsquo;_y} = \\inner{k_y - k\u0026rsquo;_y}{k_y} - \\inner{k_y - k\u0026rsquo;_y}{k\u0026rsquo;_y} = (k_y - k\u0026rsquo;_y)(y) - (k_y - k\u0026rsquo;_y)(y) = 0 $$ for any $y \\in \\X$. In other words, $k_y(x) = k\u0026rsquo;_y(x)$ for every $x \\in \\X$ by the positive definite property of norms and hence the kernel is unique. $\\square$\nEpilogue We covered quite a lot of ground in this blog post but I didn\u0026rsquo;t even define a kernel as we commonly use in machine learning! In the next post I will do that and cover its fundamental properties.\nSome resources I recommend to go into more depth on what\u0026rsquo;s covered here are:\n Halmos, P: Finite-Dimensional Vector Spaces. Rudin, W: Real and Complex Analysis. (Chapter - 4) Rudin, W: Functional Analysis. Conway, J: A course in functional analysis.  ","date":1593826510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593826510,"objectID":"0947cda7e02dd4e13d4c0959ccd45212","permalink":"https://makkar.github.io/post/kernels/","publishdate":"2020-07-03T21:35:10-04:00","relpermalink":"/post/kernels/","section":"post","summary":"Functional analysis background for kernel theory.","tags":[],"title":"Kernels - Part 0","type":"post"},{"authors":[],"categories":[],"content":"I want to show how these two concepts are a single mathematical idea.\nPartition A partition of a non-empty set, $X$, is a disjoint class $\\{X_i\\}_{i \\in I}$ of non-empty subsets of $X$ whose union is $X$. The $X_i$'s are called the partition sets.\nFor example, if $X = \\mathbb{R}$, then $X$ can be partitioned as $$ X = \\bigcup_{n \\in \\mathbb{Z}}[n, n+1) $$\nBinary Relation A binary relation, or simply relation, $R$, in the set $X$ is a subset of $X \\times X$.\nFor $x, y \\in X$, we denote the fact $(x,y) \\in R$ by writing $x R y$. A function may be defined as a special kind of binary relation.\nEquivalence relation Let\u0026rsquo;s assume that a partition of our non-empty set $X$ is given, and we associate with this partition a relation, $\\sim$, in $X$ defined as follows: $x \\sim y$ if $x$ and $y$ belong to the same partition set. It can easily checked that the relation $\\sim$ satisfies:\n $x \\sim x$ for every $x \\in X$ (reflexivity); $x \\sim y \\implies y \\sim x$ (symmetry); $x \\sim y$ and $y \\sim z$ $\\implies$ $x \\sim z$ (transitivity).  Any relation in $X$ which possesses these three properties is called an equivalence relation in $X$.\nExamples  Let $X = \\mathbb{Z}$ and let $x \\sim y$ if $2 | x-y$ for $x,y \\in X$. Then clearly $\\sim$ is an equivalence relation in $X$. Let $X$, $Y$ be any non-empty sets and $f$ be a mapping from $X$ onto $Y$. Let $x \\sim y$ if $f(x) = f(y)$ for $x,y \\in A$. This defines an equivalence relation in $X$. Indeed, $f(x) = f(x)$, and so $x \\sim x$. If $f(x) = f(y)$ then $f(y) = f(x)$, and so $x \\sim y \\implies y \\sim x$. Finally, if $f(x) = f(y)$ and $f(y) = f(z)$ then $f(x) = f(z)$, and so $x \\sim y$ and $y \\sim z$ $\\implies$ $x \\sim z$. The first example is a special case of this one if we take $X = \\mathbb{Z}$, $Y = \\{0,1\\}$ and $f(x) = x \\mod 2$.  \u0026ldquo;Relation\u0026rdquo; to partition We have just seen that each partition of $X$ has associated with it a natural equivalence relation in $X$. Let us now reverse the situation and show that a given equivalence relation in $X$ determines a natural partition of $X$.\nLet $\\sim$ be an equivalence relation in $X$. For every $x \\in X$ define the set $$ [x] := \\{ y \\in X : y \\sim x\\} $$ called the equivalence set of $x$. We show that the class of all distinct equivalence sets forms a partition of $X$.\nBy reflexivity, $x \\in [x]$ for every $x \\in X$, and thus each equivalence set is non-empty and their union is $X$. We now need to show that any two equivalence sets $[x_1]$ and $[x_2]$ are either disjoint or identical. We prove this by showing that if $[x_1]$ and $[x_2]$ are not disjoint then they are identical. To this end, let $z$ be a common element of $[x_1]$ and $[x_2]$. Let $y$ be any element of $[x_1]$. Using transitivity, $$ y \\sim x_1 \\sim z \\sim x_2 $$ Therefore, $y \\in [x_2]$. Since $y$ was an arbitrary element of $[x_1]$, we get $[x_1] \\subseteq [x_2]$. We can similarly show that $[x_2] \\subseteq [x_1]$. In short, $[x_1] = [x_2]$.\nWe have shown that there is no real distinction between partitions of a set and equivalence relations in the set. They are two \u0026ldquo;equivalent\u0026rdquo; approaches for the same mathematical idea. The approach we choose in an application depends entirely on our own convenience.\n","date":1592695005,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592695005,"objectID":"d1bbc0efd74ed683d7616f54612bc205","permalink":"https://makkar.github.io/post/equivalence-relations/","publishdate":"2020-06-20T19:16:45-04:00","relpermalink":"/post/equivalence-relations/","section":"post","summary":"I want to show how these two concepts are a single mathematical idea.","tags":[],"title":"Partitions and Equivalence Relations","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limm}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sn}{\\{s_n\\}} \\newcommand{\\snk}{\\{s_{n_k}\\}} \\newcommand{\\supk}{\\sup_{k \\geq n}} \\newcommand{\\infk}{\\inf_{k \\geq n}} \\newcommand{\\sups}{\\sup_{k \\geq n} s_k} \\newcommand{\\infs}{\\inf_{k \\geq n} s_k} \\newcommand{\\cc}{\\mathsf{c}} $$\nMost of what follows is taken from Rudin\u0026rsquo;s Principles of Mathematical Analysis.\nIntroduction The concept of upper and lower limits (commonly denoted by $\\limsup$ and $\\liminf$ respectively) shows up routinely when discussing the limiting behaviour of a sequence. For example, consider the Big O notation, $\\mathcal{O}$, defined as $f(n) = \\mathcal{O}(g(n))$ iff there exists a positive real number $c$ and an integer $N$ such that for every $n \\in \\Z$, $n \u0026gt; N$, we have $|f(n)| \\leq c g(n)$. This can be succinctly written as $$ \\lims \\frac{|f(n)|}{g(n)} \u0026lt; \\infty $$\nIn this post, I aim to expound on the concept of upper and lower limits so that the second formulation of Big O notation above becomes easier to understand than the first one.\nDefinitions I start with some definitions.\nDefinition 1: A sequence is function defined on the set $\\N$ of positive integers. If $f(n) = x_n$, for $n \\in \\N$, we denote the sequence $f$ by the symbol $\\{x_n\\}$, or sometimes by $x_1, x_2, \\ldots$.\nDefinition 2: A sequence $\\{p_n\\}$ in a metric space $X$ is said to converge if there is a point $p \\in X$ with the following property: For every $\\e \u0026gt; 0$ there is an integer $N$ such that $n \\geq N$ implies that $d(p_n, p) \u0026lt; \\e$. We write this as $\\limm p_n = p$ or as $p_n \\to p$.\nDefinition 3: Let $\\{s_n\\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\geq M$. We then write $s_n \\to +\\infty$. Similarly for $s_n \\to -\\infty$.\nNote: The symbol $\\to$ is now used for certain type of divergent sequences as well.\nDefinition 4: Given a sequence $\\{p_n\\}$, consider a sequence $\\{n_k\\}$ of positive integers, such that $n_1 \u0026lt; n_2 \u0026lt; \\cdots$. Then the sequence $\\{p_{n_k}\\}$, which is a composition of the functions $\\{n_k\\}$ and $\\{p_n\\}$, is called a subsequence of ${p_n}$. If $\\{p_{n_k}\\}$ converges, its limit is called a subsequential limit of $\\{p_n\\}$.\nDefinition 5: Let $\\sn$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system, i.e., $x \\in \\RR := \\R \\cup \\{+\\infty, -\\infty\\}$) such that $s_{n_k} \\to x$ for some subsequence $\\{s_{n_k}\\}$. Therefore, this set contains all the subsequential limits of $\\sn$ plus possibly the numbers $+\\infty$ and $-\\infty$. We define $$ s^* = \\sup E $$ $$ s_* = \\inf E $$ The numbers $s^*$ and $s_*$ are called the upper and lower limits of $\\sn$. We use the notation $$ \\lims s_n = s^* $$ $$ \\limi s_n = s_* $$ It immediately follows that $s_* \\leq s^*$.\nThe fact that $E$ is non-empty (and thus taking $\\sup$ or $\\inf$ makes sense) follows from the observation that either $\\sn$ is bounded or unbounded. If it is bounded then it must contain a convergent subsequence (Bolzano–Weierstrass theorem) and thus at least one element, or if it is unbounded then it must contain either $+\\infty$ or $-\\infty$.\nSome useful lemmas Now let us prove some interesting lemmas that will be useful later.\nLemma 1: The subsequential limits of a sequence $\\{p_n\\}$ in a metric space $X$ form a closed subset of $X$.\nProof: Let $E$ be the set of all subsequential limits of $\\{p_n\\}$ and let $q$ be a limit point of $E$. We have to show that $q \\in E$. To show this we will construct a subsequence of $\\{p_n\\}$ which converges to $q$.\nChoose $n_1$ so that $p_{n_1} \\neq q$. If no such $n_1$ exists, then $E$ has only one element, $q = p_1 = p_2 = \\cdots$, and there is nothing to prove. Define $\\delta = d(q, p_{n_1})$. Suppose $n_1, \\ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E$, there is an $x \\in E$ with $d(q, x) \u0026lt; \\frac{\\delta}{2^i}$. Since $x \\in E$, there is an $n_i \u0026gt; n_{i-1}$ such that $d(x, p_{n_i}) \u0026lt; \\frac{\\delta}{2^i}$. Thus $$ d(q, p_{n_i}) \\leq d(q, x) + d(x, p_{n_i}) \u0026lt; \\frac{\\delta}{2^{i-1}} \\quad \\text{for } i = 1,2,\\ldots $$ This implies $p_{n_k} \\to q$, and thus $q \\in E$. $\\square$\nLemma 2: Let $F$ be a nonempty closed set of real numbers which is bounded above. Let $\\alpha = \\sup F$. Then $\\alpha \\in F$.\nProof: Assume for the sake of the contradiction that $\\alpha \\notin F$. Then since $F^\\cc$ is an open set (because $F$ is closed) there exists an $\\e \u0026gt; 0$ such that $(\\alpha - \\e, \\alpha + \\e) \\subset F^\\cc$. But this implies $\\alpha - \\frac{\\e}{2}$ is an upper bound for $F$ which is lower that $\\alpha$. This gives us our required contradiction. $\\square$\nProperties We now have all the tools to prove a very useful characterization of upper and lower limits and the highlight of this post.\nTheorem 1: Let $\\sn$ be a sequence of real numbers. Let $E$ and $s^*$ have the same meaning as in Definition 5. Then $s^*$ has the following two properties:\n $s^* \\in E$. If $x \u0026gt; s^*$, there is an integer $N$ such that $n \\geq N$ implies $s_n \u0026lt; x$.  Moreover, $s^*$ is the only number with these two properties.\nOf course, an analogous result is true for $s_*$.\nProof: We start by showing the two properties.\n  We divide it into three cases depending on what value $s^*$ takes:\nIf $s^* = +\\infty$, then $E$ is not bounded above; hence $\\sn$ is not bounded above, and thus there is a subsequence $\\{s_{n_k}\\}$ such that $s_{n_k} \\to +\\infty$. Therefore, $+\\infty \\in E$ and thus $s^* \\in E$.\nIf $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists by the definition of $\\sup$. Therefore, $s^* \\in E$ follows from the Lemmas 1 and 2, and the fact that $s^* = \\sup E$.\nIf $s^* = -\\infty$, then $E$ contains only one element, namely $-\\infty$, and there is no subsequential limit. Thus, $s^* \\in E$.\n  Suppose for the sake of contradiction that there is a number $x \u0026gt; s^*$ such that $s_n \\geq x$ for infinitely many values of $n$. Let\u0026rsquo;s denote this set of $n$'s with $\\mathcal{K}$ and let $\\{s_k\\}$$_{k \\in \\mathcal{K}}$ be this subsequence. If $\\{s_k\\}_{k \\in \\mathcal{K}}$ is unbounded then $s^* = +\\infty$ contradicting the fact that there exists an $x \u0026gt; s^*$. And if $\\{s_k\\}_{k \\in \\mathcal{K}}$ is bounded that it contains a convergent subsequence (Bolzano–Weierstrass theorem). Suppose this convergent subsequence converges to $y$. Then $y \\geq x \u0026gt; s^*$. This contradicts the definition of $s^*$.\n  To show the uniqueness, suppose there are two distinct numbers, $p$ and $q$, which satisfy the two properties, and suppose $p \u0026lt; q$. Choose $x$ such that $p \u0026lt; x \u0026lt; q$. Since $p$ satisfies the second property, we have $s_n \u0026lt; x$ for $n \\geq N$. But then $q$ cannot satisfy the second property. $\\square$\nAn intuitive theorem:\nTheorem 2: If $s_n \\leq t_n$ for $n \\geq N$, where $N \\in \\N$ is fixed, then $$\\limi s_n \\leq \\limi t_n,$$ $$\\lims s_n \\leq \\lims t_n$$\nProof: Let $s^* = \\lims s_n$ and $t^* = \\lims t_n$. Suppose for the sake of contradiction $t^* \u0026lt; s^*$. Choose $x$ such that $t^* \u0026lt; x \u0026lt; s^*$. Then by the second property of Theorem 1 there is an integer $N_1$ such that $n \\geq N_1$ implies $t_n \u0026lt; x$. Also by the first property there exists a subsequence $\\snk$ such that $s_{n_k} \\to s^*$. This implies that there exists an integer $N_2$ such that $n \\geq N_2$ implies $x \u0026lt; s_n$. But then for $n \\geq \\max\\{N_1, N_2\\}$ we have $t_n \u0026lt; x \u0026lt; s_n$. This gives us our required contradiction.\nA similar argument can be made for the $\\liminf$ case. $\\square$\nNext we give a necessary and sufficient condition for the convergence of a sequence in terms of its $\\liminf$ and $\\limsup$.\nTheorem 3: For a real-valued sequence $\\sn$, $\\limm s_n = s \\in \\RR$ if and only if $$\\lims s_n = \\limi s_n = s$$\nProof: We divide the analysis into three cases.\nFirst, let $s \\in \\R$. Then if $\\lims s_n = \\limi s_n = s$, Theorem 1 implies that for any $\\e \u0026gt; 0$ we have $s_n \\in (s-\\e, s+\\e)$ for all but finitely many $n$, which means $s_n \\to s$. On the other hand if $s_n \\to s$ then every subsequence $\\snk$ must converge to $s$ and hence $\\lims s_n = \\limi s_n = s$.\nNow let $s = +\\infty$. Then $s_n \\to s$, i.e., for every $M \\in \\R$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\geq M$ if and only $\\limi s_n = +\\infty$, and then $\\lims s_n = +\\infty$ since $\\limi s_n \\leq \\lims s_n$.\nLastly, let $s = -\\infty$. Then $s_n \\to s$, i.e., for every $M \\in \\R$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\leq M$ if and only $\\lims s_n = -\\infty$, and then $\\limi s_n = -\\infty$ since $\\limi s_n \\leq \\lims s_n$. $\\square$\nUpper and lower limits - a reprise There is an equivalent way to express upper and lower limits.\nDefinition 6: Let $\\sn$ be a sequence of real numbers. We define the notation $$\\sups := \\sup \\{ s_k : k \\geq n\\}$$ $$\\infs := \\inf \\{ s_k : k \\geq n\\}$$\nWe note that the sequence $\\{ \\sups \\}$$_{n \\in \\N}$ is monotonically decreasing and the sequence $\\{ \\infs \\}$$_{n \\in \\N}$ is monotonically increasing, and thus their limits exist in $\\RR$.\nWe now show the equivalence of the two ways of looking at upper and lower limits.\nTheorem 4: Let $\\sn$ be a sequence of real numbers. Then $$\\limm \\sups = \\lims s_n$$ $$\\limm \\infs = \\limi s_n$$\nProof: We will prove the first equation. The proof for the second is similar. We prove the equation in two steps.\nLet $S = \\{ s_n : n \\in \\N \\}$. Let\u0026rsquo;s show that $\\sup S = +\\infty$ if and only if $\\lims s_n = + \\infty$. Suppose first that $\\sup S = + \\infty$. Then we construct a subsequence $\\snk$ as follows. We let $n_1 = 1.$ Suppose $n_1, \\ldots, n_{k}$ are chosen and let $$S_k = \\{ n \\in \\N : s_n \\geq \\max\\{s_{n_1}, \\ldots, s_{n_k}, k\\} + 1 \\}$$ Notice that $S_k$ is infinite as otherwise we can find an $M \\in \\R$ such that $s_n \\leq M$ for all $n \\geq 1$, contradicting the fact that $\\sup S = + \\infty$. We pick $n_{k+1}$ to be the smallest element of $S_k$ which is bigger than $n_k$. The resulting subsequence satisfies the condition that $s_{n_k} \\geq k$ for $k \\geq 2$ and thus we conclude that $s_{n_k} \\to +\\infty$ which gives $\\lims s_n = +\\infty$. Now suppose that $\\lims s_n = +\\infty$. From Theorem 1 we can conclude that there exists a subsequence $\\snk$ such that $s_{n_k} \\to +\\infty$. This immediately implies $\\sup S = +\\infty$.\nFor the second step, suppose $\\sup S \u0026lt; + \\infty$. Let $$a_n = \\sups$$ Notice that $a_1 \u0026lt; +\\infty$ and $\\{a_n\\}$ is a monotonically decreasing sequence. Therefore, we have that either $\\{a_n\\}$ is lower bounded in which case it converges to, say, $a$ or it is not in which case $\\limm a_n = -\\infty$. Since $s_n \\leq a_n$, by Theorem 2 we can conclude $$ \\lims s_n \\leq \\lims a_n = \\limm a_n $$ where the last equality follows from Theorem 3. We will now show that $$ \\lims s_n \\geq \\limm a_n $$ which will give us our required equality. If $\\limm a_n = -\\infty$ there is nothing to prove and so we assume that $a \u0026gt; -\\infty$. Let $\\e \u0026gt; 0$ be given and let $$ B = \\{ n \\in \\N : s_n \\geq a - \\e \\} $$ We claim that $B$ is infinite. Indeed, if $B$ were finite we can find $N \\in \\N$ so that $N \\geq \\max(B)$. This will imply that $s_n \\leq a - \\e$ for all $n \\geq N$ and so $a_n \\leq a - \\e$ for $n \\geq N$. But then by Theorem 2 we would conclude $a = \\limm a_n \\leq a-\\e$, which is absurd. Thus $B$ is infinite and we let $\\snk$ be a subsequence of $\\sn$ with $n_k \\in B$. Notice that $\\lims s_{n_k} \\leq \\lims s_n$, since any subsequential limit of $\\snk$ is also a subsequential limit of $\\sn$. This along with Theorem 2 give $$ a - \\e \\leq \\lims s_{n_k} \\leq \\lims s_n $$ Since $\\e$ was arbitrary we conclude that $\\lims s_n \\geq a$, which is what we wanted. $\\square$\nMore properties These theorems open new avenues to discover more properties of upper and lower limits.\nTheorem 5: Let $\\sn$ be a sequence of real numbers. Then $$\\limi s_n = - \\lims (-s_n)$$\nProof: TODO\nSubadditivity of $\\limsup$:\nTheorem 6: For any two real sequences $\\{a_n\\}$ and $\\{b_n\\}$, $$ \\lims (a_n + b_n) \\leq \\lims a_n + \\lims b_n $$ provided the sum on the right is not of the form $\\infty - \\infty$.\nProof: If $\\lims a_n = \\infty$ then as by assumption the right side is not $\\infty - \\infty$, it is $\\infty$ and there is nothing to prove. Similarly for the case $\\lims b_n = \\infty$. We may thus assume that $$ \\lims a_n = A \u0026lt; \\infty \\text{ and } \\lims b_n = B \u0026lt; \\infty $$ We note that $\\sup_{k \\geq 1} a_k \u0026lt; \\infty$, $\\sup_{k \\geq 1} b_k \u0026lt; \\infty$, and so TODO\nSuperadditivity of $\\liminf$:\nTheorem 7: For any two real sequences $\\{a_n\\}$ and $\\{b_n\\}$, $$ \\limi (a_n + b_n) \\geq \\limi a_n + \\limi b_n $$ provided the sum on the right is not of the form $\\infty - \\infty$.\nProof: TODO\n","date":1592426033,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592426033,"objectID":"4fb8d43db098b4713ceef892d1d72774","permalink":"https://makkar.github.io/post/upper-and-lower-limits/","publishdate":"2020-06-17T16:33:53-04:00","relpermalink":"/post/upper-and-lower-limits/","section":"post","summary":"A reference for a useful concept.","tags":[],"title":"Upper and Lower Limits","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c7634463bc503571b81622dfbdaae48f","permalink":"https://makkar.github.io/hilbert/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/hilbert/","section":"","summary":"Blog","tags":null,"title":"Hilbert's Hotel","type":"widget_page"},{"authors":null,"categories":null,"content":"Title: Bayesian nonparametric ensemble Date: June 17, 2020 Tags: bayesian Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).\nIntroduction $$ \\mathbb{E}[X] = \\int_{\\Omega} X ;\\text{dP} $$\nProblem Setting BNE ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"79a589979f0bc90d50d8da1239fec4e1","permalink":"https://makkar.github.io/research/bayesian-ensemble/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/bayesian-ensemble/","section":"research","summary":"Title: Bayesian nonparametric ensemble Date: June 17, 2020 Tags: bayesian Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).","tags":null,"title":"","type":"research"}]