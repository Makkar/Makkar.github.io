[{"authors":["admin"],"categories":null,"content":"I am a graduate student at Columbia University interested in theoretical machine learning. I am affiliated with the Electrical Engineering Department and the Data Science Institute and am part of a thriving machine learning community here at Columbia. I am advised by Prof. John Paisley.\nPreviously, I was at Goldman Sachs working with Dr. Howard Karloff on machine learning applications for surveillance models. I received my Bachelors degree from Indian Institute of Technology (IIT) Delhi.\nIf you\u0026rsquo;d like to chat, feel free to write me at a \u0026lsquo;dot\u0026rsquo; makkar \u0026lsquo;at\u0026rsquo; columbia \u0026lsquo;dot\u0026rsquo; edu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://makkar.github.io/author/aditya-makkar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/aditya-makkar/","section":"authors","summary":"I am a graduate student at Columbia University interested in theoretical machine learning. I am affiliated with the Electrical Engineering Department and the Data Science Institute and am part of a thriving machine learning community here at Columbia.","tags":null,"title":"Aditya Makkar","type":"authors"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\bF}{\\mathbb{F}} \\newcommand{\\C}{\\mathcal{C}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\ZZ}{\\Z_{\\geq 0}^N} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\NN}{\\mathcal{N}} \\newcommand{\\LL}{\\mathcal{L}} \\newcommand{\\PP}{\\mathbb{P}} \\newcommand{\\QQ}{\\mathbb{Q}} \\newcommand{\\OO}{\\mathcal{O}} \\newcommand{\\I}{\\mathcal{I}} \\newcommand{\\sP}{\\mathscr{P}} \\newcommand{\\sO}{\\mathscr{O}} \\newcommand{\\Prob}[1]{\\bP\\left( #1 \\right)} \\newcommand{\\eq}[1]{\\begin{align*}#1\\end{align*}} \\newcommand{\\eql}[1]{\\begin{align}#1\\end{align}} \\newcommand{\\ind}[1]{\\mathbf{1}_{#1}} \\newcommand{\\indo}[1]{\\mathbf{1}_{#1}(\\omega)} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\G}{\\mathcal{G}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\cH}{\\mathcal{H}} \\newcommand{\\HH}{\\mathcal{H}} \\newcommand{\\M}{\\mathcal{M}} \\newcommand{\\A}{\\mathcal{A}} \\newcommand{\\E}{\\mathcal{E}} \\newcommand{\\cR}{\\mathcal{R}} \\newcommand{\\cK}{\\mathcal{K}} \\newcommand{\\K}{\\mathscr{K}} \\newcommand{\\PO}{\\mathfrak{P}} \\newcommand{\\f}{\\mathfrak{f}} \\newcommand{\\probsp}{(\\Omega, \\F, \\PP)} \\newcommand{\\integ}[1]{\\int_{\\Omega} #1 \\dmu} \\newcommand{\\B}{\\mathcal{B}} \\newcommand{\\Bo}{\\B(\\R)} \\newcommand{\\Bon}[1]{\\B(\\R^{#1})} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sumn}{\\sum_{n=1}^{\\infty}} \\newcommand{\\trans}{\\intercal} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\ex}[1]{\\exp\\left{#1\\right}} \\newcommand{\\comp}{\\mathsf{c}} \\newcommand{\\emp}{\\varnothing} \\newcommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor} \\newcommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil} \\newcommand{\\se}[1]{\\left\\{ #1 \\right\\}} \\newcommand{\\set}[2]{\\left\\{ #1 \\; : \\; #2 \\right\\}} \\newcommand{\\sett}[2]{\\left\\{ #1 \\; | \\; #2 \\right\\}} \\newcommand{\\Ex}[1]{\\bE\\left[#1\\right]} \\newcommand{\\Exc}[2]{\\bE\\left(#1 \\mid #2\\right)} \\newcommand{\\Pc}[2]{\\PP\\left( \\left. #1 \\, \\right\\vert \\, #2\\right)} \\newcommand{\\pard}[2]{\\frac{\\partial #1}{\\partial #2}} \\newcommand{\\dd}{\\mathrm{d}} \\newcommand{\\PM}{\\widehat{\\otimes}_p} \\newcommand{\\nn}{{n \\in \\N}} \\newcommand{\\oi}{[0, \\infty)} \\newcommand{\\gr}[1]{[\\![ #1 ]\\!]} \\newcommand{\\grr}[1]{]\\!] #1 ]\\!]} \\newcommand{\\grl}[1]{[\\![ #1 [\\![} \\newcommand{\\grrl}[1]{]\\!] #1 [\\![} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dy}{dy} \\DeclareMathOperator{\\du}{du} \\DeclareMathOperator{\\dz}{d\\matr{z}} \\DeclareMathOperator{\\dt}{dt} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator{\\dP}{d\\PP} \\DeclareMathOperator{\\dQ}{d\\QQ} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\Ord}{\\mathcal{O}} \\DeclareMathOperator{\\bE}{\\mathbb{E}} \\DeclareMathOperator{\\bP}{\\mathbb{P}} \\DeclareMathOperator{\\V}{\\mathrm{Var}} \\DeclareMathOperator{\\Log}{\\mathrm{Log}} $$\nI have been attending a reading course on stochastic analysis led by Professor Ioannis Karatzas, where the students take turn in presenting a topic of their choice. I recently presented on Choquet\u0026rsquo;s theory of capacities and its applications to measure theory and in the general theory of processes. This blog post is based on this presentation. I have freely copied * from many sources, but my primary reference are the unfortunately unpublished lecture notes by Prof. Karatzas.\nIntroduction Kolmogorov laid the modern axiomatic foundations of probability theory with the German monograph Grundbegriffe der Wahrscheinlichkeitsrechnung which appeared in Ergebnisse Der Mathematik in 1933. This was a period of intense discussions on the foundations of probability, and a majority of probabilists at the time considered measure theory not only a waste of time, but also an offense to \u0026ldquo;probabilistic intuition\u0026rdquo; [1]. But by 1950, with the work of Doob in particular, these discussions of foundations had been settled.\nContinuous-time processes, on the other hand, were difficult to tame even with measure theory: if a particle is subject to random evolution, to show that its trajectory is continuous, or bounded, requires that all time values be considered, whereas classical measure theory can only handle a countable infinity of time values. Thus, not only does probability depend on measure theory, but it also requires more of measure theory than the rest of analysis [1].\nThe missing pieces of the puzzle, which will be the highlight of this and the next blog post, are the debut, section and projection theorems. These theorems are also indispensable in many applications, for instance in dynamic programming and stochastic control [3].\nTo get a taste of these theorems, let\u0026rsquo;s recall a famous error made by Lebesgue in the paper Sur les fonctions repr√©sentables analytiquement published in 1905. Consider the measurable space $(\\R^2, \\B(\\R^2))$ and the projection map $\\pi$ given by $\\R^2 \\ni (x,y) \\mapsto \\pi(x,y) = y \\in \\R$. It is easy to see that for any open set $O$ in $\\R^2$, the set $\\pi(O)$ is also open in $\\R$ : Recall that the standard topology on $\\R^2$ is same as the product topology on $\\R^2$. By definition of the product topology on $\\R^2$ an open set $O$ in $\\R^2$ is of the form $O = \\bigcup_{i \\in I} \\bigcap_{j \\in J_i} U_{ij} \\times V_{ij}$ for open $U_{ij}, V_{ij}$ in $\\R$, $I$ arbitrary and $J_i$ finite. A simple argument gives $\\pi_2(O) = \\bigcup_{i \\in I} \\bigcap_{j \\in J_i} U_{ij}$ which is open in $\\R$. In fact, more generally projection from product topology is open. Now it seems reasonable to expect that for any Borel set $B \\in \\B(\\R^2)$ its projection is also a Borel set in $\\B(\\R)$, and Lebesgue assumed this in his paper. But, in fact, this is FALSE! The error was spotted in around 1917 by Mikhail Suslin, who realised that the projection need not be Borel, and this lead to his investigation of analytic sets and to begin the study of what is now known as descriptive set theory [2].\nThe problem is projection doesn\u0026rsquo;t commute with countable decreasing intersection. For example, consider the decreasing sequence of sets $S_n = (0, 1/n) \\times \\R$. Then $\\pi(S_n) = \\R$ for all $n$, giving $\\bigcap_\\nn \\pi(S_n) = \\R$, but $\\bigcap_\\nn S_n = \\emp$, giving $\\pi \\left( \\bigcap_\\nn S_n \\right) = \\emp$. The measurable projection theorem stated next will be one of the highlights of this post.\nMeasurable Projection Theorem:  Let $\\probsp$ be a complete probability space, let $(K, \\B(K))$ be a locally compact separable metric space endowed with the collection of its Borel sets, and denote by $\\pi$ the projection of $K \\times \\Omega$ onto $\\Omega$. Then, for every $B \\in \\B(K) \\otimes \\F$, the projection $\\pi(B) \\in \\F$.\nWhy is proving such results difficult? As mentioned above it\u0026rsquo;s because projection doesn\u0026rsquo;t behave nicely with intersections. Nevertheless, let us try to see how one might try to prove the above theorem. A standard approach in measure theory is to construct a collection like\n$$ \\mathscr{E} = \\set{S \\subseteq K \\times \\Omega}{\\pi(S) \\in \\F} $$\nwhich contains the sets satisfying the desired property, and show that it is a $\\sigma-$algebra containing a simple collection, say $\\mathscr{A},$ such that it easy to show that elements of $\\mathscr{A}$ satisfy the desired property and $\\mathscr{A}$ generates $\\B(K) \\otimes \\F$, because then we will have $\\B(K) \\otimes \\F \\subseteq \\mathscr{E}$. To this end, let\n$$ \\mathscr{A} = \\set{S \\subseteq K \\times \\Omega}{S = E \\times F \\text{ for } E \\in \\B(K), F \\in \\F} $$\nThen it is easy to see that $\\mathscr{A} \\subseteq \\mathscr{E}$, $\\mathscr{A}$ generates $\\B(K) \\otimes \\F$ and that $\\mathscr{A}$ is an algebra. If we could show that $\\mathscr{E}$ is a monotone class, then we would be done on account of monotone class theorem. Increasing sequences are easily handled, in fact if $\\se{S_n} _ \\nn \\subseteq K \\times \\Omega$ is any sequence, then\n$$ \\pi\\left(\\bigcup_{n=1}^\\infty S_n\\right) = \\bigcup_{n=1}^\\infty \\pi(S_n) $$\nBut if $S_1 \\supseteq S_2 \\supseteq \\cdots$, then in general we CANNOT say\n$$ \\pi\\left(\\bigcap_{n=1}^\\infty S_n\\right) = \\bigcap_{n=1}^\\infty \\pi(S_n) $$\nEnter Choquet\u0026rsquo;s theory of capacities. It provides the language to prove results like these. We know that every finite measure $\\mu$ on $\\R^d$ has the interior regularity property\n$$ \\mu(B) = \\sup_{\\substack{K \\in \\K(\\R^d) \\\\ K \\subseteq B}} \\mu(K), \\quad \\forall \\; B \\in \\B(\\R^d) $$\nwhere $\\K(\\R^d)$ is the collection of compact sets in $\\R^d$. The Choquet\u0026rsquo;s theory of capacities generalizes this approximation-from-below property, and distills those properties of measure that allow for such approximation to hold in very general settings. As we will see, monotonicity and continuity from above and below properties are at play here, and notions corresponding to complements or differences will be absent.\nThe next few sections will be very abstract and it is easy to lose sight of our goal. Some people enjoy this mental gymnastics, but even if you find this dry, the reward at the end will be worth the initial struggle. We start with Choquet\u0026rsquo;s theory of capacities. The highlight of this part will be Choquet\u0026rsquo;s capacitability theorem. To prove this major result we will need to define a lot of new terminology and prove some major results like Sierpi≈Ñski\u0026rsquo;s theorem and Sion\u0026rsquo;s theorem. Armed with Choquet\u0026rsquo;s capacitability theorem we will prove Measurable Section theorem which in turn will form the backbone of various other results in measure theory. These results in measure theory will then help us prove results in general theory of processes, but this part we will discuss in the next blog post.\nPavings and Mosaics Definition 1:  Let $E$ be a nonempty set. A collection $\\E$ of subsets of $E$ is called a paving if it closed under finite unions and finite intersections. The pair $(E, \\E)$ is then called a paved space.\nThe concept of paving generalizes the concept of algebra. It is easy to check that an arbitrary intersection of pavings is also a paving, and that the collection $\\PO(E)$ of all subsets of $E$ is a paving. Thus for any collection $\\A$ of subsets of $E$, we can define the notion of paving generated by $\\A$ as the smallest paving of subsets of $E$ that contains $\\A$ by simply defining it to be the intersection of all pavings of $E$ containing $\\A$.\nDefinition 2:  For two paved spaces $(E,\\E)$ and $(F,\\F)$, the product paving of $\\E$ and $\\F$, denoted by $\\E \\otimes_p \\F$, is a paving on $E \\times F$ generated by all rectangles $\\cR = \\set{A \\times B}{A \\in \\E, B \\in \\F}$.\nUsing the fact that $(A_1 \\times B_1) \\cap (A_2 \\times B_2) = (A_1 \\cap A_2) \\times (B_1 \\cap B_2)$ we see that $\\cR$ is stable under finite intersections. Therefore, any element of $\\E \\otimes_p \\F$ is of the form $\\bigcup_{i=1}^n A_i \\times B_i$, where $A_i \\in \\E$ and $B_i \\in \\F$ for all $i=1,\\ldots,n$.\nDefinition 3:  Let $E$ be a nonempty set. A collection of subsets of $E$ which is closed under countable unions and countable intersections is called a mosaic.\nThe concept of mosaic generalizes the concept of $\\sigma-$algebra. Just like paving, it is easy to define the notion of mosaic generated by a collection. They will always occur in the context of a paving $\\E$ on $E$. We denote by $\\widehat{\\E}$ the mosaic generated by $\\E$. $\\E \\PM \\F$ will denote the mosaic generated by the product paving $\\E \\otimes_p \\F$.\nHenceforth the notation $\\E$ will be used for a paving on $E$.\nProperties of pavings and mosaics Just like the results connecting algebra and $\\sigma-$algebra, we have results connecting pavings and mosaics.\nLemma 1:  If $A \\in \\E$ implies $A^\\comp = E \\setminus A \\in \\widehat{\\E}$, then $\\widehat{\\E} = \\sigma(\\E)$.\nProof:  Follows immediately from the monotone class theorem. $\\square$\nAs an example, if $E$ is a separable, locally compact metric space, and $\\E = \\K(E)$ is the collection of compact subsets of $E$, then this property holds. Too see this, note that in metric spaces compact sets are closed and thus their complement is open. It is a standard result in topology that in this case every open set is a countable union of compact sets. In fact, every second-countable locally compact Hausdorff space is $\\sigma-$compact.\nLemma 2:  The mosaic $\\widehat{\\E}$ is the smallest collection of subsets of $E$ that contains $\\E$ and is closed under countable increasing unions and under countable decreasing intersections. In other words, if $\\M(\\E)$ denotes the monotone class generated by $\\E$, then $\\widehat{\\E} = \\M(\\E)$.\nProof:  Since a mosaic is a monotone class and since $\\E \\subseteq \\widehat{\\E}$, we have $\\M(\\E) \\subseteq \\widehat{\\E}$. For the other side, we will be done if we show that $\\M(\\E)$ is a mosaic, since $\\E \\subseteq \\M(\\E)$.\nThe first property to note is that a monotone paving is a mosaic. To see this, let $\\cR$ be a monotone paving and $A_1, A_2, \\ldots \\in \\cR$. Then since $\\cR$ is a paving, $B_n = \\bigcup_{i=1}^n A_i \\in \\cR$ for all $\\nn$. But $\\se{B_n}_\\nn$ is an increasing sequence and $\\cR$ is a monotone class, and therefore their union $\\bigcup_\\nn A_n = \\bigcup_\\nn B_n \\in \\cR$, and hence $\\cR$ is closed under countable unions. Similarly for countable intersections.\nTherefore, we will be done if we show that $\\M(\\E)$ is a paving. To this end, for any $B \\subseteq E$, let $$\\cK(B) := \\set{A \\subseteq E}{ A \\cup B, A \\cap B \\in \\M(\\E)}$$ Notice that by symmetry, $A \\in \\cK(B) \\iff B \\in \\cK(A)$. If $A_1 \\subseteq A_2 \\subseteq \\cdots \\in \\cK(B)$ is an increasing sequence, then $$ \\bigcup_\\nn A_n \\cup B = \\bigcup_\\nn (A_n \\cup B) \\in \\M(\\E) $$ $$ \\bigcup_\\nn A_n \\cap B = \\bigcup_\\nn (A_n \\cap B) \\in \\M(\\E) $$ and similarly for decreasing sequences. Therefore, if $\\cK(B) \\neq \\emp$, then it is a monotone class. If $A, B \\in \\E$, then by the definition of paving, $A \\in \\cK(B)$. Since this is true for every $A \\in \\E$, we have $\\E \\subseteq \\cK(B)$, and $\\cK(B)$ is a monotone class containing $\\E$. Since $\\M(\\E)$ is the smallest monotone class containing $\\E$, we have $$\\M(\\E) \\subseteq \\cK(B)$$ Hence if $A \\in \\M(\\E)$ and $B \\in \\E$, then $A \\in \\cK(B)$, and therefore $B \\in \\cK(A)$. Since this is true for every $B \\in \\E$, it follows that $$\\M(\\E) \\subseteq \\cK(A)$$ The validity of this relation for every $A \\in \\M(\\E)$ is equivalent to the assertion that $\\M(\\E)$ is a paving. $\\square$\nCompact pavings Definition 4:  A paving $\\E$ is a compact paving, if every decreasing sequence of nonempty elements of $\\E$ has a nonempty intersection.\nFor example, if $E$ is a separable metric space, the collection, $\\K(E)$, of all compact subsets of $E$ is a compact paving. This is easy to see in light of the fact that every compact subset of $E$ is closed and this allows use of Cantor\u0026rsquo;s intersection theorem. We define $$\\E_\\delta := \\set{\\bigcap_{\\nn} A_n}{A_n \\in \\E \\text{ for all } n \\in \\N}$$ and note that if $\\E$ is a compact paving then so is $\\E_\\delta$. The next lemma tells us when it acceptable to commute projection with countable intersections.\nLemma 3:  Let $K$ and $E$ be two nonempty sets, and denote by $\\pi$ the projection of $K \\times E$ onto $E$. Suppose that $\\cH$ is a paving of subsets of $K \\times E$ with the property that, for every $x \\in E$, the collection $\\cH(x) := \\set{H(x)}{H \\in \\cH}$ is a compact paving on $K$.\nThen, for every decreasing sequence $\\se{H_n} _ {n \\in \\N}$ of sets in the paving $\\cH_\\delta$, we have $$\\pi\\left(\\bigcap _ {n \\in \\N} H_n\\right) = \\bigcap _ {n \\in \\N} \\pi(H_n)$$ Proof:  $\\pi\\left(\\bigcap_{n \\in \\N} H_n\\right) \\subseteq \\bigcap_{n \\in \\N} \\pi(H_n)$ is easy to see because if $x \\in \\pi\\left(\\bigcap_{n \\in \\N} H_n\\right)$ then there exists $(y,x) \\in K \\times E$ such that $(y,x) \\in \\bigcap_{n \\in \\N} H_n$ which implies $x \\in \\pi(H_n)$ for all $n \\in \\N$.\nFor the other side, let $x \\in \\bigcap_{n \\in \\N} \\pi(H_n)$. Then the sequence $\\se{H_n(x)}_{n \\in \\N}$ is decreasing whose elements are nonempty and they are in $\\cH(x)_\\delta$. But since $\\cH(x)_\\delta$ is a compact paving by assumption, $\\bigcap_n H_n(x)$ must be nonempty, implying $x \\in \\pi\\left(\\bigcap_{n \\in \\N} H_n\\right)$. $\\square$\nEnvelopes Definition 5:  Let $(E, \\E)$ be a paved space, and fix a subset $A \\subseteq E$ as well as a decreasing sequence $\\se{A_k} _ {k \\in \\N} \\subseteq \\mathfrak{P}(E)$. We say that $A$ is an $\\E-$envelope of $\\se{A_k} _ {k \\in \\N}$, if there exists a decreasing sequence $\\se{C_k} _ {k \\in \\N} \\subseteq \\E \\cup \\se{E}$ such that $$A_k \\subseteq C_k, \\quad \\forall ; k \\in \\N \\quad \\text{ and } \\quad \\bigcap _ {k \\in \\N} C_k \\subseteq A \\tag{1} \\label{envelope}$$\nExamples   Let $E$ be a separable metric space, and $\\E$ the paving consisting of all closed subsets of $E$. Then a subset $A$ of $E$ is an $\\E-$envelope of a given decreasing sequence $\\se{A_k} _ {k \\in \\N} \\subseteq \\PO(E)$ if, and only if, $A$ contains $\\bigcap _ {k \\in \\N} \\overline{A}_k$, the intersection of the closures of the sets $A_k, k \\in \\N$ in the sequence.\n  An abstract version of the one above: Let $(E, \\E)$ be a paved space; for every subset $A$ of $E$, introduce the collection of sets $\\A := \\set{B \\in \\E \\cup \\se{E}}{A \\subseteq B}$ and assume that the intersection $\\overline{A} := \\bigcap_{B \\in \\A} B$, called the adherent of $A$ in the paving $\\E$, belongs to $\\E_\\delta \\cup \\se{E}$, i.e., $\\overline{A}$ is a countable intersection of sets in $\\E \\cup \\se{E}$. We claim that $A$ is an $\\E-$envelope of a given decreasing sequence $\\se{A_k}_{k \\in \\N} \\subseteq \\PO(E)$ if, and only if, $A$ contains $\\bigcap_{k \\in \\N} \\overline{A}_k$.\n  Lemma 4: In the setting of the last example, a subset $A$ of $E$ is an $\\E-$envelope of a given decreasing sequence $\\se{A_k} _ {k \\in \\N} \\subseteq \\PO(E)$ if, and only if, $A$ contains $\\bigcap_{k \\in \\N} \\overline{A}_k$.\nProof: The necessity is clear: if there exists a decreasing sequence $\\se{C_k}_{k \\in \\N} \\subseteq \\E \\cup \\se{E}$ such that \\eqref{envelope} is satisfied then $\\overline{A}_k \\subseteq C_k$ and therefore $\\bigcap_{k \\in \\N} \\overline{A}_k \\subseteq \\bigcap_{k \\in \\N} C_k \\subseteq A$.\nTo see the sufficiency, for every $k \\in \\N$, let $\\se{B_n^k}_{n \\in \\N} \\subseteq \\E \\cup \\se{E}$ be a decreasing sequence such that $\\overline{A}_k = \\bigcap_{n \\in \\N} B_n^k$ (such a sequence exists because of the assumption in Example 2). Then $$C_k := B_k^1 \\cap B_k^2 \\cap \\cdots \\cap B_k^k, \\quad k \\in \\N$$ defines a decreasing sequence of elements in $\\E \\cup \\se{E}$ with $A_k \\subseteq \\overline{A}_k \\subseteq C_k$ and $\\bigcap_{k \\in \\N} \\overline{A}_k =\\bigcap_{k \\in \\N} C_k$. It follows that the set $A$ envelops the sequence $\\se{A_k}_{k \\in \\N}$, if $A$ contains the countable intersection $\\bigcap_{k \\in \\N} \\overline{A}_k$; for then the decreasing sequence $\\se{C_k}_{k \\in \\N} \\subseteq \\E \\cup \\se{E}$ satisfies the requirements of \\eqref{envelope}. $\\square$\nProperties of envelopes The next lemma lists some properties of envelopes which we will be using frequently.\nLemma 5: (i) If $A$ is an envelope of a given decreasing sequence $\\se{A_n} _ {n \\in \\N} \\subseteq \\PO(E)$, then every subset of $E$ that contains $A$ is also an envelope of $\\se{A_n} _ {n \\in \\N}$.\n(ii) Two decreasing sequences of subsets of $E$ that possess a common subsequence, admit the exact same envelopes.\n(iii) The collection of envelopes of a given decreasing sequence of subsets of $E$, is closed under countable intersections.\nProof: Parts (i) and (ii) are trivial. For part (iii), let $\\se{A^k} _ {k \\in \\N}$ be a sequence of envelopes of a given decreasing sequence $\\se{A_n} _ {n \\in \\N} \\subseteq \\PO(E)$. For each $k \\in \\N$, let $\\se{B_n^k} _ \\nn \\subseteq \\E \\cup \\se{E}$ be a decreasing sequence, such that $A_n \\subseteq B_n^k$ for all $n \\in \\N$ and $\\bigcap_{n \\in \\N} B_n^k \\subseteq A^k$. Then $$C_n := B_n^1 \\cap B_n^2 \\cap \\cdots \\cap B_n^n, \\quad n \\in \\N$$ defines a decreasing sequence of elements in $\\E \\cup \\se{E}$ that satisfies $A_n \\subseteq C_n$ and $\\bigcap_{n \\in \\N} C_n = \\bigcap_{(k,n) \\in \\N^2} B_n^k \\subseteq \\bigcap_{k \\in \\N} A^k$. It follows that $\\bigcap_{k \\in \\N} A^k$ is an $\\E-$envelope of $\\se{A_n}_{n \\in \\N}$. $\\square$\nCapacitance Definition 6:  Let $E$ be a nonempty set. A collection $\\C$ of subsets of $E$ is called a capacitance, if\n(i) whenever $A \\in \\C$ and $A \\subseteq B$, then $B \\in \\C$, and\n(ii) whenever $\\se{A_n} _ {n \\in \\N}$ is an increasing sequence of subsets of $E$ such that $\\bigcup _ {n \\in \\N} A_n \\in \\C$, there is an integer $m$ such that $A_m \\in \\C$.\nIntuitively, a capacitance is a collection of \u0026ldquo;big\u0026rdquo; sets: the power set $\\PO(E)$ is a capacitance, and so are the collections of nonempty and of uncountable subsets of $E$. The notion of pre-capacity, defined next, gives a more useful example.\nDefinition 7:  A function $I \\colon \\PO(E) \\to \\RR$ is called a pre-capacity, if it is\n(i) monotone increasing, i.e., $I(A) \\le I(B)$ holds for every $A \\subseteq B$, and\n(ii) ascending, i.e., for every increasing sequence $\\se{A_n} _ {n \\in \\N}$ we have $$I\\left(\\bigcup_{n \\in \\N} A_n\\right) = \\sup_{n \\in \\N} I(A_n)$$\nIf $I \\colon \\PO(E) \\to \\RR$ is a pre-capacity, then for every given real number $t$ the collection $\\C = \\set{A \\in \\PO(E)}{I(A) \u0026gt; t}$ is a capacitance. Conversely, given a capacitance $\\C$, one can associate to it a pre-capacity by defining $I(A) := 1$ whenever $A \\in \\C$ , and $I(A) := 0$ whenever $A \\notin \\C$; this then leads to the identification $\\C = \\set{A \\in \\PO(E)}{I(A) \u0026gt; 0}$.\nHenceforth assume that there is an underlying paved space $(E, \\E)$ and a capacitance $\\C$ of subsets of $E$.\nScrapers Definition 8: A sequence $\\f = \\se{f_n} _ {n \\in \\N}$ of mappings $f_n \\colon \\left(\\PO(E)\\right)^n \\to \\PO(E)$ is called a $\\C-$scraper if\n(i) $f_n(B_1, B_2, \\ldots, B_n) \\subseteq B_n$ for all $n \\in \\N$ and for all sets $B_1, \\ldots, B_n \\in \\PO(E)$, and\n(ii) whenever $B_n \\in \\C$, then $f_n(B_1, B_2, \\ldots, B_n) \\in \\C$.\nProperty (i) expresses the intuitive notion that $f_n(B_1, B_2, \\ldots, B_n)$ \u0026ldquo;scrapes\u0026rdquo; $B_n$ and property (ii) ensures that \u0026ldquo;the scraping does not remove too big a chunk\u0026rdquo; from $B_n$. In French, scraper is called rabotage which can be translated as planing. A simple example of a scraper is the identity scraper : $f_n(B_1, \\ldots B_n) = B_n$ for all $n \\in \\N$ and for all sets $B_1, \\ldots, B_n \\in \\PO(E)$.\nDefinition 9: Given a $\\C-$scraper $\\f = \\se{f_n} _ {n \\in \\N}$, a (necessarily decreasing) sequence $\\se{B_n} _ {n \\in \\N}$ of subsets of $E$ will be called $\\f-$scraped, if for all $n \\in \\N$ we have $$B_{n+1} \\subseteq f_n(B_1, B_2, \\ldots, B_n) \\quad \\text{and} \\quad B_n \\in \\C$$\nDefinition 10: For any $B \\in \\PO(E)$ and $\\C-$scraper $\\f = \\se{f_n} _ {n \\in \\N}$, the sequence $\\se{P_n} _ {n \\in \\N} \\subseteq \\C$ defined by $P_1 := B$, $P_{n+1} := f_n(P_1, \\ldots, P_n)$ for all $n \\in \\N$ is $\\f-$scraped. It is called the $\\f-$scraped orbit of $B$.\nDefinition 11: A $\\C-$scraper $\\f = \\se{f_n}_{n \\in \\N}$ is called compatible with a given set $A \\in \\PO(E)$, if $A$ envelopes every $\\f-$scraped sequence $\\se{B_n} _ {n \\in \\N}$ with $B_1 \\subseteq A$.\nA set $A \\in \\PO(E)$ is smooth for the capacitance $\\C$, if there exists a $\\C-$scraper compatible with it.\nThe next result is central in this theory.\nTheorem 1 [Sierpi≈Ñski]: Let $(E, \\E)$ be a paved space, and $\\C$ a capacitance. The collection of subsets of $E$ which are smooth for the capacitance, is closed under countable increasing unions and under countable intersections.\nWe will come back to its proof later. Let\u0026rsquo;s prove some its consequences first.\nTheorem 2: Let $(E ,\\E)$ be a paved space, and $\\C$ a capacitance. The elements of the mosaic $\\widehat{\\E}$ generated by $\\E$ are smooth.\nProof: An easy consequence of Theorem 1, Lemma 2 and the fact that every element of $\\E$ is smooth because they are compatible with the identity scraper. $\\square$\nThis theorem is in turn very useful in proving some important results. We will discuss two of them. The first one is the metric space version of Choquet\u0026rsquo;s capacitability theorem. The proof of it will be very similar to the general Choquet\u0026rsquo;s capacitability theorem, but we will need Sion\u0026rsquo;s theorem for the general version, which will be the second result.\nDefinition 12: Let $E$ be a compact metric space, endowed with the paving $\\E = \\K(E)$ of its compact sets. Let $I$ be a metric capacity on $(E, \\E)$, i.e., a pre-capacity that \u0026ldquo;descends on compacts\u0026rdquo; in the sense that for every decreasing sequence $\\se{K_n} _ \\nn \\subseteq \\K(E)$ it satisfies $I\\left(\\bigcap _ {n \\in \\N}K_n\\right) = \\inf _ {n \\in \\N} I(K_n)$.\nTheorem 3 [Metric space version of Choquet\u0026rsquo;s capacitability theorem]: For every Borel subset $B$ of a compact metric space $E$, and any metric capacity $I \\colon \\PO(E) \\to \\RR$, we have $$ I(B) = \\sup_{\\substack{K \\in \\K(E) \\\\ K \\subseteq B}} I(K) $$ Proof: Fix an arbitrary $B \\in \\B(E)$. If $I(B) = -\\infty$ then $I(B) = I(\\emp)$, and we have our desired equality trivially. Otherwise, we need to show that whenever $I(B) \u0026gt; t$ holds for some given real number $t$, there exists a compact set $K \\subseteq B$ such that $I(K) \\ge t$. Recall that $$\\C = \\set{A \\in \\PO(E)}{I(A) \u0026gt; t}$$ is a capacitance. Also recall that a subset $A$ of $E$ is an $\\E-$envelope of a decreasing sequence $\\se{A_n}_{n \\in \\N} \\subseteq \\PO(E)$ if and only if $A$ contains $\\bigcap_{n \\in \\N}\\overline{A}_n$.\nLemma 1 gives that the mosaic $\\widehat{\\E}$ generated by $\\E = \\K(E)$ coincides with the Borel $\\sigma-$algebra $\\B(E)$. Hence by Theorem 2 every Borel set is smooth. Thus, there exists a $\\C-$scraper $\\f = \\se{f_n}_{n \\in \\N}$ compatible with the set $B$.\nConsider the $\\f-$scraped orbit of $B$, $\\se{P_n} _ {n \\in \\N} \\subseteq \\C$. By construction $B$ is an envelope of $\\se{P_n} _ {n \\in \\N}$, and hence it contains $K := \\bigcap _ {n \\in \\N} \\overline{P}_n$. $K$ is closed and hence also compact on account of being a subset of a compact set $E$; similarly for $\\overline{P}_n$ for all $n \\in \\N$. But since $\\se{P_n} _ {n \\in \\N} \\subseteq \\C$ we have $I(\\overline{P}_n) \u0026gt; t$ for all $n \\in \\N$. Now use the descending on compacts property of $I$ to get $I(K) = I\\left(\\bigcap _ {n \\in \\N} \\overline{P}_n\\right) = \\inf _ {n \\in \\N} I(\\overline{P}_n) \\ge t$. $\\square$\nTheorem 4 [Sion\u0026rsquo;s theorem]: Let $(E,\\E)$ be a paved space, and $\\C$ a capacitance. For every element $B$ of $\\C \\cap \\widehat{\\E}$, there exists a decreasing sequence $\\se{K_n} _ {n \\in \\N} \\subseteq \\C \\cap \\E$ such that $\\bigcap _ {n \\in \\N} K_n \\subseteq B$.\nProof: Theorem 2 implies that the set $B$ is smooth, and thus there exists a $\\C-$scraper $\\f = \\se{f_n} _ {n \\in \\N}$ compatible with it. Let $\\se{P_n} _ {n \\in \\N} \\subseteq \\C$ be the $\\f-$scraped orbit of $B$. Then $B$ is an envelope of $\\se{P_n} _ {n \\in \\N}$, so there exists a decreasing sequence $\\se{B_n} _ {n \\in \\N}$ of subsets of $\\E \\cup \\se{E}$ such that $\\bigcap _ \\nn B_n \\subseteq B$ and $P_n \\subseteq B_n$ for all $\\nn$. Notice $B_n \\in \\C$.\nIf the sets $B_n$ belong to the paving $\\E$ from a certain index $m$ onward, we take $K_n := B_{m+n}, n \\in \\N$ as our sequence. Otherwise if $B_n = E$ holds for all integers $n$, the set $B=E$ is the union of an increasing sequence of sets in $\\E$ because $B \\in \\widehat{\\E}$ and Lemma 2. Therefore, the fact that $B \\in \\C$ implies $B$ contains a set $K \\in \\C \\cap \\E$; it suffices then to take $K_n = K$ for all integers $n$. $\\square$\nWe now come back to the proof of Theorem 1. But first we will need the following clever operation on scrapers, and a couple of results.\nMixing of Scrapers Consider a sequence $\\se{\\f^k, k \\in \\N} = \\se{\\se{f^k_n} _ \\nn, k \\in \\N}$ of scrapers, and a bijection $\\N^2 \\ni (p,q) \\mapsto \\beta(p,q) = p \\star q \\in \\N$ which is strictly increasing in each of its arguments. For every integer $\\nn$ and sets $P_1, P_2, \\ldots, P_n$, if $n = p \\star q$, let $$ f_n(P_1, P_2, \\ldots, P_n) := f^p_q(P_{p \\star 1}, P_{p \\star 2}, \\ldots, P_{p \\star q}) $$ It is easy to see that this defines a new scraper $\\f = \\se{f_n}_\\nn$, called the mixing of the scrapers $\\se{\\f^k, k \\in \\N}$ via the bijection $\\beta$.\nTheorem 5 : Let $\\se{\\f^k, k \\in \\N}$ be a sequence of scrapers, and denote by $\\f$ its mixing by a bijection $\\beta$. In order for a subset $A$ of $E$ to be compatible with $\\f$, it suffices that it be compatible with one of the scrapers $\\f^k, k \\in \\N$.\nProof: Let $A \\in \\PO(E)$ be compatible with $\\f^k$ for some arbitrary but fixed $k$. Consider also a sequence of sets $\\se{P_n} _ \\nn$, which is $\\f-$scraped and whose first term $P_1$ is contained in $A$. We need to show that the set $A$ envelops $\\se{P_n} _ \\nn$.\nTo do this, we exploit Lemma 5 (ii) and construct a decreasing sequence $\\se{Q_n} _ \\nn \\subseteq \\PO(E)$ which is subsequence of $\\se{P_n} _ \\nn$ and show that $A$ envelops $\\se{Q_n} _ \\nn$. This will then imply $A$ envelops $\\se{P_n} _ \\nn$. To this end, let $$Q_n := P_{k \\star n} \\quad \\forall \\; \\nn$$ Because $Q_1 = P_{k \\star 1} \\subseteq P_{1 \\star 1} = P_1 \\subseteq A$ and $A$ is compatible with $\\f^k$, to show that $A$ envelops $\\se{Q_n}_\\nn$ it suffices to show $\\se{Q_n} _ \\nn$ is $\\f^k-$scraped.\nNow, $Q_n \\in \\C$ for all $\\nn$, so all that remains to be shown is that $Q_{n+1} \\subseteq f^k_n(Q_1, Q_2, \\ldots, Q_n)$ holds for all $\\nn$. Because $\\se{P_n} _ \\nn$ is $\\f-$scraped we have $$Q_{n+1} = P_{k \\star (n+1)} \\subseteq P_{1 + k\\star n} \\subseteq f_{k \\star n}(P_1, P_2, \\ldots, P_{k \\star n}) = f^k_n(Q_1, Q_2, \\ldots, Q_n)$$ giving the desired result. $\\square$\nAn immediate corollary of this theorem is : If $\\se{A_n}_\\nn$ is a sequence of smooth subsets of $E$, there exists a scraper $\\f$ which is compatible with all the sets $A_n, \\nn$.\nProof of Theorem 1 Closure under countable intersections:\nSuppose $\\se{A^k} _ {k \\in \\N}$ is a sequence of smooth sets, $A = \\bigcap _ {k \\in \\N} A^k$, and $\\f$ is a $\\C-$scraper compatible with all of the sets $A^k, k \\in \\N$. If $\\se{P_n} _ \\nn$ is an $\\f-$scraped sequence of sets such that $P_1 \\subseteq A$, then $P_1 \\subseteq A^k$ for all $k \\in \\N$. Our construction then implies $A^k$ is an $\\E-$envelope of $\\se{P_n} _ \\nn$ for all $k \\in \\N$. Lemma 5 (iii) now implies $A$ is also an $\\E-$envelope $\\se{P_n} _ \\nn$, showing that $A$ is compatible with $\\f$, and hence smooth.\nClosure under countable increasing unions:\nSuppose $\\se{A^k} _ {k \\in \\N}$ is an increasing sequence of smooth sets, $A = \\bigcup _ {k \\in \\N} A^k$, and $\\f$ is a $\\C-$scraper compatible with all of the sets $A^k, k \\in \\N$. The scraper $\\f$ doesn\u0026rsquo;t work for this case and so we create a new one. For any $n \\in \\N$ and sets $P_1, P_2, \\ldots, P_n$ we define $$ \\varphi_n(P_1, P_2, \\ldots, P_n) = \\begin{cases} P_n \u0026amp;\\text{if } A \\cap P_1 \\notin \\C \\\\ f_n(A^p \\cap P_1, P_2, \\ldots, P_n) \u0026amp;\\text{if } A \\cap P_1 \\in \\C \\end{cases} $$ where $p$ is the smallest integer such that $A^p \\cap P_1 \\in \\C$. Such an integer does exist from part (ii) of the definition of capacitance. It is easy to see that $\\Phi = \\se{\\varphi_n} _ \\nn$ is a $\\C-$scraper. It is sufficient to show that $\\Phi$ is compatible with $A$ to finish our proof.\nLet $\\se{P_n} _ \\nn$ be a $\\Phi-$scraped sequence of sets such that $P_1 \\subseteq A$. By definition $P_1 \\in \\C$ and $A \\cap P_1 = P_1$, and so from our construction $\\varphi_n(P_1, P_2, \\ldots, P_n) = f_n(A^p \\cap P_1, P_2, \\ldots, P_n)$. All elements of the sequence $A^p \\cap P_1, P_2, \\ldots, P_n, \\ldots$ are in $\\C$ and for all $n \\in \\N$ $$ P_{n+1} \\subseteq \\varphi_n(P_1, P_2, \\ldots, P_n) = f_n(A^p \\cap P_1, P_2, \\ldots, P_n) $$ and thus it follows that this sequence is $\\f-$scraped. Now since $A^p$ is compatible with $\\f$, $A^p$ is an envelope of this sequence, and also of $\\se{P_n} _ \\nn$ by Lemma 5 (ii). It follows that $A$ is an envelope of $\\se{P_n} _ \\nn$ by Lemma 5 (i) because $A^p \\subseteq A$. $\\square$\nChoquet Capacities Definition 13: A mapping $I \\colon \\PO(E) \\to \\RR$ is called a Choquet $\\E-$capacity if it is\n(i) monotone increasing, i.e., $I(A) \\le I(B)$ holds for every $A \\subseteq B$,\n(ii) ascending, i.e., for every increasing sequence $\\se{A_n} _ {n \\in \\N} \\subseteq \\PO(E)$ we have $$I\\left(\\bigcup _ {n \\in \\N} A_n\\right) = \\sup _ {n \\in \\N} I(A_n)$$ (iii) descending on pavings, i.e., for every decreasing sequence $\\se{E_n} _ {n \\in \\N} \\subseteq \\E$ we have $$I\\left(\\bigcap _ {n \\in \\N} E_n\\right) = \\inf _ {n \\in \\N} I(E_n)$$\nDefinition 14: A set $A \\in \\PO(E)$ is called $I-$capacitable if $$I(A) = \\sup_{\\substack{K \\in \\E_\\delta \\\\ K \\subseteq A}} I(K)$$\nExamples   Consider a paved space $(E, \\E)$ with compact paving $\\E$, and define $I(A) = 0$ if $A = \\emp$, $I(A) = 1$ if $A \\neq \\emp$. Then $I$ is a Choquet $\\E-$capacity. The property (iii) in the definition of capacity reflects now the assumption that the paving $\\E$ is compact.\n  Consider a probability space $\\probsp$, then the outer measure $$\\PP^*(A) := \\inf_{\\substack{B \\in \\F \\ A \\subseteq B}} \\PP(B)$$ is a Choquet $\\F-$capacity. Proof of this is a standard exercise in measure theory, albeit with a different terminology of continuity from below.\n  Consider a locally compact, separable metric space $K$, and the paving $\\K$ of its compact subsets. If $\\pi$ denotes the projection of $K \\times \\Omega$ onto $\\Omega$ and $$I(A) := \\PP^*(\\pi(A)) \\text{ for all } A \\in \\PO(K \\times \\Omega),$$ then $I$ is a Choquet $(\\K \\otimes_p \\F)-$capacity. Proof of this follows from $\\pi\\left( \\bigcup_n A_n\\right) = \\bigcup_n \\pi(A_n)$ and Lemma 3 combined with the properties of outer measure.\n  Theorem 6 [Choquet\u0026rsquo;s capacitability theorem]: Consider a paved space $(E, \\E)$, and let $I \\colon \\PO(E) \\to \\RR$ be a Choquet $\\E-$capacity. Then every set $A \\in \\widehat{\\E}$ is $I-$capacitable.\nProof: Fix an arbitrary set $A \\in \\widehat{\\E}$. If $I(A) = -\\infty$ then $I(A) = I(\\emp)$, and we have our desired equality trivially. Otherwise, we need to show that whenever $I(A) \u0026gt; t$ holds for some given real number $t$, there exists a set $K \\in \\E_\\delta$ with $K \\subseteq A$ and $I(K) \\ge t$.\nRecall that $$\\C = \\set{B \\in \\PO(E)}{I(B) \u0026gt; t}$$ is a capacitance. Then $A \\in \\C$, and from Sion\u0026rsquo;s Theorem (Theorem 4) there exists a decreasing sequence $\\se{K_n} _ \\nn$ of elements in $\\E \\cap \\C$ such that $\\bigcap _ \\nn K_n \\subseteq A$. But then $I\\left(\\bigcap _ \\nn K_n\\right) = \\inf _ \\nn I(K_n) \\ge t$, and thus we can take $K = \\bigcap_\\nn K_n$. $\\square$\nWe are now ready to prove some major results in measure theory.\nMeasurable Projection Theorem 7 [Measurable Projection]: Let $\\probsp$ be a complete probability space, let $(K, \\B(K))$ be a locally compact separable metric space endowed with the collection of its Borel sets, and denote by $\\pi$ the projection of $K \\times \\Omega$ onto $\\Omega$. Then, for every $B \\in \\B(K) \\otimes \\F$, the projection $\\pi(B) \\in \\F$.\nProof: We start by noticing $\\B(K) \\PM \\F = \\B(K) \\otimes \\F$. This follows from the fact that if $A \\in \\B(K) \\otimes_p \\F$, then $A = \\bigcup _ {i=1}^n U_i \\times V_i$ for some $U_i \\in \\B(K)$ and $V_i \\in \\F$, and thus it can be shown $A^\\comp \\in \\B(K) \\PM \\F$, and thus Lemma 1 gives $\\B(K) \\PM \\F = \\sigma(\\B(K) \\otimes_p \\F) = \\B(K) \\otimes \\F$.\nConsider the paving $\\K$ on $K$ consisting of all compact subsets of $K$, and introduce the $(\\K \\otimes_p \\F)-$capacity $I(A) = \\PP^*(\\pi(A))$, $A \\in \\B(K \\times \\Omega)$. Recall that $\\B(K) = \\widehat{\\K}$, and so $\\B(K) \\PM \\F = \\K \\PM \\F$, the mosaic generated by the paving $\\K \\otimes_p \\F$ ($\\supseteq$ is trivial. For $\\subseteq$ let $A \\in \\B(K) \\otimes_p \\F$ and show $A \\in \\K \\PM \\F$).\nChoquet\u0026rsquo;s capacitability theorem (Theorem 6) thus guarantees that every set in $\\B(K) \\otimes \\F$ is $I-$capacitable. In particular, for every integer $n \\in \\N$, there exists a set $C_n \\in (\\K \\otimes_p \\F)_\\delta$ contained in $B$ and such that\n$$I(C_n) \\le I(B) \\le I(C_n) + (1/n) \\tag{2} \\label{ms}$$\nBecause $C_n \\in (\\K \\otimes_p \\F)_\\delta$, $C_n$ is a countable intersection $C_n = \\bigcap _ {m \\in \\N} G_n^m$, where each $G_n^m$ is a finite union of sets of the form $U \\times V (U \\in \\K, V \\in \\F)$. Letting $H_n^m = \\bigcap _ {i=1}^m G_n^i$, we see that $H_n^1 \\supseteq H_n^2 \\supseteq \\cdots$ and $C_n = \\bigcap _ {m \\in \\N} H_n^m$, where now $H_n^m$ is also a finite union of sets of the form $U \\times V (U \\in \\K, V \\in \\F)$.\nThe form of $H_n^m$ immediately implies $\\pi(H_n^m) \\in \\F$ for all $(m,n) \\in \\N^2$. Lemma 3 implies $$\\pi(C_n) = \\pi\\left(\\bigcap _ {m \\in \\N} H_n^m\\right) = \\bigcap _ {m \\in \\N} \\pi(H_n^m) \\in \\F, \\quad \\forall \\; n \\in \\N$$ which further implies $\\pi\\left(\\bigcup _ {n \\in \\N} C_n\\right) = \\bigcup _ {n \\in \\N} \\pi(C_n) \\in \\F$.\nOn the other hand, $C_n \\subseteq B$ for all $\\nn$ and thus $\\pi\\left(\\bigcup _ {n \\in \\N} C_n\\right) \\subseteq B$. But \\eqref{ms} implies that the difference of these two sets is a $\\PP-$null set, and the completeness of the probability space gives the desired conclusion $\\pi(B) \\in \\F$. $\\square$\nMeasurable Graph Definition 15: We call a set $G \\in \\B(K) \\otimes \\F$ measurable graph, if for every $\\omega \\in \\Omega$ the section $G(\\omega) = \\set{y \\in K}{(y,\\omega) \\in G}$ contains at most one point.\nTheorem 8 [Measurable Graph]: A subset $G$ of $K \\times \\Omega$ is a measurable graph, if and only if, there exists a set $\\Xi \\in \\F$ and a measurable mapping $g \\colon \\Xi \\to K$, such that $G = \\set{(y,\\omega) \\in K \\times \\Xi}{y = g(\\omega)}$.\nProof: Sufficiency. If $\\Xi$ and $g$ are as stated, the set $G = \\set{(y,\\omega) \\in K \\times \\Xi}{y = g(\\omega)}$ equals the pre-image $\\varphi^{-1}(\\Delta)$ of the diagonal $\\Delta = \\set{(y,y) \\in K \\times K}{y \\in K}$ under the mapping $$K \\times \\Xi \\ni (y,\\omega) \\mapsto \\varphi(y,\\omega) := (y, g(\\omega)) \\in K \\times K$$ This mapping $\\varphi$ is $(\\B(K) \\otimes \\F)-$measurable because of the fact that $\\B(K \\times K) = \\B(K) \\otimes \\B(K)$ and that $g$ is measurable. Since $\\Delta$ is a closed set, $\\Delta \\in \\B(K \\times K)$ and thus $G$ is a measurable graph.\nNecessity. Suppose that $G$ is a measurable graph, and let $\\Xi := \\pi(G)$. Then $\\Xi \\in \\F$ by Measurable Projection theorem. For every $\\omega \\in \\Xi$, define $g(\\omega)$ to be the unique element of the set $G(\\omega)$. We want to show $g \\colon \\Xi \\to K$ is measurable. Indeed for any $H \\in \\B(K)$, it is easy to see that $g^{-1}(H) = \\pi\\left(G \\cap (H \\times \\Omega)\\right) \\in \\F$ where the inclusion follows from Measurable Projection theorem. $\\square$\nDebut Now let $K = [0, \\infty)$, the case important in stochastic processes.\nDefinition 16: Let $A \\subseteq [0, \\infty) \\times \\Omega$. The debut of $A$ is the nonnegative function $D_A \\colon \\Omega \\to \\RR$ defined as $$D_A(\\omega) = \\inf\\set{t \\in [0, \\infty)}{(t, \\omega) \\in A}$$\nTheorem 9 [Measurable Debut]: Let $\\probsp$ be a complete probability space, and consider a measurable set $A \\in \\B([0, \\infty)) \\otimes \\F.$ Then the debut $D_A$ of this set is a random variable.\nProof: For any given real number $t \u0026gt; 0$, the set $D_A^{-1}([0,t)) = \\set{\\omega \\in \\Omega}{D_A(\\omega) \u0026lt; t}$ is the projection onto $\\Omega$ of the measurable subset $A \\cap ([0, t) \\times \\Omega) \\in \\B([0, \\infty)) \\otimes \\F$. To see this, note $\\omega \\in D_A^{-1}([0,t))$ $\\iff$ $0 \\le D_A(\\omega) \u0026lt; t$ $\\iff$ $\\exists , s \\in [0,t)$ such that $(s, \\omega) \\in A$ $\\iff$ $\\omega \\in \\pi(A \\cap ([0, t) \\times \\Omega))$. Measurable Projection theorem shows that this set is in $\\F$. $\\square$\nMeasurable Section Let $\\probsp$ be a complete probability space, and consider a set $A \\subseteq \\oi \\times \\Omega$. Then for every $\\omega \\in \\pi(A)$, there exists a $t \\in \\oi$ such that $(t, \\omega) \\in A$. In other words, we can define a mapping $Z \\colon \\pi(A) \\to \\oi$. It is convenient to extend $Z$ to the whole of $\\Omega$ by setting $Z = \\infty$ on $\\Omega \\setminus \\pi(A)$. When is $Z$ measurable? The measurable section theorem (also known as measurable selection theorem) says that it is possible to define $Z$ to be measurable if $A \\in \\B(\\oi) \\otimes \\F$.\nDefinition 17: For a mapping $Z \\colon \\Omega \\to [0, \\infty]$ we shall define its graph as the product set $$\\gr{Z} := \\set{(t, \\omega) \\in [0, \\infty) \\times \\Omega}{Z(\\omega) = t}$$\nThe condition $(Z(\\omega), \\omega) \\in A$ whenever $Z \u0026lt; \\infty$ can then be expressed by saying $\\gr{Z} \\subseteq A$.\nTheorem 9 [Measurable Section]: Let $\\probsp$ be a complete probability space, and consider a measurable set $A \\in \\B([0, \\infty)) \\otimes \\F$. Then there exists a random variable $Z \\colon \\Omega \\to [0, \\infty]$ with $\\gr{Z} \\subseteq A$ and $\\se{Z \u0026lt; \\infty} = \\pi(A)$.\nProof: We shall show first, that for every $\\eps \u0026gt; 0$ there exists a random variable $Z_\\eps \\colon \\Omega \\to [0, \\infty]$ with $\\gr{Z_\\eps} \\subseteq A$ and $\\Prob{\\pi(A)} \\le \\Prob{Z_\\eps \u0026lt; \\infty} + \\eps$.\nTo this end, recall that if $K = [0, \\infty)$ and $\\K$ is the paving of all compact subsets of $K$, then we have the $\\K \\otimes_p \\F-$capacity $I(A) = \\PP^*(\\pi(A)), A \\in \\PO(K \\times \\Omega)$. Therefore, every $A \\in \\B(K) \\otimes \\F$ is $I-$capacitable, and fixing $A$, there exists for every $\\eps \u0026gt; 0$ a set $C_\\eps \\in (\\K \\otimes_p \\F)_\\delta$ such that $$C_\\eps \\subseteq A, \\quad I(C_\\eps) \\le I(A) \\le I(C_\\eps) + \\eps \\tag{3} \\label{eq_sec}$$ Let $Z_\\eps := D_{C_\\eps}$ be the debut of this $C_\\eps$, then Measurable Debut theorem (Theorem 9) implies $Z_\\eps$ is a random variable. For every $\\omega \\in \\Omega$, the section $C_\\eps(\\omega)$ is a compact subset of $K$ (use the facts that compact$\\iff$closed and bounded here, and $C_\\eps \\in (\\K \\otimes_p \\F)_\\delta$). Notice that if $(t, \\omega) \\in \\gr{D_{C_\\eps}}$ then $D_{C_\\eps}(\\omega) = t$ which is same as saying $t = \\inf \\set{s \\in [0, \\infty)}{(s,\\omega) \\in C_\\eps}$, but since $C_\\eps(\\omega)$ is closed this implies $(t, \\omega) \\in C_\\eps$. Therefore, $\\gr{Z_\\eps} \\subseteq C_\\eps$, showing the first requirement.\nThe second requirement $\\Prob{\\pi(A)} \\le \\Prob{Z_\\eps \u0026lt; \\infty} + \\eps$ is true because $\\pi(A) \\in \\F$ by Measurable Projection theorem (Theorem 7) and $\\se{Z_\\eps \u0026lt; \\infty} = \\pi(C_\\eps) \\in \\F$, and now use \\eqref{eq_sec}.\nLet us now construct a random variable $Z$ to satisfy the properties claimed in the theorem. Set $A_1 = A$; from above there exists a random variable $Z_1 \\colon \\Omega \\to [0, \\infty]$ with $\\gr{Z_1} \\subseteq A_1$ and $\\Prob{\\pi(A_1)} \\le 2 \\Prob{Z_1 \u0026lt; \\infty}$, by taking $\\eps = \\Prob{Z_1 \u0026lt; \\infty} \u0026gt; 0$; if it happens that $\\Prob{Z_1 \u0026lt; \\infty} = 0$, then $\\Prob{\\pi(A_1)} = 0$ and the inequality is still true.\nSet $A_2 = A_1 \\setminus ([0, \\infty) \\times \\se{Z_1 \u0026lt; \\infty})$ and, reasoning as before, construct $Z_2 \\colon \\Omega \\to [0, \\infty]$ with $\\gr{Z_2} \\subseteq A_2$ and $\\Prob{\\pi(A_2)} \\le 2 \\Prob{Z_2 \u0026lt; \\infty}$.\nContinuing this way, we obtain a sequence $\\se{Z_n} _ {\\nn}$ such that $\\gr{Z_n} \\subseteq A$, the projections $\\pi(\\gr{Z_n})$ are disjoint, and we have $$\\sum_{k=1}^n \\Prob{Z_k \u0026lt; \\infty} \\ge (1 - 2^{-n}) \\Prob{\\pi(A)}, \\quad \\forall \\; \\nn \\tag{4} \\label{eq_notsure}$$ The random variable $Z$ defined as $Z := Z_k$ on $\\se{Z_k \u0026lt; \\infty}$ for each $k \\in \\N$, and $Z := \\infty$ otherwise, satisfies therefore $\\gr{Z} \\subseteq A$, thus also $\\se{Z \u0026lt; \\infty} \\subseteq \\pi(A).$\nOn the other hand, letting $n \\to \\infty$ in \\eqref{eq_notsure}, we see that $\\se{Z \u0026lt; \\infty}$ and $\\pi(A)$ have the same probability. Therefore, the completeness of the probability space implies these two sets can be made equal. $\\square$\nEpilogue With this we are done laying the foundations. In the next blog post, we will discuss applications of these results in the general theory of processes.\nReferences  Paul-Andr√© Meyer. Stochastic Processes from 1950 to the Present (Translated from the French by Jeanine Sedjro), 2009.  Almost Sure blog by George Lowther. Nicole El Karoui and Xiaolu Tan. Capacities, Measurable Selection and Dynamic Programming Part I: Abstract Framework, 2013. Claude Dellacherie. Capacities and analytic sets, Part of the Lecture Notes in Mathematics book series (LNM, volume 839), Springer-Verlag, 1981. Claude Dellacherie. Capacit√©s et processus stochastiques, Springer-Verlag, 1972. Claude Dellacherie and Paul-Andr√© Meyer. Probabilities and Potential, North-Holland Publishing Company, 1978.   *: If you steal from one author, it‚Äôs plagiarism; if you steal from many, it‚Äôs research.\n","date":1618288317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618288317,"objectID":"66f2a07c0a8f6c7f401d967721783009","permalink":"https://makkar.github.io/post/grlthprcs/","publishdate":"2021-04-13T00:31:57-04:00","relpermalink":"/post/grlthprcs/","section":"post","summary":"Choquet's theory of capacities and its applications in measure theory","tags":[],"title":"General Theory of Processes - Part 1","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\ZZ}{\\Z_{\\geq 0}^N} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\NN}{\\mathcal{N}} \\newcommand{\\LL}{\\mathcal{L}} \\newcommand{\\PP}{\\mathbb{P}} \\newcommand{\\OO}{\\mathcal{O}} \\newcommand{\\I}{\\mathcal{I}} \\newcommand{\\Prob}[1]{\\PP \\left( #1 \\right)} \\newcommand{\\eq}[1]{\\begin{align*}#1\\end{align*}} \\newcommand{\\eql}[1]{\\begin{align}#1\\end{align}} \\newcommand{\\ind}[1]{\\mathbf{1}_{#1}} \\newcommand{\\indo}[1]{\\mathbf{1}_{#1}(\\omega)} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\G}{\\mathcal{G}} \\newcommand{\\cH}{\\mathcal{H}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\D}{\\mathcal{D}} \\newcommand{\\EE}{\\mathcal{E}} \\newcommand{\\probsp}{(\\Omega, \\F, \\PP)} \\newcommand{\\integ}[1]{\\int_{\\Omega} #1 \\dmu} \\newcommand{\\B}{\\mathcal{B}} \\newcommand{\\Bo}{\\B(\\R)} \\newcommand{\\Bon}[1]{\\B(\\R^{#1})} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sumn}{\\sum_{n=1}^{\\infty}} \\newcommand{\\trans}{\\intercal} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\ex}[1]{\\exp\\left{#1\\right}} \\newcommand{\\comp}{\\mathsf{c}} \\newcommand{\\emp}{\\varnothing} \\newcommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor} \\newcommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil} \\newcommand{\\se}[1]{\\left\\{ #1 \\right\\}} \\newcommand{\\set}[2]{\\left\\{ #1 \\; : \\; #2 \\right\\}} \\newcommand{\\sett}[2]{\\left\\{ #1 ; | ; #2 \\right\\}} \\newcommand{\\Ex}[1]{\\E\\left(#1\\right)} \\newcommand{\\Exc}[2]{\\E\\left(#1 \\mid #2\\right)} \\newcommand{\\Pc}[2]{\\PP\\left( \\left. #1 \\, \\right\\vert \\, #2\\right)} \\newcommand{\\pard}[2]{\\frac{\\partial #1}{\\partial #2}} \\newcommand{\\dd}{\\mathrm{d}} \\newcommand{\\ph}[1]{\\varphi^{-1}\\left(#1\\right)} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dy}{dy} \\DeclareMathOperator{\\du}{du} \\DeclareMathOperator{\\dz}{d\\matr{z}} \\DeclareMathOperator{\\dt}{dt} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator{\\dP}{d\\PP} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\Ord}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} \\DeclareMathOperator{\\V}{\\mathrm{Var}} \\DeclareMathOperator{\\Log}{\\mathrm{Log}} \\DeclareMathOperator{\\O}{\\mathcal{O}} $$\nIntroduction The concept of conditional probability is central to probability theory and excellent treatment of it can be found in many books. My aim with this blog post is to consolidate in one place some ideas around it which helped me form a better intuition. These ideas will be useful if you have already been exposed to this concept from a textbook and just want one more person\u0026rsquo;s ramblings about it.\nI will start by defining conditional expectation and stating some of its properties. It will be a grave injustice to claim my discussion of it is complete since I don\u0026rsquo;t even prove its existence; this section exists solely for establishing notation. I will then spend some time discussing conditional probability, relating it to the traditional notion of\n$$ \\PP(A \\mid B) = \\frac{\\PP(A \\cap B)}{\\PP(B)} $$\nThese discussions will naturally lead to the notions of regular conditional probability and regular conditional distribution which I discuss next.\nConditional expectation Recall the concept of conditional expectation.\nTheorem 1:  Let $\\probsp$ be a probability space, and $X$ a random variable with $\\Ex{|X|} \u0026lt; \\infty$. Let $\\G$ be a sub-$\\sigma$-algebra of $\\F$. Then there exists a random variable $Y$ such that\n $Y$ is $\\G$ measurable, $\\Ex{|Y|} \u0026lt; \\infty$, $\\int_G Y \\dP = \\int_G X \\dP$ for every $G \\in \\G$.  Remarks:  (1.) It is easy to see from the $\\pi-\\lambda$ theorem that the last condition can be relaxed such that $\\int_G Y \\dP = \\int_G X \\dP$ for every $G$ in some $\\pi$-system which contains $\\Omega$ and generates $\\G$. (2.) If $Y'$ is another random variable with the three properties above then $Y\u0026rsquo; = Y$ a.s.. Therefore, $Y$ in the theorem above is called a version of the conditional expectation. The notation $\\Exc{X}{\\G}$ is used to denote this unique (up to a.e. equivalence) random variable.\nThe proof of this standard theorem can be found in any probability text. See [1,2] for example.\nDefinition 1:  In the setting of Theorem 1, if $Z$ is a random variable, we write $\\Exc{X}{Z}$ for $\\Exc{X}{\\sigma(Z)}$.\nThe fact that conditional expectation is defined as a random variable might come as surprising, but the correspondence with the traditional usage of conditional expectation as a number becomes clear once you realize that here we are conditioning on a $\\sigma$-algebra instead of a single event. For example, consider the life expectancy of a new born baby conditioned on sex. This is a random variable that takes one value for males and another value for females.\nProperties of conditional expectation For completeness I state some useful properties of conditional expectation. You can find the proofs in [1,2] for example. Most of them are parallels to well-known properties of (unconditional) expectation. Assume that all the $X$'s satisfy $\\Ex{|X|} \u0026lt; \\infty$ and let $\\G, \\cH$ be sub-$\\sigma$-algebras of $\\F$.\n [Linearity] $\\Exc{a_1 X_1 + a_2 X_2}{\\G} = a_1 \\Exc{X_1}{\\G} + a_2 \\Exc{X_2}{\\G}$ a.s. [Positivity] If $X \\ge 0$, then $\\Exc{X}{\\G} \\ge 0$ a.s. [Monotone convergence theorem for conditional expectation] If $\\Ex{|Y|} \u0026lt; \\infty$ and $Y \\le X_n \\uparrow X$ a.s., then $\\Exc{X_n}{\\G} \\uparrow \\Exc{X}{\\G}$ a.s. [Fatou\u0026rsquo;s lemma for conditional expectation] If $\\Ex{|Y|} \u0026lt; \\infty$ and $Y \\le X_n$ for all $n \\ge 1$ a.s., then $\\Exc{\\limi X_n}{\\G} \\le \\limi \\Exc{X_n}{\\G}$ a.s. [Dominated convergence theorem for conditional expectation] If $|X_n| \\le |Y|$ for all $n \\ge 1$, $\\Ex{Y} \u0026lt; \\infty$, and $X_n \\to X$ a.s., then $\\Exc{X_n}{\\G} \\to \\Exc{X}{\\G}$ a.s. [Tower property] If $\\cH \\subseteq \\G$, then $\\Exc{\\Exc{X}{\\G}}{\\cH} = \\Exc{\\Exc{X}{\\cH}}{\\G} = \\Exc{X}{\\cH}$ a.s. [Taking out what\u0026rsquo;s known] If $Y$ is $\\G$-measurable and bounded, then $$\\Exc{Y X}{\\G} = Y \\Exc{X}{\\G} \\quad \\text{a.s.}\\tag{1} \\label{eq1}$$ If $p \u0026gt; 1$, $1/p + 1/q = 1$, $X \\in L^p(\\Omega, \\F, \\PP)$ and $Y \\in L^q(\\Omega, \\G, \\PP)$, then \\eqref{eq1} again holds. If $X$ is a nonnegative $\\F$-measurable random variable, $Y$ is a nonnegative $\\G$-measurable random variable, $\\Ex{X} \u0026lt; \\infty$ and $\\Ex{XY} \u0026lt; \\infty$, then \\eqref{eq1} again holds. [Role of independence] If $\\cH$ is independent of $\\sigma(\\sigma(X), \\G)$, then $\\Exc{X}{\\sigma(\\G, \\cH)} = \\Exc{X}{\\G}$ a.s.  Conditional probability Definition 2:  In the setting of Theorem 1, if $A \\in \\F$, we let $\\Pc{A}{\\G}$ to mean $\\Exc{\\ind{A}}{\\G}$ and call it the conditional probability of $A$ given $\\G.$ Here $\\ind{A}$ is the indicator random variable. If $B \\in \\F$, we let $\\Pc{A}{B}$ to mean $\\Exc{\\ind{A}}{\\ind{B}}$.\nJust like conditional expectation, conditional probability, as defined above, is a random variable! Unlike conditional expectation this isn\u0026rsquo;t very palpable and deserves more rumination [3]. We have our probability space $\\probsp$ and let $A,B \\in \\F$ be such that $\\PP(B) \\neq 0$ and $\\PP(B^\\comp) \\neq 0$. Then our traditional notion of conditional probability tells us that the conditional probability of $A$ given $B$ is defined by\n$$ \\PP_B(A) = \\frac{\\PP(A \\cap B)}{\\PP(B)} $$\nLet us investigate how $\\PP_B(A)$ depends on $B$. To this end, introduce the discrete measurable space $(\\Lambda, 2^\\Lambda)$ with $\\Lambda = \\se{\\lambda_1, \\lambda_2}$, and a measurable mapping $T \\colon \\Omega \\to \\Lambda$ such that\n$$ T(\\omega) = \\begin{cases} \\lambda_1 \u0026amp; \\text{ if } \\omega \\in B \\\\\n\\lambda_2 \u0026amp; \\text{ if } \\omega \\in B^\\comp \\end{cases} $$\nDefine the two measures $\\nu_A$ and $\\nu$ on $(\\Lambda, 2^\\Lambda)$ as follows for any $E \\subseteq \\Lambda$\n$$ \\nu_A(E) = \\Prob{A \\cap T^{-1}(E)} $$\n$$ \\nu(E) = \\Prob{T^{-1}(E)} $$\nThen it is easy to see that\n$$ \\PP_B(A) = \\frac{\\nu_A(\\se{\\lambda_1})}{\\nu(\\se{\\lambda_1})} $$\n$$ \\PP_{B^\\comp}(A) = \\frac{\\nu_A(\\se{\\lambda_2})}{\\nu(\\se{\\lambda_2})} $$\nIn other words conditional probability may be viewed as a measurable function on $\\Lambda$. This can easily be generalized to any finite setting as follows. Let $\\se{A_1, \\ldots, A_n} \\subseteq \\F$ be a partition of $\\Omega$, i.e., $A_i \\cap A_j = \\emp$ for $i \\neq j$ and $\\bigcup_i A_i = \\Omega$. Introduce the discrete measurable space $(\\Lambda, 2^\\Lambda)$ with $\\Lambda = \\se{\\lambda_1, \\ldots, \\lambda_n}$. Define a measurable mapping $T \\colon \\Omega \\to \\Lambda$ such that $T(\\omega) = \\lambda_i$ whenever $\\omega \\in A_i$. Define the measures $\\nu_{A_1}, \\ldots, \\nu_{A_n}, \\nu$ on $(\\Lambda, 2^\\Lambda)$ as follows for any $E \\subseteq \\Lambda$\n$$ \\nu_{A_i}(E) = \\Prob{A_i \\cap T^{-1}(E)} \\quad \\text{for all } i = 1, \\ldots, n $$\n$$ \\nu(E) = \\Prob{T^{-1}(E)} $$\nThen once again we have for any $A \\in \\F$\n$$ \\PP_{A_i}(A) = \\frac{\\PP(A \\cap A_i)}{\\PP(A_i)} = \\frac{\\nu_{A_i}(\\se{\\lambda_i})}{\\nu(\\se{\\lambda_i})} \\quad \\text{for all } i = 1, \\ldots, n $$\nThese considerations are what motivated the definition of conditional probability in general cases as you see in Definition 2. If $T$ is any measurable mapping from $\\probsp$ into a measurable space $(\\Lambda, \\LL)$, and if we write $\\nu_A(E) = \\Prob{A \\cap T^{-1}(E)}$ where $A \\in \\F$ and $E \\in \\LL$, then it is clear that $\\nu_E$ and $\\PP \\circ T^{-1}$ are measures on $\\LL$ such that $\\nu_A \\ll \\PP \\circ T^{-1}$. Radon-Nikodym theorem now implies that there exists an $\\PP \\circ T^{-1}$-integrable function $p_A$, unique upto $\\PP \\circ T^{-1}$-a.e., such that\n$$ \\Prob{A \\cap T^{-1}(E)} = \\int_E p_A(\\lambda) \\; \\PP \\circ T^{-1}(\\dd \\lambda) \\quad \\text{for all } E \\in \\LL $$\nWe anoint $p_A(\\lambda)$ as the conditional probability of $A$ given $\\lambda$ or the conditional probability of $A$ given that $T(\\omega) = \\lambda$. Note that here we are conditioning on a measurable mapping $T$ instead of a sub-$\\sigma$-algebra, but this notion is related to conditioning on $\\sigma(T)$ as will become clear ahead. Keep this \u0026ldquo;rumination\u0026rdquo; in mind when we discuss regular conditional distribution later.\nLet\u0026rsquo;s look at our definition of conditional probability from the other direction and show that $\\Pc{A}{B}$ as defined in Definition 2 conforms to our traditional usage. To start, note that $\\sigma(\\ind{B}) = \\se{\\emp, B, B^\\comp, \\Omega}$, and since $\\Pc{A}{B}$ is $\\sigma(\\ind{B})$ measurable, it must be constant on each of the sets $B, B^\\comp$, thereby necessitating\n$$ \\Pc{A}{B}(\\omega) = \\begin{cases} \\frac{\\PP(A \\cap B)}{\\PP(B)} \u0026amp; \\text{ if } \\omega \\in B \\\\\n\\frac{\\PP(A \\cap B^\\comp)}{\\PP(B^\\comp)} \u0026amp; \\text{ if } \\omega \\in B^\\comp \\end{cases} $$ because of property 3. of Theorem 1 by taking $G$ to be $B$ and $B^\\comp$. Of course, if any of the sets $B$ or $B^\\comp$ is of measure $0$, then you can take the corresponding value for $\\Pc{A}{B}(\\omega)$ to be anything in $[0,1]$ and it won\u0026rsquo;t matter since the concept of conditional probability is defined up to sets of measure $0$.\nProperties of conditional probability Positivity property (property 2. above) and monotone convergence property (property 3. above) imply $0 \\le \\Pc{A}{\\G} \\le 1$ a.s. for any $A \\in \\F$, $\\Pc{A}{\\G} = 0$ a.s. if and only if $\\PP(A) = 0$, and $\\Pc{A}{\\G} = 1$ a.s. if and only if $\\PP(A) = 1.$\nLet $A_1, A_2, \\ldots \\in \\F$ be a sequence of disjoint sets. By linearity (property 1. above) and monotone convergence theorem for conditional expectation, we see that\n$$ \\Pc{\\bigcup_n A_n}{\\G} = \\sum_n \\Pc{A_n}{\\G} \\quad \\text{a.s.} \\tag{2} \\label{eq2} $$\nIf $A_n \\in \\F$, $n \\ge 1$ and $\\limn A_n = A$, then we also have $\\limn \\Pc{A_n}{\\G} = \\Pc{A}{G}$ a.s..\nIt seems very tempting from the foregoing discussion to claim that $\\Pc{\\cdot}{\\G}$ is a probability measure on $\\F$ for almost all $\\omega \\in \\Omega$, but except for some nice spaces, which we will discuss below, this isn\u0026rsquo;t true. Let us first try to see this intuitively [2]. Equation \\eqref{eq2} holds for all $\\omega \\in \\Omega$ EXCEPT for some null set which may well depend on the particular sequence $\\se{A_n}$. It does NOT stipulate that there exists a fixed null set $N \\in \\F$ such that\n$$ \\Pc{\\bigcup_n A_n}{\\G}(\\omega) = \\sum_n \\Pc{A_n}{\\G}(\\omega), \\quad \\omega \\in N^\\comp $$\nfor every disjoint sequence $\\se{A_n} \\subseteq \\F$. Except in trivial cases, there are uncountably many disjoint sequences, and therefore we will need uncountable union of such null sets to be of measure $0$, which of course may not even be defined let alone be of measure $0$. To further drive this point home you can take a look at an explicit example of how this can fail in an exercise in [3] page 210.\nRegular conditional probability Motivated by our discussion above, we define regular conditional probability as follows.\nDefinition 3:  Let $\\probsp$ be a probability space and $\\G, \\cH$ be sub-$\\sigma$-algebras of $\\F$. A regular conditional probability on $\\cH$ given $\\G$ is a function $\\PP(\\cdot, \\cdot) \\colon \\cH \\times \\Omega \\to [0,1]$ such that\n for a.e. $\\omega \\in \\Omega$, $\\PP(\\cdot, \\omega)$ is a probability measure on $\\cH$, for each $A \\in \\cH$, $\\PP(A, \\cdot)$ is a $\\G$-measurable function on $\\Omega$ coinciding with the conditional probability of $A$ given $\\G$, i.e., $\\PP(A, \\cdot) = \\Pc{A}{\\G}$ a.s.  Let\u0026rsquo;s show that this definition is not outrageous by showing that it agrees with our traditional notion of conditional pdf and conditional expectation. So suppose that $X$ and $Y$ are random variables which have a joint probability density function $f_{X,Y}(x,y).$ This means that we are considering the probability space $(\\R^2, \\B(\\R^2), \\PP)$ with $X$ and $Y$ being the coordinate random variables, i.e. $(x,y) \\mapsto x$ and $(x,y) \\mapsto y$ respectively, and having an absolutely continuous distribution function $F_{X,Y}(x,y)$ such that\n$$ F_{X,Y}(x,y) = \\int_{-\\infty}^y \\int_{-\\infty}^x f_{X,Y}(s,t) \\; \\dd s \\, \\dd t $$\nWe recall that $f_X(x) = \\int_\\R f_{X,Y}(x,y) \\; \\dd y$ and $f_Y(y) = \\int_\\R f_{X,Y}(x,y) \\; \\dd x$ act as probability density functions for $X$ and $Y$ respectively, and\n$$ f_{X \\mid Y}(x \\mid y) = \\begin{cases} \\frac{f_{X,Y}(x,y)}{f_Y(y)} \u0026amp; \\text{ if } f_Y(y) \\neq 0 \\\\\n0 \u0026amp; \\text{ otherwise} \\end{cases} $$ defines the elementary conditional pdf $f_{X \\mid Y}$ of $X$ given $Y$. By Fubini\u0026rsquo;s theorem $f_X$ and $f_Y$ are Borel functions on $\\R$ and so $f_{X \\mid Y}$ is a Borel function on $\\R^2$. Let $\\cH = \\B(\\R^2) = \\sigma(X, Y)$ and $\\G = \\sigma(Y) = \\R \\times \\B(\\R)$. For $A \\in \\cH$ and $\\omega = (x,y) \\in \\R^2$ we define\n$$ \\PP(A, \\omega) = \\int_{\\set{s}{(s,y) \\in A}} f_{X \\mid Y}(s \\mid y) \\; \\dd s $$\nThen for each $\\omega \\in \\R^2$, $\\PP(\\cdot, \\omega)$ is a probability measure on $\\cH$, and for each $A \\in \\cH$, $\\PP(A, \\cdot)$ is a Borel function in $y$ and hence $\\G$-measurable. To verify that $\\PP(A, \\cdot) = \\Pc{A}{\\G}$ for any $A \\in \\cH$ we just need to verify property 3. of Theorem 1. To this end, fix $A \\in \\cH$ and $G \\in \\G$, and note that $G$ must be of the form $G = \\R \\times B$ for $B \\in \\B(\\R)$. Thus\n\\begin{align*} \\int_G \\PP(A, \\omega) \\, \\dP(\\omega) \u0026amp;= \\int_B \\int_R \\PP(A, (s,t)) f_{X \\mid Y}(s,t) \\; \\dd s \\, \\dd t \\quad \\text{(by absolute continuity and Fubini\u0026rsquo;s theorem)} \\\\\n\u0026amp;= \\int_B \\int_R \\left[ \\int_{\\set{u}{(u,t) \\in A}} f_{X \\mid Y}(u \\mid t) \\; \\dd u \\right] f_{X \\mid Y}(s,t) \\; \\dd s \\, \\dd t \\\\\n\u0026amp;= \\int_B \\left[ \\int_{\\set{u}{(u,t) \\in A}} f_{X \\mid Y}(u \\mid t) \\; \\dd u \\right] f_{Y}(t) \\; \\dd t \\\\\n\u0026amp;= \\int_B \\int_{\\set{u}{(u,t) \\in A}} f_{X, Y}(u, t) \\; \\dd u \\, \\dd t \\\\\n\u0026amp;= \\int_B \\int_{\\R} \\ind{A}(u,t) f_{X, Y}(u, t) \\; \\dd u \\, \\dd t \\\\\n\u0026amp;= \\int_{G} \\ind{A}(\\omega) \\; \\dP(\\omega) \\end{align*}\nand so $\\PP(A, \\cdot) = \\Exc{\\ind{A}}{\\G} = \\Pc{A}{\\G}$. Hence, $\\PP(A, \\omega)$ is a regular probability measure on $\\cH$ given $\\G$.\nFor the corresponding analysis for conditional expectation, let $h$ be a Borel function on $\\R^2$ such that $\\Ex{|h(X,Y)|} = \\int_\\R \\int_\\R |h(x,y)| f_{X,Y}(x,y) \\; \\dd x \\, \\dd y \u0026lt; \\infty$. Set\n$$ g(y) = \\int_\\R h(s, y) f_{X \\mid Y}(s \\mid y) \\; \\dd s $$\n$g(y)$ is the traditional conditional density of $h(X, Y)$ given $Y = y$. Then the claim is that $g(Y) = \\Exc{h(X,Y)}{\\sigma(Y)}$ a.s.. The typical element of $\\sigma(Y)$ has the form $\\set{\\omega \\in \\R^2}{Y(\\omega) \\in B}$, where $B \\in \\B(\\R).$ Hence, we must show that\n$$ L = \\Ex{h(X,Y) \\ind{B}(Y)} = \\Ex{g(Y) \\ind{B}(Y)} = R $$\nBut we can write $L$ and $R$ as\n\\begin{align*} L \u0026amp;= \\int \\int h(x,y) \\ind{B}(y) f_{X,Y}(x,y) \\; \\dd x \\, \\dd y \\\\\nR \u0026amp;= \\int g(y) \\ind{B}(y) f_Y(y) \\; \\dd y \\end{align*}\nand they are equal by Fubini\u0026rsquo;s theorem.\nIn general, we have the following useful theorem (taken from [2]) which allows us to view conditional expectations as ordinary expectations relative to the measure induced by regular conditional probability.\nTheorem 2:  Consider the setting of Definition 3 and denote $\\PP_\\omega(\\cdot) = \\PP(\\cdot, \\omega)$. Let $X$ be an $\\cH$-measurable function with $\\Ex{X} \u0026lt; \\infty$. Then\n$$ \\Exc{X}{\\G}(\\omega) = \\int_\\Omega X \\; \\dP_\\omega \\quad \\text{a.s.} \\tag{3} \\label{eq3} $$\nProof:  Recall the monotone class theorem for functions:\n Let $\\mathscr{H}$ be a family of nonnegative functions on $\\Omega$ which contains all indicators of sets of some class $\\cH$ of subsets of $\\Omega$. If either (i) $\\cH$ is a $\\pi$-class and $\\mathscr{H}$ is a $\\lambda$-system, or (ii) $\\cH$ is a $\\sigma$-algebra and $\\mathscr{H}$ is a monotone system, then $\\mathscr{H}$ contains all nonnegative $\\sigma(\\cH)$-measurable functions.\n By separate considerations of $X^+$ and $X^-$, it may be supposed that $X \\ge 0$. Let\n$$ \\mathscr{H} = \\set{X}{X \\ge 0, X \\text{ is } \\cH \\text{-measurable, and \\eqref{eq3} holds for } X} $$\nBy the definition of regular conditional probability, $\\ind{A} \\in \\mathscr{H}$ for $A \\in \\cH$. $\\cH$ is a $\\sigma$-algebra. Let\u0026rsquo;s show that $\\mathscr{H}$ is a monotone system. If $X_1, X_2 \\in \\mathscr{H}$ and $c_1, c_2 \\ge 0$, then $c_1 X_1 + c_2 X_2 \\ge 0$, $c_1 X_1 + c_2 X_2$ is $\\cH$-measurable and Equation \\eqref{eq3} holds because of linearity of expectation and conditional expectation, and thus $c_1 X_1 + c_2 X_2 \\in \\mathscr{H}$. If $\\se{X_n} \\subseteq \\mathscr{H}$ such that $X_n \\uparrow X$, then $X \\ge 0$, $X$ is $\\cH$-measurable, and Equation \\eqref{eq3} holds for $X$ because of monotone convergence theorem for expectation and conditional expectation, and thus $X \\in \\mathscr{H}$. Therefore, by the monotone class theorem $\\mathscr{H}$ contains all nonnegative $\\cH$-measurable functions. $\\square$\nRegular conditional distribution In some cases even the concept of regular conditional probability in inadequate, and that motivates the concept of regular conditional distributions.\nDefinition 4:  Let $\\probsp$ be a probability space, $\\G \\subseteq \\F$ a $\\sigma$-algebra, $(\\Lambda, \\LL)$ a measurable space, and $T \\colon \\Omega \\to \\Lambda$ a measurable mapping. A regular conditional distribution for $T$ given $\\G$ is a function $\\PP_T \\colon \\LL \\times \\Omega \\to [0,1]$ such that\n for a.e. $\\omega \\in \\Omega$, $\\PP_T(\\cdot, \\omega)$ is a probability measure on $\\LL$, for each $A \\in \\LL$, $\\PP_T(A, \\cdot)$ is a $\\G$-measurable function on $\\Omega$ coinciding with the conditional probability of $T^{-1}(A)$ given $\\G$, i.e., $\\PP_T(A, \\cdot) = \\Pc{T^{-1}(A)}{\\G}$ a.s.  It is clear that when $\\Lambda = \\Omega$, $\\LL = \\cH \\subseteq \\F$ and $T$ is the identity map, $\\PP_T$ is exactly the regular conditional probability as defined in Definition 3.\nNow would be a good time to reread the first \u0026ldquo;rumination\u0026rdquo; in the section Conditional Probability and realize that the definition of regular conditional distribution is in fact well motivated.\nA corresponding version of Theorem 2 exists, proof of which I\u0026rsquo;ll leave as an easy exercise:\nTheorem 3:  In the setting of Definition 4, if $\\PP_T^\\omega(A) = \\PP_T(A, \\omega)$ and $h \\colon \\Lambda \\to \\R$ is a Borel function with $\\Ex{|h(T)|} \u0026lt; \\infty$, then\n$$ \\Exc{h(T)}{\\G}(\\omega) = \\int_\\Lambda h(\\lambda) \\; \\PP_T^\\omega(\\dd \\lambda) $$\nTo see the power of thinking about conditional probabilities like this, let\u0026rsquo;s give an unbelievably short proof of conditional H√∂lder\u0026rsquo;s inequality [2]. Contrast it with other proofs.\nTheorem 4:  If $X,Y$ are random variables on $\\probsp$, $\\G \\subseteq \\F$ is a $\\sigma-$algebra and $ 1 \u0026lt; p \u0026lt; \\infty$, $1/p + 1/q = 1$, then\n$$ \\Exc{|XY|}{\\G} \\le \\left( \\Exc{|X|^p}{\\G} \\right)^{1/p} \\left( \\Exc{|Y|^q}{\\G} \\right)^{1/q} \\quad \\text{a.s.} $$\nProof:  For $B \\in \\B(\\R^2)$ and $\\omega \\in \\Omega$, let $\\PP_{X,Y}^\\omega(B) = \\PP_{X,Y}(B, \\omega)$ be the regular conditional distribution for $(X,Y)$ given $\\G$. Theorem 3 allows us to write\n\\begin{align*} \\Exc{|XY|}{\\G}(\\omega) \u0026amp;= \\int_{\\R^2} |x y| \\; \\PP_{X,Y}^\\omega(\\dd (x,y)) \\\\\n\\left( \\Exc{|X|^p}{\\G} \\right)^{1/p} \u0026amp;= \\left( \\int_{\\R^2} |x|^p \\; \\PP_{X,Y}^\\omega(\\dd (x,y)) \\right)^{1/p} \\\\\n\\left( \\Exc{|Y|^q}{\\G} \\right)^{1/q} \u0026amp;= \\left( \\int_{\\R^2} |y|^q \\; \\PP_{X,Y}^\\omega(\\dd (x,y)) \\right)^{1/q} \\end{align*}\nAnd now our desired inequality follows immediately from the ordinary H√∂lder\u0026rsquo;s inequality. $\\square$\nExistence of regular conditional distribution Before we discuss their existence, let us define the concept of standard Borel space [6].\nDefinition 5:  Let $(X, \\X)$ and $(Y, \\Y)$ be measurable spaces. They are called isomorphic is there exists a bijection $f \\colon X \\to Y$ such that $f$ and $f^{-1}$ are both measurable. The function $f$ is called an isomorphism.\nDefinition 6:  A measurable space $(X, \\X)$ is called a standard Borel space if it satisfies any of the following equivalent conditions:\n $(X, \\X)$ is isomorphic to some compact metric space with the Borel $\\sigma$-algebra; $(X, \\X)$ is isomorphic to some Polish space (i.e., a separable complete metric space) with the Borel $\\sigma$-algebra; $(X, \\X)$ is isomorphic to some Borel subset of some Polish space with the Borel $\\sigma$-algebra.  As you can guess most spaces we deal with are standard Borel spaces. Durrett [5] calls these space nice since we already have too many things named after Borel. I am not sure I agree with his reasoning but I like Durrett\u0026rsquo;s terminology.\nThe next two theorems show the existence of regular conditional distribution and are taken from [5, Section 4.1.3]. See also [4, Section V.8].\nTheorem 5:  Regular conditional distribution exists if $(\\Lambda, \\LL)$ is nice.\nA generalization of the last theorem:\nTheorem 6:  Suppose $(\\Lambda, \\LL)$ is a nice space, $T$ and $S$ are measurable mappings from $\\Omega$ to $\\Lambda$, and $\\G = \\sigma(S)$. Then there exists a function $\\mu \\colon \\Lambda \\times \\LL \\to [0,1]$ such that\n for a.e. $\\omega \\in \\Omega$, $\\mu(S(\\omega), \\cdot)$ is a probability measure on $\\LL$, for each $A \\in \\LL$, $\\mu(S(\\cdot), A) = \\Pc{T^{-1}(A)}{\\G}$ a.s.  It is instructive to prove Theorem 5 in the special case when $(\\Lambda, \\LL) = (\\R^n, \\B(\\R^n))$. The theorem and the proof is taken from [2].\nTheorem 7:  In the setting of Definition 4, let $(\\Lambda, \\LL) = (\\R^n, \\B(\\R^n))$ and $T = (T_1, \\ldots, T_n) \\colon \\Omega \\to \\R^n$. Then there exists a regular conditional distribution for $T$ given $\\G$.\nProof:  Let\u0026rsquo;s recall the definition of an $n$-dimensional distribution function on $\\R^n$ (the Russian convention of left-continuous distribution function).\n An $n$-dimensional distribution function on $\\R^n$ is a function $F$ satisfying: $$\\lim_{x_j \\to - \\infty} F(x_1, \\ldots, x_n) = 0, \\quad 1 \\le j \\le n \\tag{i}$$ $$\\lim_{\\substack{x_j \\to \\infty \\\\ 1 \\le j \\le n}} F(x_1, \\ldots, x_n) = 1 \\tag{ii}$$ $$\\lim_{y_j \\uparrow x_j} F(x_1, \\ldots, x_{j-1}, y_j, x_{j+1}, \\ldots, x_n) = F(x_1, \\ldots, x_j, \\ldots, x_n), \\quad 1 \\le j \\le n \\tag{iii}$$ \\begin{align*}\\Delta_n^{a,b} \u0026amp;:= F(b_1, \\ldots, b_n) - \\sum_{j=1}^n F(b_1, \\ldots, b_{j-1}, a_j, b_{j+1}, \\ldots, b_n) \\\\ \u0026amp;+ \\sum_{1 \\le j \u0026lt; k \\le n} F(b_1, \\ldots, b_{j-1}, a_j, b_{j+1}, \\ldots, b_{k-1}, a_k, b_{k+1}, \\ldots, b_n) - \\cdots (-1)^n F(a_1, \\ldots, a_n) \\\\ \u0026amp;\\ge 0 \\tag{iv}\\end{align*} \n We will try to construct a distribution function on $\\R^n$. To this end, for any rational number $r_1, \\ldots, r_n$ and $\\omega \\in \\Omega$, define\n$$ F_n^\\omega(r_1, \\ldots, r_n) = \\Pc{\\bigcap_{i=1}^n \\se{T_i \u0026lt; r_i}}{\\G}(\\omega) \\tag{4} \\label{eq4} $$\nIt\u0026rsquo;s evident that the properties of conditional probability discussed above imply there is a null set $N \\in \\G$ such that for $\\omega \\in N^\\comp$ and all rational numbers $r_i, r_i\u0026rsquo;, q_{i,m}$ the following holds\n$$ F_n^\\omega(r_1, \\ldots, r_n) \\ge F_n^\\omega(r_1\u0026rsquo;, \\ldots, r_n\u0026rsquo;) \\text{ if } r_i \u0026gt; r_i\u0026rsquo;, \\, 1 \\le i \\le n $$\n$$ F_n^\\omega(r_1, \\ldots, r_n) = \\lim_{\\substack{q_{i,m} \\uparrow r_i \\\\ 1 \\le i \\le n}} F_n^\\omega(q_{1, m}, \\ldots, q_{n, m}) $$\n$$ \\lim_{r_i \\to -\\infty} F_n^\\omega(r_1, \\ldots, r_n) = 0, \\quad 1 \\le i \\le n $$\n$$ \\lim_{\\substack{r_i \\to \\infty \\\\ 1 \\le i \\le n}} F_n^\\omega(r_1, \\ldots, r_n) = 1 $$\n$$ \\Delta_n^{r, r\u0026rsquo;} F_n^\\omega \\ge 0 \\text{ if } r \\le r\u0026rsquo; $$\nwhere $r \\le r'$ means $r_i \\le r_i'$ for all $1 \\le i \\le n$. Having defined $F_n^\\omega$ for rational values, define for any real numbers $x_1, \\ldots, x_n$ as follows\n$$ F_n^\\omega(x_1, \\ldots, x_n) = \\begin{cases} \\lim_{\\substack{r_i \\uparrow x_i \\\\ r_i \\in \\Q \\\\ 1 \\le i \\le n}} F_n^\\omega(r_1, \\ldots, r_n) \u0026amp; \\text{ if } \\omega \\in N^\\comp \\\\\n\\Prob{\\bigcap_{i=1}^n \\se{T_i \u0026lt; r_i}} \u0026amp; \\text{ if } \\omega \\in N \\end{cases} \\tag{5} \\label{eq5} $$\nThen for each $\\omega \\in \\Omega$, $F_n^\\omega(x_1, \\ldots, x_n)$ is an $n$-dimensional distribution function and hence determines a Lebesgue-Stieltjes measure $\\mu_\\omega$ on $\\B(\\R^n)$ with $\\mu_\\omega(\\R^n) = 1$. For $B \\in \\B(\\R^n)$ and $\\omega \\in \\Omega$ define\n$$ \\PP_T(B, \\omega) = \\mu_\\omega(B) $$\nIf\n\\begin{align*} \\cH \u0026amp;= \\set{B \\in \\B(\\R^n)}{\\PP_T(B, \\cdot) = \\Pc{T^{-1}(B)}{\\G} \\text{ a.s.} } \\\\\n\\D \u0026amp;= \\set{B \\in \\B(\\R^n)}{B = [-\\infty, r_1) \\times \\cdots \\times [-\\infty, r_n), \\, r_i \\in \\Q} \\end{align*}\nthen a moment\u0026rsquo;s reflection will convince you that that $\\cH$ is a $\\lambda-$class, $\\D$ is a $\\pi-$class, and $\\cH \\supseteq \\D$. Hence, by the $\\pi-\\lambda$ theorem $\\cH \\supseteq \\sigma(\\D) = \\B(\\R^n)$, or in other words, $\\PP_T(B, \\omega)$ is a regular conditional distribution for $T$ given $\\G$. $\\square$\nIn fact, this theorem is easily extended to $(\\R^\\infty, \\B(\\R^\\infty))$ as follows: For all $n \\ge 1$, define $F_n^\\omega$ as in Equation \\eqref{eq4}. Select the null set $N \\in \\G$ such that in addition to the conditions it satisfies above we also have the consistency condition\n$$ \\lim_{r_{n+1} \\to \\infty} F_n^\\omega(r_1, \\ldots, r_n, r_{n+1}) = F_n^\\omega(r_1, \\ldots, r_n), \\quad n \\ge 1 $$\nFor reals $x_1, \\ldots, x_n$ define just like Equation \\eqref{eq5}. Then for each $\\omega \\in \\Omega$, $\\se{F_n^\\omega, \\, n \\ge 1}$ is a consistent family of distribution functions, and hence by the Kolmogorov extension theorem there exists a unique measure $\\mu_\\omega$ on $(\\R^\\infty, \\B(\\R^\\infty))$ whose finite dimensional distributions are $\\se{F_n^\\omega, \\, n \\ge 1}$. Define $\\PP_T(B, \\omega) = \\mu_\\omega(B)$ for $B \\in \\B(\\R^\\infty)$. If\n\\begin{align*} \\cH \u0026amp;= \\set{B \\in \\B(\\R^\\infty)}{\\PP_T(B, \\cdot) = \\Pc{T^{-1}(B)}{\\G} \\text{ a.s.} } \\\\\n\\D \u0026amp;= \\bigcup_{n=1}^\\infty \\set{B \\in \\B(\\R^\\infty)}{B = [-\\infty, r_1) \\times \\cdots \\times [-\\infty, r_n) \\times \\R \\times \\R \\times \\cdots, \\, r_i \\in \\Q} \\end{align*}\nthen $\\cH$ is a $\\lambda-$class, $\\D$ is a $\\pi-$class, and $\\cH \\supseteq \\D$. Hence, by the $\\pi-\\lambda$ theorem $\\cH \\supseteq \\sigma(\\D) = \\B(\\R^\\infty)$.\nReferences  Williams, David. Probability with Martingales. Cambridge mathematical textbooks, Cambridge University Press, 1991. Chow, Yuan Shih and Teicher, Henry. Probability Theory: Independence, Interchangeability, Martingales, 3rd edn. Springer-Verlag New York, 1997. Halmos, P. R.. Measure Theory. Van Nostrand, Princeton, N. J., 1950; Springer-Verlag, Berlin and New York, 1974. Parthasarathy, K. R.. Probability Measures on Metric Spaces. AMS Chelsea Publishing, 1967. Durrett, R.: Probability: Theory and Examples, 5th edn. Cambridge University Press, 2019.  https://encyclopediaofmath.org/wiki/Standard_Borel_space  ","date":1612563879,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612563879,"objectID":"89e76b2b8aafbb3d437b1acad32a24b6","permalink":"https://makkar.github.io/post/regularcondprob/","publishdate":"2021-02-05T17:24:39-05:00","relpermalink":"/post/regularcondprob/","section":"post","summary":"Some intuition behind this concept","tags":[],"title":"A note on conditional probability","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\ZZ}{\\Z_{\\geq 0}^N} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\NN}{\\mathcal{N}} \\newcommand{\\LL}{\\mathcal{L}} \\newcommand{\\PP}{\\mathbb{P}} \\newcommand{\\OO}{\\mathcal{O}} \\newcommand{\\I}{\\mathcal{I}} \\newcommand{\\Prob}[1]{\\PP \\left( #1 \\right)} \\newcommand{\\eq}[1]{\\begin{align*}#1\\end{align*}} \\newcommand{\\eql}[1]{\\begin{align}#1\\end{align}} \\newcommand{\\ind}[1]{\\mathbf{1}_{#1}} \\newcommand{\\indo}[1]{\\mathbf{1}_{#1}(\\omega)} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\G}{\\mathcal{G}} \\newcommand{\\EE}{\\mathcal{E}} \\newcommand{\\probsp}{(\\Omega, \\F, \\PP)} \\newcommand{\\integ}[1]{\\int_{\\Omega} #1 \\dmu} \\newcommand{\\B}{\\mathcal{B}} \\newcommand{\\Bo}{\\B(\\R)} \\newcommand{\\Bon}[1]{\\B(\\R^{#1})} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sumn}{\\sum_{n=1}^{\\infty}} \\newcommand{\\trans}{\\intercal} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\ex}[1]{\\exp\\left{#1\\right}} \\newcommand{\\comp}{\\mathsf{c}} \\newcommand{\\emp}{\\varnothing} \\newcommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor} \\newcommand{\\ceil}[1]{\\left \\lceil{#1}\\right \\rceil} \\newcommand{\\se}[1]{\\left\\{ #1 \\right\\}} \\newcommand{\\set}[2]{\\left\\{ #1 ; : ; #2 \\right\\}} \\newcommand{\\sett}[2]{\\left\\{ #1 ; | ; #2 \\right\\}} \\newcommand{\\Ex}[1]{\\E\\left[#1\\right]} \\newcommand{\\pard}[2]{\\frac{\\partial #1}{\\partial #2}} \\newcommand{\\dd}{\\mathrm{d}} \\newcommand{\\ph}[1]{\\varphi^{-1}\\left(#1\\right)} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dy}{dy} \\DeclareMathOperator{\\du}{du} \\DeclareMathOperator{\\dz}{d\\matr{z}} \\DeclareMathOperator{\\dt}{dt} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator{\\dP}{d\\PP} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\Ord}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} \\DeclareMathOperator{\\V}{\\mathrm{Var}} \\DeclareMathOperator{\\Log}{\\mathrm{Log}} \\DeclareMathOperator{\\O}{\\mathcal{O}} $$\nIntroduction Finding a longest increasing subsequence is a well-known problem in computer science (note that I use the article \u0026ldquo;a\u0026rdquo; instead of \u0026ldquo;the\u0026rdquo; because there could be multiple longest subsequences): Given a sequence $\\se{a_1, \\ldots, a_n}$ of real numbers, we want to find a subsequence $\\se{a_{i_1}, \\ldots, a_{i_k}}$ such that $0 \\le i_1 \u0026lt; \\cdots \u0026lt; i_k \\le n$, $a_{i_1} \\le \\cdots \\le a_{i_k}$, and the subsequence is as long as possible. It has a very easy dynamic programming solution with a time complexity of $\\O(n^2)$ and a slightly more involved solution with a time complexity of $\\O(n \\log n)$. But we are not interested in these algorithms in this blog post.\nWe are interested in studying the asymptotics of the length of the longest increasing subsequences of a sequence whose elements are coming from a random permutation. This simple to state problem will take us on a beautiful journey touching topics from combinatorics and probability theory. In particular, we will get to see the very elegant technique of Poissonization.\nProblem Let us start by stating precisely what we are trying to prove. To do that we first define some notation. For any integer $n \\ge 1$, let $S_n$ be the group of permutations of order $n$, i.e., it contains all permutations of $\\se{1, 2, \\ldots, n}$ and hence $S_n$ contains $n!$ elements. If $\\pi \\in S_n$ then a subsequence of $\\pi$ is a sequence $\\se{\\pi(i_1), \\ldots, \\pi(i_k)}$ such that $1 \\leq i_1 \u0026lt; \\cdots \u0026lt; i_k \\leq n$. It is an increasing subsequence if $\\pi(i_1) \u0026lt; \\cdots \u0026lt; \\pi(i_k)$ and similarly for the decreasing subsequence. Consider the uniform measure $\\mu_n$ on the discrete measurable space $(S_n, 2^{S_n})$, i.e., $\\mu_n(\\pi) = 1 / n!$ for any $\\pi \\in S_n$. For $\\pi \\in S_n$ define $l_n(\\pi)$ to be the maximal length of an increasing subsequence of $\\pi$, i.e., $l_n(\\pi)$ is the largest $k$ such that there are integers $1 \\leq i_1 \u0026lt; \\cdots \u0026lt; i_k \\leq n$ so that $\\pi(i_1) \u0026lt; \\cdots \u0026lt; \\pi(i_k)$. Similarly define $d_n(\\pi)$ to be the maximal length of a decreasing subsequence of $\\pi$.\nSince $l_n$ is a random variable we can consider its expectation $L_n = \\E[l_n]$ on the probability space $(S_n, 2^{S_n}, \\mu_n)$. It can be explicitly written as $$ L_n = \\frac{1}{n!} \\sum_{\\pi \\in S_n} l_n(\\pi) \\tag{1} \\label{eq1} $$ We want to study the limiting properties of the sequence $\\se{L_n}_{n \\in \\N}$. Specifically we will show that $$ \\frac{L_n}{\\sqrt{n}} \\to \\gamma \\text{ almost surely} \\tag{2} \\label{eq2} $$ for some constant $\\gamma$. It is known that $\\gamma = 2$. We will not be showing this, but we will show that $1 \\le \\gamma \\le e$.\nCombinatorial results We now prove some useful combinatorial results. The first result is called the Erd≈ës‚ÄìSzekeres theorem.\nTheorem 1 [Erd≈ës‚ÄìSzekeres theorem]:  In any sequence $\\se{a_1, a_2, \\ldots, a_{mn+1}}$ of $mn+1$ distinct real numbers, there exists either an increasing subsequence $a_{i_1} \u0026lt; \\cdots \u0026lt; a_{i_{m+1}}$ $(i_1 \u0026lt; \\cdots \u0026lt; i_{m+1})$ of length $m+1$, or a decreasing subsequence $a_{j_1} \u0026gt; \\cdots \u0026gt; a_{j_{n+1}}$ $(j_1 \u0026lt; \\cdots \u0026lt; j_{n+1})$ of length $n+1$.\nProof:  For $1 \\leq i \\leq mn+1$ define $t_i$ to be the length of a longest increasing subsequence starting at $a_i$, i.e., the first element of a longest increasing subsequence must be $a_i$ and the rest of the $a$'s must have indices greater than $i$. If $t_i \\geq m+1$ for some $i$ then we are done since we then have an increasing subsequence of length $m+1$, so assume $t_i \\leq m$ for all $i$. Since $t_i \\geq 1$, pigeonhole principle now implies that there is some integer $1 \\leq k \\leq m$ such that $t_i = k$ for at least $n+1$ $i$'s. Let $t_i = k$ for all $i \\in (j_1 \u0026lt; \\cdots \u0026lt; j_{n+1})$. Now note that if $a_{j_l} \u0026lt; a_{j_{l+1}}$ for some $1 \\leq l \\leq n$, then we would obtain an increasing subsequence of length $k+1$ starting at $a_{j_l}$ because there is an increasing subsequence of length $k$ starting at $a_{j_{l+1}}$. But this contradicts the fact that $t_{j_l} = k,$ and thus $a_{j_l} \u0026gt; a_{j_{l+1}}$ for all $1 \\leq l \\leq n$. But now this gives us a decreasing subsequence $a_{j_1} \u0026gt; \\cdots \u0026gt; a_{j_{n+1}}$ of length $n+1$. $\\square$\nThe next two theorems prove lower and upper bounds for $L_n / \\sqrt{n}$.\nTheorem 2:  $L_n / \\sqrt{n}$ is lower bounded as follows $$ L_n \\geq \\sqrt{n} \\text{ for all } n \\geq 1 \\tag{3} \\label{eq3} $$\nProof:  Theorem 1 implies $l_n(\\pi) d_n(\\pi) \\geq n$ for all $\\pi \\in S_n$. Note that for every permutation $\\pi \\in S_n$ there exists an inverse permutation $\\pi'$ such that $l_n(\\pi) = d_n(\\pi\u0026rsquo;)$ and thus we can write $L_n$ also as $$ L_n = \\frac{1}{n!} \\sum_{\\pi \\in S_n} d_n(\\pi) \\tag{4} \\label{eq4} $$ Therefore, averaging the two ways of computing the expectation $L_n$ (Equations \\eqref{eq1} and \\eqref{eq4}) and using the AM-GM inequality we have $$ L_n = \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\frac{l(\\pi) + d(\\pi)}{2} \\geq \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\sqrt{l(\\pi) d(\\pi)} \\geq \\frac{1}{n!} \\sum_{\\pi \\in S_n} \\sqrt{n} = \\sqrt{n} \\qquad \\square $$\nTheorem 3:  Upper bound: $$ \\lims \\frac{L_n}{\\sqrt{n}} \\leq e \\tag{5} \\label{eq5} $$\nProof:  If $\\pi \\in S_n$ and $1 \\leq k \\leq n$ let $X_{n,k}(\\pi)$ be the number of increasing subsequences of $\\pi$ which are of length $k$. They are exactly those subsets $S \\subseteq \\se{1, \\ldots, n}$ such that $|S| = k$ and if $S = \\se{i_1 \u0026lt; \\cdots \u0026lt; i_k}$ then $\\pi(i_1) \u0026lt; \\cdots \u0026lt; \\pi(i_k)$. On the probability space $(S_n, 2^{S_n}, \\mu_n)$, by the linearity of expectation, the expected value of $X_{n,k}$ is given by the number of ways to select $k$ subsets of $\\se{1, \\ldots, n}$ times the probability that a selection has all the elements in the increasing order. The number of ways is simply $\\binom{n}{k}$ and the probability is $1/k!$ and thus $$ \\E[X_{n,k}] = \\frac{1}{k!} \\binom{n}{k} $$ The Taylor expansion of $e^x$ implies $e^x \\ge x^k / k!$. Substituting $x = k$ we get $k! \\geq (k/e)^k$. Therefore, we get $$ \\E[X_{n,k}] = \\frac{1}{k!} \\binom{n}{k} = \\frac{n(n-1) \\cdots (n-k-1)}{(k!)^2} \\leq \\frac{n^k}{(k/e)^{2k}} $$\nNow for a discrete random variable $X_{n,k}$ we can write $\\E[X_{n,k}] = \\sum_{i=0}^n \\mu_n(X_{n,k} \\geq i)$ and thus $\\mu_n(X_{n,k} \\geq 1) \\leq \\Ex{X_{n,k}}$. Also note that $l_n(\\pi) \\geq k$ if and only if $X_{n,k}(\\pi) \\geq 1$. We thus get $$ \\mu_n(l_n \\geq k) = \\mu_n(X_{n,k} \\geq 1) \\leq \\E[X_{n,k}] \\leq \\frac{n^k}{(k/e)^{2k}} $$ Fix an arbitrary $\\delta \u0026gt; 0$ and let $k = \\ceil{(1+\\delta)e\\sqrt{n}}$ in the inequality above to get $$ \\mu_n(l_n \\geq k) \\leq \\frac{n^k}{(k/e)^{2k}} \\leq \\left( \\frac{1}{1+\\delta} \\right)^{2k} \\leq \\left( \\frac{1}{1+\\delta} \\right)^{2(1+\\delta)e\\sqrt{n}} $$ Since $l_n \\leq n$, we have $$ L_n = \\Ex{l_n} \\leq \\mu_n(l_n \u0026lt; k) (1+\\delta)e\\sqrt{n} + \\mu_n(l_n \\geq k) n \\leq (1+\\delta)e\\sqrt{n} + O(e^{-c\\sqrt{n}}) $$ where $c$ is some positive constant that depends on $\\delta$. Since $\\delta$ was arbitrary, we can let $\\delta \\to 0$ and then take $\\limsup$ to get Equation \\eqref{eq5}. $\\square$\nPoissonization To be able to show \\eqref{eq2} we will draw a correspondence between this problem of longest increasing subsequences and a seemingly unrelated problem called the Poissonized version. This Poissonized version will allow us to use the powerful Subadditive Ergodic Theorem (Theorem 5 below) to show \\eqref{eq2}.\nTo this end, assume an underlying probability space $\\probsp$ and let $N$ be a Poisson random measure on $\\mathbb{R} _ + ^2$ with mean measure given by the Lebesgue measure on $\\mathbb{R} _ + ^2$. In other words $N:\\Omega \\times \\B(\\R _ + ^2) \\to \\bar{\\R} _ +$ is a transition kernel from $(\\Omega, \\F)$ into $(\\R_+^2, \\B(\\R_+^2))$ with $\\int_\\Omega \\PP(\\dd \\omega) N(\\omega, A) = \\text{Leb}(A)$ for any $A \\in \\B(\\R_+^2)$. We can view the random process as a sequence $\\se{(X_i, Y_i)} _ {i \\geq 1}$ of independent random variables taking values in $\\R _ + ^2$ and having a uniform probability measure (more correctly, Lebesgue measure on $(\\R_+^2, \\B(\\R_+^2))$). If we let $R_{s,t}$ denote the rectangle with vertices $(s,s), (s,t), (t,t)$ and $(t,s)$, then for each outcome $\\omega \\in \\Omega$, we can think of having $\\text{Poisson}(\\text{Leb}(R_{s,t}(\\omega)))$ distributed number of such points inside the rectangle $R_{s,t}(\\omega).$\nFor $s \u0026lt; t \\in [0, \\infty)$ let $Z_{s,t}$ be the random variable denoting the length of the longest increasing path lying in the rectangle $R_{s,t}$, i.e., $Z_{s,t}$ is the largest integer $k$ for which there are points $(X_1,Y_1), \\dots, (X_k, Y_k)$ in the Poisson process with $s \u0026lt; X_1 \u0026lt; \\cdots \u0026lt; X_k \u0026lt; t$ and $s\u0026lt; Y_1 \u0026lt; \\cdots \u0026lt; Y_k \u0026lt; t$.\nLet $\\tau(n)$ be the smallest value of $t \\in [0, \\infty)$ for which there are $n$ points in $R_{0,t}$. Let the $n$ points in $R_{0, \\tau(n)}$ be written as $\\se{(X_i, Y_i)}_{1 \\leq i \\leq n}$ such that $0 \u0026lt; X_1 \u0026lt; \\cdots \u0026lt; X_n \\leq \\tau(n)$ (the inequalities are strict almost surely since they have continuous distributions). Let $\\pi_n \\in S_n$ be the unique permutation such that $Y_{\\pi_n(1)} \u0026lt; \\cdots \u0026lt; Y_{\\pi_{n}(n)}$ (It is not difficult to see the existence and the uniqueness). Then $$ \\pi_n \\text{ is a uniformly random sample of } S_n \\text{ , and } Z_{0, \\tau(n)} = l_n(\\pi_n) \\tag{6} \\label{eq6} $$ The second claim is obvious from the definition of $\\pi_n$. The first claim is equivalent to showing that if $U_1, \\ldots, U_n$ are independent random variables sampled from a uniform distribution on $[0,1]$ and if $U_{(1)}, \\ldots, U_{(n)}$ are the order statistics, i.e., $U_{(k)}$ is the $k$th smallest among $U_1, \\ldots, U_n$, then the probability that $U_{(1)}, \\ldots, U_{(n)}$ is same as $U_{\\pi(1)}, \\ldots, U_{\\pi(n)}$ for any $\\pi \\in S_n$ is $1/n!$. But this is obvious from the independence of $U_i$'s. The next theorem is from Durrett:\nTheorem 4:  $\\tau(n) / \\sqrt{n} \\to 1$ almost surely.\nProof:  Let $S_n$ be the number of points in $R_{0,\\sqrt{n}}$. Since $(R_{0, \\sqrt{n}} \\setminus R_{0, \\sqrt{n-1}}) \\cap (R_{0, \\sqrt{m}} \\setminus R_{0, \\sqrt{m-1}}) = \\emp$ for $n \\neq m$, the definition of a Poisson random measure implies $\\se{S_n - S_{n-1}}_{n \\geq 1}$ are independent Poisson random variables with mean $1$. The strong law of large numbers now implies $S_n / n \\to 1$ almost surely. For any $\\e \u0026gt; 0$ we can find an $n$ large enough such that $S_{n(1-\\e)} \u0026lt; n \u0026lt; S_{n(1+\\e)}$ but then this means $\\sqrt{n(1-\\e)} \\leq \\tau(n) \\leq \\sqrt{n(1+\\e)}$ which is same as the statement of the theorem since $\\e$ was arbitrary. $\\square$\nThe last theorem along with \\eqref{eq6} implies $Z_{0, \\sqrt{n}} \\to L_n$ almost surely and thus $$ \\frac{Z_{0,n}}{n} \\to \\frac{L_{n^2}}{n} \\text{ almost surely} \\tag{7} \\label{eq7} $$\nBack to longest increasing subsequences Having established the connection between the two ways of looking at the problem of longest increasing subsequences, we now freely jump between the two characterizations and use them to prove our results.\nDefine $W_{s,t} = - Z_{s,t}$. We now check that $W_{m,n}, 0 \\le m \u0026lt; n$ satisfies the conditions required for the Subadditive Ergodic Theorem. I state the theorem below for completeness. Check out Theorem 6.4.1 in Durrett for a proof.\nTheorem 5 [Subadditive Ergodic Theorem]:  Suppose $W_{m,n}, 0 \\le m \u0026lt; n$ satisfy:\n $W_{0,m} + W_{m,n} \\ge W_{0,n}$, $\\se{W_{nk, (n+1)k}, n \\ge 1}$ is a stationary sequence for each $k$, The distribution of $\\se{W_{m,m+k}, k \\ge 1}$ does not depend on $m$, $\\Ex{W_{0,1}^+} \u0026lt; \\infty$ and $\\inf_{n \\ge 1} \\frac{1}{n} \\Ex{W_{0,1}} = \\beta \u0026gt; - \\infty$.  Then\n $\\limn \\frac{1}{n} \\Ex{W_{0,1}} = \\beta$, $W = \\limn \\frac{1}{n} W_{0,1}$ exists almost surely and in $L^1$, and $\\Ex{X} = \\beta$, If all stationary sequences in 2. are ergodic, then $W = \\beta$ almost surely.  Coming back to the problem, let $0 \u0026lt; m \u0026lt; n$ then we claim $Z_{0,m} + Z_{m,n} \\leq Z_{0,n}$. To see this fix $\\omega \\in \\Omega$ and let $Z_{0,m}(\\omega) = a$ and $Z_{m,n}(\\omega) = b$. Then there exist $(X_1(\\omega), Y_1(\\omega)), \\ldots (X_a(\\omega), Y_a(\\omega)), (X_{a+1}(\\omega), Y_{a+1}(\\omega)), \\ldots, (X_{a+b}(\\omega), Y_{a+b}(\\omega))$ such that $0 \u0026lt; X_1(\\omega) \u0026lt; \\cdots \u0026lt; X_a(\\omega) \u0026lt; m$, $0 \u0026lt; Y_1(\\omega) \u0026lt; \\cdots \u0026lt; Y_a(\\omega) \u0026lt; m$ and $m \u0026lt; X_{a+1}(\\omega) \u0026lt; \\cdots \u0026lt; X_{a+b}(\\omega) \u0026lt; n$, $m \u0026lt; Y_{a+1}(\\omega) \u0026lt; \\cdots \u0026lt; Y_{a+b}(\\omega) \u0026lt; n$. But then it\u0026rsquo;s clear that $0 \u0026lt; X_1(\\omega) \u0026lt; \\cdots \u0026lt; X_{a+b}(\\omega) \u0026lt; n$, $0 \u0026lt; Y_1(\\omega) \u0026lt; \\cdots \u0026lt; Y_{a+b}(\\omega) \u0026lt; n$ and we have $Z_{0,m}(\\omega) + Z_{m,n}(\\omega) \\leq Z_{0,n}(\\omega)$. Since $\\omega$ was arbitrary equation our claim is true. Therefore, $W_{0,m} + W_{m,n} \\ge W_{0,n}$ and condition 1. is true.\nFor condition 2. we want to show that $\\se{W_{nk, (n+1)k, n \\geq 1}}$ is a stationary and ergodic sequence for all $k \\geq 1$. This is clear from the observation that $Z_{ik, (i+1)k} \\stackrel{d}{=} Y_{0,k}$ since $\\text{Leb}(R_{ik, (i+1)k}) = \\text{Leb}(R_{0,k})$ and thus by the definition of the Poisson random measure the number of points in each rectangle is an i.i.d. Poisson random variable. Checking the condition 3. is similar to condition 2..\nFor condition 4. note that $W_{0,1}^+ = Z_{0,1}$ and $Z_{0,1} \\leq \\text{Poisson}(1)$ since there are $\\text{Poisson}(1)$ number of points inside $[0,1]^2$ and at most all of them can be arranged in the increasing order. Thus, $\\Ex{W_{0,1}^+} \u0026lt; \\Ex{\\text{Poisson}(1)} = 1 \u0026lt; \\infty$. Now note that since $W_{0,n} = - Z_{0,n}$ $$ \\inf_{n \\geq 1} \\frac{1}{n} \\Ex{W_{0,n}} = - \\sup_{n \\geq 1} \\frac{1}{n} \\Ex{Z_{0,n}} $$ and thus to show the second part of condition 4. we need to show that $\\sup_{n \\geq 1} \\frac{1}{n} \\Ex{Z_{0,n}} \u0026lt; \\infty$. But this is immediate from Equation \\eqref{eq5} in Theorem 3 and Equation \\eqref{eq7}. Therefore, the subadditive ergodic theorem now implies $$ \\frac{Z_{0,n}}{n} \\to \\gamma \\text{ a.s.} $$ where $\\gamma$ from Equations \\eqref{eq3} and \\eqref{eq5} lies in $[1,e]$ and we are done since this implies \\eqref{eq2}.\nEpilogue I got introduced to this problem from an exam question in a math course I took recently. I recommend the book \u0026ldquo;The Surprising Mathematics of Longest Increasing Subsequences\u0026rdquo; by Dan Romik, which is freely available online, for a lot more content.\n","date":1608934628,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608934628,"objectID":"870f9e68745844b26ccf4093462dc6f7","permalink":"https://makkar.github.io/post/lisprob/","publishdate":"2020-12-25T17:17:08-05:00","relpermalink":"/post/lisprob/","section":"post","summary":"A probabilistic analysis","tags":[],"title":"Longest Increasing Subsequence","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\PP}{\\mathbb{P}} \\newcommand{\\eq}[1]{\\begin{align*}#1\\end{align*}} \\newcommand{\\eql}[1]{\\begin{align}#1\\end{align}} \\newcommand{\\ind}[1]{\\mathbf{1}_{#1}} \\newcommand{\\indo}[1]{\\mathbf{1}_{#1}(\\omega)} \\newcommand{\\F}{\\mathcal{F}} \\newcommand{\\probsp}{(\\Omega, \\F, \\PP)} \\newcommand{\\integ}[1]{\\int_{\\Omega} #1 \\dmu} \\newcommand{\\B}{\\mathcal{B}} \\newcommand{\\Bo}{\\B(\\R)} \\newcommand{\\Bon}[1]{\\B(\\R^{#1})} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sumn}{\\sum_{n=1}^{\\infty}} \\newcommand{\\trans}{\\intercal} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\comp}{\\mathsf{c}} \\newcommand{\\emp}{\\varnothing} \\newcommand{\\floor}[1]{\\left \\lfloor{#1}\\right \\rfloor} \\newcommand{\\se}[1]{\\left{ #1 \\right}} \\newcommand{\\set}[2]{\\left{ #1 ; : ; #2 \\right}} \\newcommand{\\sett}[2]{\\left{ #1 ; | ; #2 \\right}} \\newcommand{\\Ex}[1]{\\E\\left[#1\\right]} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dy}{dy} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator{\\dpr}{d\\PP} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\Ord}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} $$\nI recently came across this simple to state puzzle:\n Suppose you have a coin that has probabilities $p$ for heads and $1-p$ for tails. You play the following game with a friend. The first player picks one of the outcomes $HH$, $TH$, $HT$ and $TT$. The second player observes the choice of the first player and picks one of the remaining 3 outcomes. You then proceed to toss the coin infinitely many times independently constructing an infinite sequence of $H$‚Äôs and $T$‚Äôs. A player wins if their choice appears first in the sequence. For example, if player one chose $HH$ and player two chose $TT$ then $HTHTHTTHTHHH\u0026hellip;$ results in a victory for player two since $TT$ occurred before $HH$ in the sequence. When is it better to go first in this game?\n Let\u0026rsquo;s make the conditions under which a player wins more concrete.\nThe strategy:  It‚Äôs better to go as the first player in the game if for some choice $a \\in \\{HH,TT,HT,TH\\}$, the probability of winning the game is greater than $1/2$ no matter the choice of the second player. It‚Äôs better to go as the second player in the game if for every choice of the first player, we can find some element of the set $\\{HH,TT,HT,TH\\} \\setminus \\{a\\}$, where $a$ is the choice of the first player, such that the probability of winning the game is greater than $1/2$.\nAs an example, suppose $p = 3/4$, then if the first player chooses $HH$, then no matter the choice of the second player, the probability of the first player winning the game is greater than $1/2$. To see this note that the probability of getting two consecutive heads in the first two coin tosses itself is $9/16 \u0026gt; 1/2$.\nOn first impression it seems as if it must be better to go as the first player, no matter the value of $p$, since the first player has more choices, but as we will see, the fact that the second player has the advantage of choosing the outcome after observing the choice of the first player gives him an advantage for certain values of $p$.\nBut before we do that, we should prove an implicit assumption: the game ends with a winner in a finite number of moves with probability $1$. But to do that we first need to define a probability space on which the game is played. In particular, does it even exist? Indeed it does as we see below.\nConstructing the probability space To formalize our analysis we need to define a sequence of i.i.d. random variables, one for each coin toss. However, the existence of a probability space on which we can define this sequence is not obvious at all. For example, the following claim shows that we cannot always construct desired number of random variables on a measurable space.\nClaim:  There do not exist uncountably many independent, non-constant random variables on $([0,1], \\B([0,1]), \\lambda)$, where $\\lambda$ is the Lebesgue measure on the Borel $\\sigma$-algebra $\\B([0,1])$.\nProof:  Assume that $\\{X_i \\}_{i \\in I}$ is a collection of independent non-constant random variables. Define the collection $\\{Y_i \\} _{i \\in I}$ by letting $Y_i = X_i \\ind{|X_i| \u0026lt; C_i}$ where $C_i$ is large enough that $Y_i$ isn\u0026rsquo;t a constant. The collection formed by the random variables $Z_i = Y_i - \\Ex{Y_i}$ is a collection of independent random variables. Note that the random variables $Z_i$ are in the separable Hilbert space $L^2([0,1], \\lambda)$ and are orthogonal to each other. But a separable Hilbert can have only countably such elements. Thus, $I$ must be countable. $\\square$\nFortunately the situation isn\u0026rsquo;t so bleak for our case and Kolmogorov extension theorem allows us to claim the existence of a probability space on which there exists our required sequence of i.i.d. random variables if we can show the existence of probability spaces $(\\Omega_n, \\F_n, \\PP_n)$ for $n$ coin tosses that satisfy the consistency conditions. This is very easy: let $\\Omega_n = \\{H, T\\}^n$, $\\F_n = 2^{\\Omega_n}$, $\\PP({\\omega}) = p^{m} (1-p)^{n-m}$ where $m = $ number of $H$ in $\\omega \\in \\Omega_n$, and finally $\\PP(A) = \\sum_{\\omega \\in A} \\PP({\\omega})$ for any $A \\in \\F_n$. It is easy to see that this construction satisfies the consistency conditions and thus there exists a unique probability measure $\\PP$ defined on the measurable space $(\\Omega, \\F)$ where $\\Omega = \\{H, T\\}^{\\N}$ and $\\F$ is the $\\sigma$-algebra generated by the cylinder sets.\nThe game has a winner with probability $1$ We can now safely say the following statement: Let $\\{X_n\\}$ be a sequence of i.i.d. Bernoulli random variables such that $X_n = H$ or $T$ depending on the result of the $n$th coin toss. To show that the game has a winner with probability $1$ we need to prove that in the sequence $X_1, X_2, \\ldots$ all of the four choices $HH$, $TH$, $HT$ and $TT$ appear in a finite number of coin tosses with probability $1$.\nTo that end, fix any of the four choices $HH$, $TH$, $HT$ and $TT$, and call it $XY$. Let $E_n$ be the event that $X_{2n-1} = X$ and $X_{2n} = Y$. Then $\\PP(E_n) = \\e \u0026gt; 0$ independent of $n$, where $\\e$ is some positive real number dependent on $XY$ (for example, if $XY = HT$ then $\\e = p(1-p)$). Now the events $\\{E_n\\}$ are independent and $\\sum_{n=1}^\\infty \\PP(E_n) = \\infty$, and thus we can apply the second Borel-Cantelli lemma to get $\\PP(E_n \\text{ i.o.}) = 1$. But this immediately implies that each of the four outcomes appear in that sequence infinitely many times with probability $1$.\nBack to the game Recall the strategy outlined above. Without loss of generality we may assume $p \\geq 1/2$ because otherwise we can just flip the tags $H$ and $T$. From a first player perspective we just want to find one of the choices $HH$, $TH$, $HT$ or $TT$. Let\u0026rsquo;s calculate the minimum $p$ we get for each choice.\nIf player $1$ chooses $HH$ and player $2$ chooses $TH$, then it\u0026rsquo;s better to be player $1$ if $p \u0026gt; 1/\\sqrt{2}$. Checking for other choices of player $2$ we see that $TH$ is the optimal choice.\nIf player $1$ chooses $TT$ and player $2$ chooses $HT$, then for no $p \\geq 1/2$ it is better to be player $1$.\nIf player $1$ chooses $HT$ and player $2$ chooses $HH$, then for no $p \\geq 1/2$ it is better to be player $1$.\nIf player $1$ chooses $TH$ and player $2$ chooses $HT$, then for no $p \\geq 1/2$ it is better to be player $1$.\nOverall it is better to be player $1$ if $p \u0026gt; 1/\\sqrt{2}$ or if $p \u0026lt; 1 - 1/\\sqrt{2}$. And on the other hand it is better to be player $2$ if $1 - 1/\\sqrt{2} \u0026lt; p \u0026lt; 1/\\sqrt{2}$.\n","date":1602449945,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602449945,"objectID":"0b70a585f13f500ad0fdb86e23aa126b","permalink":"https://makkar.github.io/post/infcoingame/","publishdate":"2020-10-11T16:59:05-04:00","relpermalink":"/post/infcoingame/","section":"post","summary":"A discussion of a counter-intuitive game played with coin tosses","tags":[],"title":"A coin tossing game","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\P}{\\mathcal{P}} \\newcommand{\\CC}{\\mathcal{C}} \\newcommand{\\M}{\\mathcal{M}} \\newcommand{\\NN}{\\mathcal{N}} \\newcommand{\\L}{\\mathcal{L}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\trans}{\\intercal} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator{\\dom}{d\\omega} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} \\DeclareMathOperator{\\O}{\\mathcal{O}} \\DeclareMathOperator{\\E}{\\mathbb{E}} $$\nIntroduction We discussed some functional analysis and the basics of Reproducing Kernel Hilbert Space theory in the previous two articles. The aim of this article is to discuss the need for approximating the kernel matrix and one method in particular to do that, Random Fourier Features.\nLet\u0026rsquo;s start by discussing Kernel Ridge Regression, a simple application of kernel methods.\nKernel Ridge Regression Ridge Regression Let\u0026rsquo;s recall the ridge regression model. We have some training data of the form:\n$$ (x_1, y_1), \\ldots, (x_n, y_n) \\in (\\X, \\Y) $$ where $\\X = \\R^d$ and $\\Y = \\R$. We represent the training data more succinctly by letting $\\matr{y} = [y_1, \\ldots, y_n]^\\trans$ and $\\matr{X}$ be an $n \\times d$ matrix whose $i$th row is $x_i^\\trans$. Then in ridge regression the likelihood model is\n$$ \\matr{y} \\sim \\NN(\\matr{X} \\matr{w}, \\sigma^2 \\matr{I}_n) $$ where $\\matr{w}$ is a $d$-dimensional column vector representing the weights to learned, and $\\sigma^2 \u0026gt; 0$. We also assume a Gaussian prior on $\\matr{w}$\n$$ \\matr{w} \\sim \\NN\\left(0, \\frac{1}{\\lambda} \\matr{I}_d\\right) $$\nThen the maximum a posteriori (MAP) estimate is given by\n\\begin{align} \\matr{w}_{\\text{MAP}} \u0026amp;= \\argmax _{\\matr{w}} \\; \\ln p(\\matr{w} | \\matr{y}, \\matr{X}) \\\\ \u0026amp;= \\argmax _{\\matr{w}} \\; \\ln p(\\matr{y} | \\matr{w}, \\matr{X}) + \\ln p(\\matr{w}) \\\\ \u0026amp;= \\argmax _{\\matr{w}} \\; \\underbrace{-\\frac{1}{2\\sigma^2} (\\matr{y} - \\matr{X} \\matr{w})^\\trans (\\matr{y} - \\matr{X} \\matr{w}) - \\frac{\\lambda}{2} \\matr{w}^\\trans \\matr{w}} _{\\L} \\end{align}\nIf we call this objective $\\L$, then $\\matr{w}_{\\text{MAP}}$ is given by the solution of the equation formed by equating the gradient of $\\L$ with respect to $\\matr{w}$ to $0$. Thus,\n$$ 0 = \\nabla_{\\matr{w}} \\L = \\frac{1}{\\sigma^2} \\matr{X}^\\trans \\matr{y} - \\frac{1}{\\sigma^2} \\matr{X}^\\trans \\matr{X} \\matr{w} - \\lambda \\matr{w} $$\nand we get\n$$ \\matr{w}_{\\text{MAP}} = (\\lambda \\sigma^2 \\matr{I} + \\matr{X}^\\trans \\matr{X})^{-1} \\matr{X}^\\trans \\matr{y} $$\nwhich is called the ridge regression solution, and we denote it by $\\matr{w}_{\\text{RR}}$. We are inverting a $d \\times d$ matrix taking $\\O(d^3)$ time, and matrix multiplication of a $d \\times d$ matrix with a $d \\times n$ matrix taking $\\O(d^2 n)$ time.\nKernel Ridge Regression The previous model suffers from limited expressiveness. As we have seen before, the idea of kernel methods is to define a map which takes our inputs from $\\X$ to an RKHS $\\H$:\n$$ \\phi : \\X \\to \\H $$\nand then apply the linear model of ridge regression above to $\\phi(x_i)$ instead of $x_i$. $\\H$ could be an infinite-dimensional vector space, but for now suppose it is $D$-dimensional. Let $\\matr{\\Phi}$ represent the $n \\times D$ matrix whose $i$th row is $\\phi(x_i)^\\trans$. Then the ridge regression solution is given by\n\\begin{equation} \\matr{w}_{\\text{RR}} = (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans \\matr{y} \\end{equation}\nWe immediately realize the problem here: we are inverting a $D \\times D$ matrix and $D$ can be huge! Fortunately, we have a matrix trick that we can exploit, the push-though identity. It looks like this\n\\begin{align} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) \u0026amp;= (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi}) \\matr{\\Phi}^\\trans \\quad \\text{(can be seen by expanding)} \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) \u0026amp;= (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi}) \\matr{\\Phi}^\\trans \\quad \\text{(left multiplying by } (\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\text{ )} \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) \u0026amp;= \\matr{\\Phi}^\\trans \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans) (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \u0026amp;= \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\quad \\text{(right multiplying by } (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\text{ )} \\\\\n(\\lambda \\sigma^2 \\matr{I}_D + \\matr{\\Phi}^\\trans \\matr{\\Phi})^{-1} \\matr{\\Phi}^\\trans \u0026amp;= \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\end{align}\nAnd thus we get $$ \\matr{w}_{\\text{RR}} = \\matr{\\Phi}^\\trans (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\matr{y} $$\nWe are now inverting an $n \\times n$ matrix taking $\\O(n^3)$ time, and matrix multiplication of a $D \\times n$ matrix with a $n \\times n$ matrix taking $\\O(D n^2)$ time.\nNotice that $\\matr{\\Phi} \\matr{\\Phi}^\\trans$ is the gram matrix, and let\u0026rsquo;s denote it by $\\matr{K}$. It\u0026rsquo;s $(i,j)$th entry is $\\matr{K}_{i,j} = \\inner{\\phi(x_i)}{\\phi(x_j)} = K(x_i, x_j)$, where $K : \\R^d \\times \\R^d \\to \\C$ is the kernel function corresponding to the RKHS $\\H$.\nIf we denote\n$$ \\matr{\\alpha} = (\\lambda \\sigma^2 \\matr{I}_n + \\matr{\\Phi} \\matr{\\Phi}^\\trans)^{-1} \\matr{y} = (\\lambda \\sigma^2 \\matr{I}_n + \\matr{K})^{-1} \\matr{y} $$\nthen we have\n$$ \\matr{w}_{\\text{RR}} = \\matr{\\Phi}^\\trans \\matr{\\alpha} = \\sum _{i=1}^n \\alpha_i \\phi(x_i) $$\nand prediction for a test data point, $x$, is\n$$ \\matr{w}_{\\text{RR}}^\\trans \\phi(x) = \\sum _{i=1}^n \\alpha_i \\phi(x_i)^\\trans \\phi(x) = \\sum _{i=1}^n \\alpha_i k _{x_i}(x) $$\nwhere $k_{x_i}$ denotes the reproducing kernel for the point $x_i$.\nIf this looks suspiciously similar to the Representer theorem (Theorem-8), then that\u0026rsquo;s because it is an instance of it! So prediction takes $\\O(n)$ time.\nThe point of this discussion was to show that Kernel Ridge Regression is computationally expensive. Simply storing the gram matrix, $\\matr{K}$, takes $\\O(n^2)$ space. We need to find methods which can circumvent using $\\matr{K}$ if we are to apply kernel methods to anything other than the smallest of the data sets.\nKernel approximation The idea behind kernel approximation methods is to replace the gram matrix, $\\matr{K}$, with a low rank approximation, i.e., find an $n \\times S$ matrix, $\\matr{Z}$, such that\n$$ \\matr{K} \\approx \\matr{Z} \\matr{Z}^\\trans $$\nWe want $S \\ll n$, and thus we significantly reduce our time and space complexity. $\\matr{Z}$ takes $\\O(nS)$ space. Inversion of $(\\lambda \\sigma^2 \\matr{I}_n + \\matr{Z} \\matr{Z}^\\trans)$ takes $\\O(n S^2)$ time.\nOne way to think about this is we want to find a mapping of $x_i$'s to some latent space such that the inner product in this latent space approximates the kernel\n$$ \\matr{K}_{i,j} = K(x_i, x_j) \\approx z_i^\\trans z_j $$\nwhere $z_i$ denotes the $i$th row of $\\matr{Z}$. One way to do that is Nystr√∂m approximation, which I won\u0026rsquo;t be discussing at all. The other method is to use Random Fourier Features. This approach was introduced by Rahimi and Recht in their seminal 2007 paper Random Features for Large-Scale Kernel Machines. It relies on a result from functional analysis called Bochner\u0026rsquo;s theorem, so let\u0026rsquo;s digress and discuss that first.\nBochner\u0026rsquo;s theorem Definition 28: A function $K : \\R^n \\to \\C$, is said to be positive-definite if\n$$ \\sum _{i,j = 1}^n \\alpha_i \\conj{\\alpha_j} K(x_i - x_j) \\geq 0 $$\nfor every choice of $x_1, \\ldots, x_n \\in \\R^n$ and for every choice of complex numbers $\\alpha_1, \\ldots, \\alpha_n$.\nNotice how similar this definition is to the definition of a kernel function. Positive-definite functions have some nice properties:\nLemma 3: If $K: \\R^n \\to \\C$ is a positive-definite function, then $K(-x) = \\conj{K(x)}$ for every $x \\in \\R^n$.\nProof: It is easy to see that $K(0) \\geq 0$. In the definition above, let $n = 2$, $x_1 = x$, $x_2 = 0$, $\\alpha_1$ be an arbitrary complex number, and $\\alpha_2 = 1$. Then applying the definition of a positive-definite function, we get\n$$ (1 + |\\alpha_1|^2)K(0) + \\alpha_1 K(x) + \\conj{\\alpha_1}K(-x) \\geq 0 $$\nLet $\\alpha_1 = 1$, then $$ 2K(0) + K(x) + K(-x) \\geq 0 $$\nIn particular, $K(x) + K(-x)$ is real, which implies\n$$ K(x) + K(-x) = \\conj{K(x)} +\\conj{K(-x)} $$\nSimilarly, letting $\\alpha_1 = i$, we get $i(K(x) - K(-x))$ is real, which implies\n$$ K(x) - K(-x) = -\\conj{K(x)} + \\conj{K(-x)} $$\nAdding these two equations, we get $K(-x) = \\conj{K(x)}$. $\\square$\nLemma 4: If $K: \\R^n \\to \\C$ is a positive-definite function, then $K$ is bounded. In particular, $|K(x)| \\leq K(0)$ for every $x \\in \\R^n$.\nProof: From lemma-3, $K(0)$ must be real. In the definition above, let $n = 2$, $x_1 = 0$, $x_2 = x$, $\\alpha_1 = |K(x)|$, and $\\alpha_2 = -\\conj{K(x)}$. Then applying the definition of a positive-definite function, we get\n$$ 2K(0) |K(x)|^2 - |K(x)| K(x) K(-x) - \\conj{K(x)} |K(x)| K(x) \\geq 0 $$\nNow use lemma-3 to substitute $\\conj{K(x)}$ for $K(-x)$ in the middle term to get\n$$ 2K(0) |K(x)|^2 - 2 |K(x)|^3 \\geq 0 $$\nIf $|K(x)| = 0$, then we obviously have our result, since we can easily show $K(0) \\geq 0$, otherwise we can divide by $2|K(x)|^2$ to get\n$$ K(0) - |K(x)| \\geq 0 $$\nwhich is our desired result. $\\square$\nThe following theorem is the converse of Bochner\u0026rsquo;s theorem. We state it first since it is easier to prove.\nTheorem 9 [Converse of Bochner\u0026rsquo;s theorem]: The Fourier transform of every finite Borel measure on $\\R^n$ is positive-definite.\nProof: Let $\\mu$ be a finite Borel measure on $\\R^n$. Let $K : \\R^n \\to \\C$ be the Fourier transform of $\\mu$, i.e.,\n$$ K(x) = \\int_{\\R^n} \\exp(-i \\omega^\\trans x) \\dmu(\\omega) $$\nLet $x_1, \\ldots, x_n \\in \\R^n$ and $\\alpha_1, \\ldots, \\alpha_n \\in \\C$ be arbitrary. Then\n\\begin{align} \\sum _{i,j = 1}^n \\alpha_i \\conj{\\alpha_j} K(x_i - x_j) \u0026amp;= \\sum _{i,j = 1}^n \\alpha_i \\conj{\\alpha_j} \\int _{\\R^n} \\exp\\{-i \\omega^\\trans (x_i - x_j)\\} \\dmu(\\omega) \\\\\n\u0026amp;= \\int _{\\R^n} \\left| \\sum _{i = 1}^n \\alpha_i \\exp(-i \\omega^\\trans x_i) \\right|^2 \\dmu(\\omega) \\\\\n\u0026amp;\\geq 0 \\end{align}\nThus, $K$ is positive-definite. $\\square$\nTheorem 10 [Bochner\u0026rsquo;s theorem]: If $K$ is continuous and positive-definite, then $K$ is the Fourier transform of a finite positive Borel measure.\nI\u0026rsquo;ll skip the proof since it is beyond my understanding. The proof can be easily found on the internet, see here or here for example.\nThis finite positive Borel measure is often called as spectral measure. It is easy to see that $K(0) = \\mu(\\R^n)$, and thus if we assume $K(0) = 1$ then the spectral measure is a probability measure.\nRandom Fourier Features Picking up where we left: We want to find a low-dimensional mapping of $x_i$'s into a latent space such that we can approximate the kernel computation with an inner product in this latent space. We will use Bochner\u0026rsquo;s theorem for this. To apply this theorem we need to limit ourselves to a special class of kernels.\nDefinition 29: A kernel function $K : \\R^d \\times \\R^d \\to \\C$ is called shift-invariant if $K(x, y) = K(x-y, 0)$.\nGaussian and Laplacian kernels are examples of shift-invariant kernels. Note that the function $K\u0026rsquo;: \\R^d \\to \\C$ defined by $K\u0026rsquo;(x) = K(x, 0)$ is positive-definite if $K$ is a shift-invariant kernel. Bochner\u0026rsquo;s theorem then implies the existence of a spectral measure, $\\mu$, such that\n$$ K(x, y) = K(x-y, 0) = K\u0026rsquo;(x-y) = \\int _{\\R^d} \\exp\\{-i\\omega^\\trans (x-y)\\} \\dmu(\\omega) $$\nLet $p$ denote the Radon-Nikodym derivative of $\\mu$ with respect to the Lebesgue measure on $\\R^d$, then we can write the equation above as\n\\begin{equation} K(x, y) = \\int _{\\R^d} \\exp\\{-i\\omega^\\trans (x-y)\\} p(\\omega) \\dom \\tag{$\\star$} \\end{equation}\nFor example, if we take the Gaussian kernel\n$$ K(x,y) = \\exp\\left(-\\frac{\\norm{x-y}^2_2}{2 \\sigma^2}\\right) $$\nthen since $K(0,0) = 1$, $p$ is a probability density, and can be easily shown to be Gaussian\n$$ p \\sim \\NN\\left(0, \\frac{1}{\\sigma^2} \\matr{I}_d\\right) $$\nWe now see an obvious way to approximate the kernel from equation $(\\star)$: use Monte Carlo approximation. If we take $S$ samples, $\\omega_1, \\ldots, \\omega_S \\sim p(\\omega)$,\n\\begin{align} K(x, y) \u0026amp;= \\int _{\\R^d} \\exp\\{-i\\omega^\\trans (x-y)\\} p(\\omega) \\dom \\\\\n\u0026amp;\\approx \\frac{1}{S} \\sum _{s=1}^S \\exp\\{-i\\omega^\\trans_s (x-y)\\} \\\\\n\u0026amp;= \\inner{z(x)}{z(y)} \\end{align}\nwhere $z : \\R^d \\to \\R^S$ is the latent mapping given by\n$$ z(x) = \\frac{1}{\\sqrt{S}}[\\exp(-i \\omega ^\\trans _1 x), \\ldots, \\exp(-i \\omega ^\\trans _S x)]^\\trans $$\nWe can get another mapping by noting that if the kernel is real, then from $(\\star)$ we can see that the RHS must also be real and we can thus write it as\n\\begin{equation} K(x, y) = \\int _{\\R^d} \\cos\\{\\omega^\\trans (x-y)\\} p(\\omega) \\dom \\end{equation}\nUsing the fact that $\\cos(a - b) = \\cos(a) \\cos(b) + \\sin(a) \\sin(b)$, we can write $\\cos\\{\\omega^\\trans (x-y)\\} = \\cos(\\omega^\\trans x) \\cos(\\omega^\\trans y) + \\sin(\\omega^\\trans x) \\sin(\\omega^\\trans y)$\nThus, if we define $z_1 : \\R^d \\to \\R^{2S}$ by\n$$ z_1(x) = \\frac{1}{\\sqrt{S}}[\\cos(\\omega^\\trans_1 x), \\ldots, \\cos(\\omega^\\trans_S x), \\sin(\\omega^\\trans_1 x), \\ldots, \\sin(\\omega^\\trans_S x)]^\\trans $$\nwe have $\\inner{z_1(x)}{z_1(y)} \\approx K(x, y)$.\nAnother mapping that is used is $z_2 : \\R^d \\to \\R^{S}$ defined by\n$$ z_2(x) = \\sqrt{\\frac{2}{S}}[\\cos(\\omega^\\trans_1 x + b_1), \\ldots, \\cos(\\omega^\\trans_S x + b_S)]^\\trans $$\nwhere $b_1, \\ldots, b_S \\sim \\text{Unif}[0, 2\\pi]$. It is not too difficult to show that\n$$ \\E_{\\omega, b}[z_2(x)^\\trans z_2(y)] = \\E_{\\omega}[z_1(x)^\\trans z_1(y)] $$\nThere exist many modifications to the basic RFF approach described here. For example, we can do Quasi-Monte Carlo approximations instead of Monte Carlo approximations. See the paper Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels by Avron, Sindhwani, Yang and Mahoney (2016). Or we could sample from a modified distribution in Fourier space, given by the leverage function of the kernel. See the paper Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees by Avron et al (2017).\nExtension to a more general class of kernels I have often seen kernels being represented as an integral of a product of two functions neatly separating the dependence on $x$ and $y$:\n$$ K(x,y) = \\int_{\\Omega} \\varphi(\\omega, x) \\varphi(\\omega, y) \\dmu(\\omega) $$\nfor some function $\\varphi(\\cdot, \\cdot)$ and measure $\\mu$ on $\\Omega$. I found a good explanation of when this is possible in the papers by Bach (2017) Breaking the Curse of Dimensionality with Convex Neural Networks and Li et al (2019) Towards a Unified Analysis of Random Fourier Features.\nLet $\\mu$ be a Borel probability measure on the compact space $\\Omega$, and $\\varphi: \\Omega \\times \\X \\to \\R$ be a function such that the functions $\\varphi(\\cdot, x) : \\Omega \\to \\R$ are measurable for all $x \\in \\X$, i.e., they are random variables. Now define the set $\\H$ to consist of all functions $f$ that can written as\n$$ f(x) = \\int_{\\Omega} h(\\omega) \\varphi(\\omega, x) \\dmu(\\omega) \\quad \\text{for all } x \\in \\X $$\nfor some $h: \\Omega \\to \\R$ such that $\\int_{\\Omega} h^2 \\dmu \u0026lt; \\infty$. Let us define the squared norm, $\\norm{f}_{\\H}^2$, as the infimum of $\\int_{\\Omega} h^2 \\dmu$ over all functions $h$ for which $f$ can be decomposed as above. Then it can be shown that $\\H$ is an RKHS with the kernel\n$$ K(x,y) = \\int_{\\Omega} \\varphi(\\omega, x) \\varphi(\\omega, y) \\dmu(\\omega) $$\nTherefore, we can again approximate this kernel with a Monte Carlo approximation:\n\\begin{align} K(x,y) \u0026amp;= \\int_{\\Omega} \\varphi(\\omega, x) \\varphi(\\omega, y) \\dmu(\\omega) \\\\\n\u0026amp;\\approx \\frac{1}{S} \\sum_{s=1}^S \\varphi(\\omega_s, x) \\varphi(\\omega_s, y) \\end{align}\nEpilogue Random Fourier Features is an easy to implement approach that allows us to apply kernel methods on large data sets. I haven\u0026rsquo;t discussed the number of samples, $S$, needed to approximate the kernel function within an error bound. You can find such results in the papers linked above.\nWith this article I conclude the series on kernels. I had initially set out to write a short piece on Random Fourier Features but the subject of kernel theory is vast and beautiful, and that short piece metastasised to three articles.\n","date":1599509135,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599509135,"objectID":"25a7b6390f505311277bb15ca56943dd","permalink":"https://makkar.github.io/post/kernels2/","publishdate":"2020-09-07T16:05:35-04:00","relpermalink":"/post/kernels2/","section":"post","summary":"Random Fourier Features","tags":[],"title":"Kernels - Part 2","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\K}{\\mathbb{K}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\P}{\\mathscr{P}} \\newcommand{\\cP}{\\mathcal{P}} \\newcommand{\\Ch}{\\mathscr{C}} \\newcommand{\\M}{\\mathcal{M}} \\newcommand{\\U}{\\mathcal{U}} \\newcommand{\\S}{\\mathcal{S}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\newcommand{\\se}[1]{\\left\\{ #1 \\right\\}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} $$\nIntroduction I have been self-studying some functional analysis recently, and Zorn\u0026rsquo;s lemma comes up often in proofs (for example, showing that every nontrivial vector space has a Hamel basis (I show this fundamental result below) or in the proof of Hahn-Banach theorem). Since I have no formal background in mathematics (my undergrad was in Mechanical engineering), it was my first time hearing about Zorn\u0026rsquo;s lemma. I read its statement from the Wikipedia article, naively assuming I understood this extremely powerful tool. But every usage of Zorn\u0026rsquo;s lemma seemed contrived to me, and it was only in retrospect that I could see how Zorn\u0026rsquo;s lemma seems like an obvious tool to apply. An excellent article which helped me understand Zorn\u0026rsquo;s lemma is How to use Zorn\u0026rsquo;s lemma by Timothy Gowers. If you didn\u0026rsquo;t know about Gowers\u0026rsquo;s article, then mentioning it is the biggest contribution of my article and I recommend you read it.\nIn this article, I want to show how to use Zorn\u0026rsquo;s lemma by stating a theorem and discussing its proof. Things \u0026ldquo;clicked\u0026rdquo; for me when I proved that theorem. But before we do that let me define some important concepts.\nPreliminaries Recall the concept of relation I defined here.\nDefinition 1:  Let $P$ be a nonempty set. A partial order relation in $P$ is a relation which is symbolized by $\\preceq$ and that satisfies the following properties for all $x,y,z \\in P$:\n Reflexivity: $x \\preceq x$; Antisymmetry: $x \\preceq y$ and $y \\preceq x$ implies $x = y$; Transitivity: $x \\preceq y$ and $y \\preceq z$ implies $x \\preceq z$.  The set $P$ is then called a partially ordered set. If in addition to these three properties, the set $P$ also satisfies\nany two elements are comparable, i.e., either $x \\preceq y$ or $y \\preceq x$.  then the relation is called a total order relation and the set $P$ is called a totally ordered set or a chain. We often write $x \\prec y$ to mean $x \\preceq y$ but $x \\neq y$.\nExamples  Let $P$ be the set of all positive integers, and let $m \\preceq n$ mean that $m$ divides $n$. Then $P$ is a partially ordered set. It is not a chain as $2$ and $3$ are not comparable, for example. Let $P$ be the set of all real numbers, and let $x \\preceq y$ mean that $y-x$ is nonnegative. Then $P$ is a chain. Note that with this choice of ordering we have our usual ordering of real numbers, which we denote by $x \\leq y$. Let $P$ be the class of all subsets of some universal set $U$, and let $A \\preceq B$ for $A,B \\in P$ mean that $A \\subseteq B$. Then $P$ is a partially ordered set. It is not a chain because if $U$ contains at least two elements, then we can find two subsets of $U$ neither of which is a subset of the other.  Definition 2:  If $P$ is a partially ordered set, an element $x \\in P$ is said to be maximal if for any $y \\in P$ for which $x \\preceq y$, we must have $x = y$.\nNote that a maximal element does not have to be bigger than everything else: it just must not be smaller than anything else. A maximal element may not exist, and if it exists it may not be unique. Examples 1 and 2 above have no maximal elements. Example 3 has one maximal element: $U$.\nDefinition 3:  Let $Q$ be a nonempty subset of a partially ordered set $P$. An element $x \\in P$ is called an upper bound of $Q$ if $y \\preceq x$ for every $y \\in Q$. An upper bound of $Q$ is called a least upper bound of $Q$ if it is less than or equal to every upper bound of $Q$.\nSimilarly for lower bound and greatest lower bound.\nNote how similar these definitions are to the definitions of supremum and infimum for real numbers. That\u0026rsquo;s because in that case they are the same. In Example 1 above, if we take $Q$ to be any finite subset of $P$, then the greatest lower bound is the greatest common divisor of all the elements of $Q$ and the least upper bound is the least common multiple. In Example 3 above, let $Q$ be any nonempty subset of $P$. Then the least upper bound is the union of all the sets in $Q$, and the greatest lower bound is the intersection of all the sets in $Q$.\nWe are now ready to state the Zorn\u0026rsquo;s lemma.\nZorn\u0026rsquo;s lemma Zorn\u0026rsquo;s lemma:  If $P$ is a partially ordered set in which every chain has an upper bound, then $P$ possesses a maximal element.\nI\u0026rsquo;ll mention in passing that Zorn\u0026rsquo;s lemma is equivalent to axiom of choice.\nExample application 1 Theorem 1:  Any infinite set $X$ can be represented as a union of a disjoint class of countably infinite subsets.\nProof:  The theorem is trivial if $X$ is countably infinite, so assume that it is uncountably infinite.\nThe first thing that we need to do is realize Zorn\u0026rsquo;s lemma can be applied here. I use the following description taken from Gowers\u0026rsquo;s article as a guiding principle:\n If you are building a mathematical object in stages and find that (i) you have not finished even after infinitely many stages, and (ii) there seems to be nothing to stop you continuing to build, then Zorn‚Äôs lemma may well be able to help you.\n The fact that we need to build $X$ by taking a union of some sets, and it could be an uncountable union and therefore we might not finish even in countably infinite number of stages, suggests Zorn\u0026rsquo;s lemma might be helpful here. Zorn\u0026rsquo;s lemma will prove the existence of a maximal element in a partially ordered set, therefore, we want to construct a partially ordered set such that we can prove that its maximal element constructs $X$.\nWe want to build our partially ordered set to be consisting of elements of this form: disjoint class of countably infinite subsets of $X$. The reason we want to define our partially ordered set this way is because, first, the subset operation makes it a partially ordered set, and second, it seems intuitively true that the maximal element of this partially ordered set, if it exists, will be such that the union of its elements is $X$.\nTo this end, let $\\P$ be the set of all disjoint classes of countably infinite subsets of $X$. $\\P$ is partially ordered by $\\subseteq$ as we saw in Example 3 above.\nTo be able to apply Zorn\u0026rsquo;s lemma we need to show that every chain in $\\P$ has an upper bound. Let $\\Ch$ be a chain in $\\P$. A natural guess for the upper bound of $\\Ch$ is $\\U = \\bigcup _{\\S \\in \\Ch} \\S$. To show that $\\U$ is an upper bound of $\\Ch$ we need to show that $\\U \\in \\P$ and $\\S \\subseteq \\U$ for every $\\S \\in \\Ch$. The second claim is trivial from $\\U$'s definition. It is easy to see that $\\U$ is a class of countably infinite subsets of $X$ because this is true for each $\\S \\in \\Ch$. To show that $\\U$ is a disjoint class, let $A, B \\in \\U$ be distinct elements of $\\U$. There exist elements $\\S_A, \\S_B \\in \\Ch$ such that $A \\in \\S_A$ and $B \\in \\S_B$. Since $\\Ch$ is a chain, either $\\S_A \\subseteq \\S_B$ or $\\S_B \\subseteq \\S_A$. Without loss of generality, $\\S_A \\subseteq \\S_B$. Then $A, B \\in \\S_B$ and this implies $A$ and $B$ are disjoint because this is true for the elements of our chain. Thus $\\U$ is a disjoint class of countably infinite subsets and hence belongs to $\\P$.\nBy the Zorn\u0026rsquo;s lemma, $\\P$ possesses a maximal element, $\\M$. If $\\bigcup _{E \\in \\M} E = X$ then we are done because $\\M$ is the required disjoint class of countably infinite subsets whose union is $X$. But it can still happen that some elements of $X$ are not present in this union. In this case we can show that these leftover elements form a finite set and we can get our required disjoint class by adding these leftover elements to any element of $\\M$. To see that the leftover elements must be finite, denote the set of leftover elements by $Y = X \\setminus \\bigcup _{E \\in \\M} E$. If $Y$ is infinite it must contain a countably infinite subset $Z \\subseteq Y$. Consider the class $\\M \\cup \\{Z\\}$. It is an element of $\\P$ because it is a disjoint class of countably infinite subsets of $X$. But $\\M$ is a strict subset of $\\M \\cup \\{Z\\}$ which contradicts the fact that $\\M$ is a maximal element of $\\P$.\nWe are done! $\\square$\nExample application 2 Let us end by showing a fundamental theorem in functional analysis.\nDefinition 4:  Let $X$ be a nontrivial vector space (i.e., $X \\neq \\se{0}$) over the field $\\K$. Then a Hamel basis of $X$ is any family $\\se{e_i}_{i \\in I}$ of vectors $e_i \\in X$ that satisfy\n The family is linearly independent, i.e., given any finite subfamily $\\se{e_j}_ {j \\in J}$ of $\\se{e_i}_{i \\in I}$ and any scalars $\\se{\\alpha_j} _{j \\in J} \\subseteq \\K$ such that $\\sum_{j \\in J} \\alpha_j e_j = 0$, then $\\alpha_j = 0$, $j \\in J$. $\\text{Span}\\left(\\se{e_i} _{i \\in I}\\right) = X$, i.e., given any vector $x \\in X$, there exists a finite subfamily $\\se{e_j} _ {j \\in J}$ of $\\se{e_i} _ {i \\in I}$ and there exist scalars $\\se{x_j} _ {j \\in J} \\subseteq \\K$, such that $x = \\sum _ {j \\in J} x_j e_j.$  Theorem 2:  Let $X$ be a nontrivial vector space, then there exists a Hamel basis of $X$.\nProof:  Let $\\cP$ denote the set formed by all linearly independent families of vectors of $X$. Hence, $\\cP$ is nonempty, since $\\cP$ contains $\\se{x}$, where $x$ is any nonzero vector of $X$. We define a partial order on the elements of $\\cP$ as follows: if $E = \\se{e_i} _ {i \\in I}$ and $F = \\se{e_j} _ {j \\in J}$ are any two elements of $\\cP$, then $E \\preceq F$ iff $E \\subseteq F$.\nThe next step is to show that if $\\mathcal{C}$ is any chain in $\\cP$, then $\\mathcal{C}$ has an upper bound in $\\cP$. To this end, define $U = \\bigcup_{S \\in \\mathcal{C}} S$. We claim that this is the desired upper bound. $U$ is in $\\cP$ because any finite subfamily $\\se{e_i} _ {i=1}^n$ of $U$ is a subfamily of some $S \\in \\mathcal{C}$ since $\\mathcal{C}$ is a chain, and therefore the vectors $\\se{e_i} _ {i=1}^n$ are linearly independent. Also, $U$ is clearly an upper bound of $\\mathcal{C}$, since $S \\subseteq U$ for all $S \\in \\mathcal{C}$.\nBy the Zorn\u0026rsquo;s lemma, $\\cP$ possesses a maximal element, $M$, which we claim to be a Hamel basis of $X$. For otherwise, there would exist a nonzero vector $x \\in X$ that cannot be written as a linear combination of elements of $M$. But then $M \\cup \\se{x}$ would be an element of $\\cP$ that satisfies $M \\prec M \\cup \\se{x}$, a contradiction. $\\square$\n","date":1596841152,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596841152,"objectID":"8a22e14e2982adf5a8ada11e4fae9c55","permalink":"https://makkar.github.io/post/zorn/","publishdate":"2020-08-07T18:59:12-04:00","relpermalink":"/post/zorn/","section":"post","summary":"Examples on how to use Zorn's lemma","tags":[],"title":"Zorn's lemma","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\Y}{\\mathcal{Y}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\modu}[1]{\\lvert#1\\rvert} \\newcommand{\\conj}[1]{\\overline{#1}} \\newcommand{\\matr}[1]{\\mathbf{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} \\DeclareMathOperator*{\\argmax}{arg\\,max} \\DeclareMathOperator*{\\argmin}{arg\\,min} $$\nIntroduction This is the second blog post in the series of blog posts on kernels. In the first part I introduced the functional analysis background for kernel theory. I highly recommend you read it before continuing. I will frequently refer to it and use the same notation. In this blog post I aim to introduce the fundamental theorems like Mercer\u0026rsquo;s theorem and Representer theorem.\nCharacterization of reproducing kernels We already defined the notion of reproducing kernels for an RKHS (Definition 20). We now turn our attention to obtaining necessary and sufficient conditions for a function $K : \\X \\times \\X \\to \\C$ to be the reproducing kernel for some RKHS. But first we must define kernel functions.\nKernel functions Let\u0026rsquo;s start by recalling a basic definition.\nDefinition 21: Let $A = (a_{i,j})$ be an $n \\times n$ complex matrix. Then $A$ is called positive if for every $\\alpha_1, \\ldots, \\alpha_n \\in \\C$ we have $$ \\sum_{i,j = 1}^{n} \\conj{\\alpha_i}\\alpha_j a_{i,j} \\geq 0 $$ We denote this by $A \\geq 0$.\nNote that if we define a vector $x \\in \\C^n$ to be such that its $i$th component is $\\alpha_i$, then the condition above can rewritten as $$ \\inner{Ax}{x} \\geq 0 $$\nAlso note that if $A \\geq 0$ then $A = A^* $, where $A^* $ denotes the Hermitian matrix (also known as the self-adjoint matrix), $\\conj{A^T}$. Therefore, positivity gives self-adjoint property for free if we are dealing with complex matrices. Things aren\u0026rsquo;t so elegant for real matrices. For the real case we need to explicitly state that the matrix $A$ is also symmetric apart from what\u0026rsquo;s stated in the definition above. Therefore, we often use the following as the definition of positive matrices:\nDefinition 21\u0026rsquo;: An $n \\times n$ matrix $A$ is positive, in symbols $A \\geq 0$, if it is self-adjoint and if $\\inner{Ax}{x} \\geq 0$ for all $x \\in \\C^n$.\nPositive matrices are also alternatively called  positive semidefinite or nonnegative matrices.\nThe following lemma connects the concept of positive matrices to its eigenvalues.\nLemma 2: A matrix $A \\geq 0$ if and only if $A = A^*$ and every eigenvalue of $A$ is nonnegative.\nProof: Let us first suppose $A \\geq 0$, then $A = A^*$ by definition. Now if $\\lambda$ is an eigenvalue of $A$ and $v$ is an eigenvector of $A$ corresponding to $\\lambda$, we have $$ 0 \\leq \\inner{Av}{v} = \\inner{\\lambda v}{v} = \\lambda \\inner{v}{v} $$ Thus, $\\lambda$ is a nonnegative number.\nFor the other side, find an orthonormal basis $v_1, \\ldots, v_n$ consisting of eigenvectors of $A$ (it exists by the Spectral theorem). Let $v_i$ be the eigenvector corresponding to the eigenvalue $\\lambda_i$. Then for any $x = \\sum_i \\alpha_i v_i$ we have $\\inner{Ax}{x} = \\sum_i \\lambda_i \\modu{\\alpha_i}^2$. Since $\\lambda_i \\geq 0$, $\\inner{Ax}{x} \\geq 0$ and $A$ must be positive. $\\square$\nWe are now ready to define kernel function.\nDefinition 22: Let $\\X$ be a set, then $K : \\X \\times \\X \\to \\C$ is called a kernel function if for every $n \\in \\N$ and for every choice of $\\{x_1, \\ldots, x_n\\} \\subseteq \\X$, the matrix $(K(x_i, x_j)) \\geq 0$. We will use the notation $K \\geq 0$ to denote that the function $K$ is a kernel function.\nKernel functions are alternatively called positive definite functions or positive semidefinite functions.\nDefinition 23: Given a kernel function $K : \\X \\times \\X \\to \\C$ and points $x_1, \\ldots, x_n \\in \\X$, the $n \\times n$ matrix $(K(x_i, x_j))$ is called the Gram matrix of $K$ with respect to $x_1, \\ldots, x_n$.\nSome examples follow:\nExamples  Linear kernels: When $\\X = \\R^d$, we can define the linear kernel function as $$K(x, y) = \\inner{x}{y} $$ It is clearly a symmetric function of its arguments, and hence self-adjoint. To prove positivity, let $x_1, \\ldots, x_n \\in \\R^d$ be an arbitrary collection of points, and consider its gram matrix $\\matr{K}$, i.e., $\\matr{K}_{i,j} = K(x_i, x_j) = \\inner{x_i}{x_j}$. Then for any $\\alpha \\in \\R^n$, we have  $$ \\inner{\\matr{K} \\alpha}{\\alpha} = \\alpha^T \\matr{K}^T \\alpha = \\alpha^T \\matr{K} \\alpha = \\sum_{i,j = 1}^n \\alpha_i \\alpha_j \\inner{x_i}{x_j} = \\left\\lVert \\sum_{i=1}^n \\alpha_i x_i \\right\\rVert^2 \\geq 0 $$\n Polynomial kernels: A natural generalization of the linear kernel on $\\R^d$ is the homogeneous polynomial kernel $$ K(x, y) = (\\inner{x}{y})^m $$ of degree $m \\geq 2$, also defined on $\\R^d$. It is clearly a symmetric function. To prove positivity, note that\n$$ K(x,y) = \\left( \\sum_{i=1}^d x_i y_i \\right)^m $$\nThis will have $D = \\binom{m+d-1}{m}$ monomials, so to simplify the analysis let\u0026rsquo;s take $m=2$. Then\n$$ K(x,y) = \\sum_{i=1}^d x_i^2 y_i^2 + 2 \\sum_{i \u0026lt; j} x_i x_j y_i y_j $$ In this case $D = \\binom{d+1}{d} = d + \\binom{d}{2}$. Define a mapping $\\Phi: \\R^d \\to \\R^D$ such that\n$$ \\Phi(x) = [x_1^2, \\ldots, x_d^2, \\sqrt{2}x_1 x_2, \\ldots, \\sqrt{2} x_{d-1} x_d ]^T $$\nThen\n$$ K(x,y) = \\inner{\\Phi(x)}{\\Phi(y)} $$\nFollowing the same argument as the first example, we can verify that the gram matrix thus formed is positive.\nThe mapping $x \\mapsto \\Phi(x)$ is often referred to as a feature map. We see that dealing with elements in the feature space, i.e. the range of $\\Phi$, is computationally expensive. The relation $K(x,y) = \\inner{\\Phi(x)}{\\Phi(y)}$ allows us compute the inner products using the kernel function instead of actually taking the inner product in a very high dimensional space. We will see that this \u0026ldquo;kernel trick\u0026rdquo; holds for very many kernel functions when we discuss Mercer\u0026rsquo;s theorem.\n  Gaussian kernels: Given some compact subset $\\X \\subseteq \\R^d$, consider the Gaussian kernel\n$$ K(x,y) = \\exp{\\left( -\\frac{1}{2 \\sigma^2} \\norm{x-y}^2 \\right)} $$\nIt is not obvious why this is a kernel function.\n  Equivalence between kernel function and reproducing kernel Let us return to the characterization of reproducing kernels. We will now prove that a function is a kernel function if and only if there is an RKHS for which it is the reproducing kernel. At this point recall Theorem-3 which states that an RKHS admits a unique reproducing kernel.\nTheorem 4: Let $\\X$ be a set and let $\\H$ be an RKHS on $\\X$ with reproducing kernel $K$. Then $K$ is a kernel function.\nProof: For some arbitrary $n \\in \\N$ fix some arbitrary collection $x_1, \\ldots, x_n \\in \\X$ and $\\alpha \\in \\C^n$. Then if we denote by $\\matr{K}$ the gram matrix of $K$ with respect to $x_1, \\ldots, x_n$, we have $$\\inner{\\matr{K} \\alpha}{\\alpha} = \\sum_{i,j = 1}^n \\conj{\\alpha_i}\\alpha_j K(x_i, x_j) = \\sum_{i,j = 1}^n \\conj{\\alpha_i}\\alpha_j \\inner{k_{x_j}}{k_{x_i}} = \\left\\lVert \\sum_{i=1}^n \\alpha_i k_{x_i} \\right\\rVert^2 \\geq 0 \\quad$$\nAnd thus $K$ is a kernel function. $\\square$\nWhat does it mean in the above proof if we have an equality? That is, if $\\inner{\\matr{K} \\alpha}{\\alpha} = 0$? This happens if and only if $\\left\\lVert \\sum_{i=1}^n \\alpha_i k_{x_i} \\right\\rVert = 0$. But this means that for every $f \\in \\H$ we have $\\sum_{i=1}^n \\conj{\\alpha_i} f(x_i) = \\inner{f}{\\sum_i \\alpha_i k_{x_i}} = 0$. Thus, in this case there is an equation of linear dependence between the values of every function in $\\H$ at this finite set of points.\nNow let us state the converse of Theorem-4. It is a deep result in RKHS theory known as the Moore‚ÄìAronszajn theorem.\nTheorem 5 [Moore‚ÄìAronszajn theorem]: Let $\\X$ be a set and let $K : \\X \\times \\X \\to \\C$ be a kernel function, then there exists a reproducing kernel Hilbert space $\\H$ of functions on $\\X$ such that $K$ is the reproducing kernel of $\\H$.\nFor a proof see here on Wikipedia.\nIn light of these two theorems we have the following notation.\nDefinition 24: Given a kernel function $K : \\X \\times \\X \\to \\C$, we let $\\H(K)$ denote the unique RKHS with the reproducing kernel $K$.\nIt is not an easy problem to start with a kernel function $K$ on some set $\\X$ and give a concrete description of $\\H(K)$. I will not be discussing this here, but there are plenty of resources available where you can see this problem getting discussed. I recommend Paulsen and Raghupathi\u0026rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces.\nMercer\u0026rsquo;s theorem Recall the Spectral theorem for finite-dimensional vector spaces: A linear operator $T: V \\to V$ for some finite dimensional vector space $V$ on $\\C$ is normal, i.e., $T T^* = T^* T$, if and only if $V$ has an orthonormal basis consisting of eigenvectors of $T$. This implies that if $\\matr{U} = [v_1, \\ldots, v_n]$ is a unitary matrix containing the $i$th eigenvector in its $i$th column and $\\matr{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)$ is a diagonal matrix containing the corresponding eigenvalues then if $\\matr{K}$ is a normal matrix then it can written as $$\\matr{K} = \\matr{U} \\matr{\\Lambda} \\matr{U}^T = \\sum_{i=1}^n \\lambda_i v_i v_i^T$$\nMercer\u0026rsquo;s theorem generalizes this decomposition to kernel functions. Let us start by defining a special type of kernel function.\nDefinition 25: Let $\\X$ be a compact metric space. A function $K : \\X \\times \\X \\to \\C$ is called a Mercer kernel if it is a continuous kernel function.\nRecall the space $L^2(\\mu)$ from Definition-8 where we now take $\\X$ (written as $X$ there) to be a compact metric space.\nDefinition 26: Given a Mercer kernel $K : \\X \\times \\X \\to \\C$, we define a linear operator $T_{K} : L^2(\\mu) \\to L^2(\\mu)$ as $$ T_K(f)(x) := \\int_{\\X} K(x, y) f(y) \\dmu(y), \\quad (x \\in \\X) $$\nWe assume that the Mercer kernel satisfies the Hilbert-Schmidt condition, stated as\n$$ \\int_{\\X \\times \\X} \\left\\lvert K(x,y) \\right\\rvert^2 \\dmu(x) \\dmu(y) \u0026lt; \\infty $$\nwhich ensures that $T_K$ is a bounded linear operator on $L^2(\\mu)$. Indeed, we have\n$$ \\lVert T_K(f) \\rVert^{2} = \\int_{\\X} \\left\\lvert \\int_{\\X} K(x, y) f(y) \\dmu(y) \\right\\rvert^2 \\dmu(x) \\leq \\norm{f}^2 \\int_{\\X \\times \\X} \\left\\lvert K(x,y) \\right\\rvert^2 \\dmu(x) \\dmu(y) $$\nwhere we have applied Schwarz inequality (Theorem-1) as follows $$ \\left\\lvert \\int_{\\X} K(x, y) f(y) \\dmu(y) \\right\\rvert^2 = \\left\\lvert T_K(f)(x) \\right\\rvert^2 = \\left\\lvert \\inner{K(x, \\cdot)}{f} \\right\\rvert^2 \\leq \\norm{K(x, \\cdot)}^2 \\norm{f}^2$$\nOperators of this type are known as Hilbert-Schmidt operators.\nWe are now ready to state the Mercer\u0026rsquo;s theorem.\nTheorem 5 [Mercer\u0026rsquo;s theorem]: Suppose that $\\X$ is a compact metric space, and $K : \\X \\times \\X \\to \\C$ is a Mercer\u0026rsquo;s kernel that satisfies the Hilbert-Schmidt condition. Then there exists an at most countable set of eigenfunctions $ (e_{i})_{i} $ for $ T_K $ that form an orthonormal basis of $L^2(\\mu)$, and a corresponding set of non-negative eigenvalues $ (\\lambda_{i})_{i} $ such that\n$$ T_K(e_i) = \\lambda_i e_i, \\quad (i \\in \\N) $$\nMoreover, $K$ has the expansion\n$$ K(x,y) = \\sum_{i} \\lambda_i e_i(x) e_i(y), \\quad (x,y \\in \\X) $$\nwhere the convergence of the series above holds absolutely and uniformly.\nI\u0026rsquo;ll skip the proof.\nAmong other things, Mercer\u0026rsquo;s theorem provides a framework for embedding an element of $\\X$ into an element of $\\ell^2(\\N)$ (for its definition, see Example-4 in the section on Hilbert spaces in the first part). More concretely, given the eigenfunctions and eigenvalues guaranteed by Mercer\u0026rsquo;s theorem, we may define a mapping $\\Phi : \\X \\to \\ell^2(\\N)$ as follows\n$$ x \\mapsto \\left( \\sqrt{\\lambda_i} e_i(x) \\right)_{i \\in \\N} $$\nTherefore, we have\n$$\\inner{\\Phi(x)}{\\Phi(y)} = \\sum_{i=1}^{\\infty} \\lambda_i e_i(x) e_i(y) = K(x,y) $$\nThis is the well-known \u0026ldquo;kernel trick\u0026rdquo;. Let us connect Mercer\u0026rsquo;s theorem to the Spectral theorem.\nLet $\\X = [d] := \\{1, 2, \\ldots, d\\}$ along with the Hamming metric be our compact metric space. Let $\\mu(\\{i\\}) = 1$ for all $i \\in [d]$ be the counting measure on $\\X$. Any function $f : \\X \\to \\C$ is equivalent to the $d$-dimensional vector $[f(1), \\ldots, f(d)]$, and any kernel function $K : \\X \\times \\X \\to \\C$ is continuous, satisfies the Hilbert-Schmidt condition, and is equivalent to a $d \\times d$ normal matrix $\\matr{K}$ where $\\matr{K}_{i,j} = K(i, j)$. The Hilbert-Schmidt operator reduces to\n$$ T_K(f)(x) = \\int_{\\X} K(x, y) f(y) \\dmu(y) = \\sum_{i=1}^{d} K(x,y) f(y) $$\nMercer\u0026rsquo;s theorem then states that there exists a set of eigenfunctions $v_1, \\ldots, v_d$ (for our $\\X$ they are equivalent to vectors) and the corresponding eigenvalues $\\lambda_1, \\ldots, \\lambda_d$ such that\n$$ \\matr{K} = \\sum_{i=1}^{d} \\lambda_i v_i v_i^T $$\nwhich is exactly the spectral theorem.\nOperations on kernels Let us now consider how various algebraic operations on kernels affect the corresponding Hilbert spaces. All this (and a lot more) can be found in the seminal paper by Aronszajn \u0026ldquo;Theory of Reproducing Kernels\u0026rdquo;.\nI state the following theorems without proof to illustrate how operations on kernels are done.\nSums of kernels Theorem 6: Suppose that $\\H_1$ and $\\H_2$ are both RKHSs with kernels $K_1$ and $K_2$, respectively. Then the space\n$$\\H = \\H_1 + \\H_2 := \\{f_1 + f_2 \\, : \\, f_1 \\in \\H_1 \\text{ and } f_2 \\in \\H_2 \\}$$\nwith the norm\n$$ \\norm{f}^{2}_{\\H} := \\inf \\left\\{ \\lVert f_1 \\rVert^{2} _ {\\H_1} + \\norm{f_2}^{2} _{\\H_2} \\, : \\, f = f_1 + f_2, f_1 \\in \\H_1, f_2 \\in \\H_2 \\right\\} $$\nis an RKHS with the kernel $K = K_1 + K_2$.\nProducts of kernels Let us first define the notion of tensor product of two (separable) Hilbert spaces $\\H_1$ and $\\H_2$ of functions, say with domains $\\X_1$ and $\\X_2$.\nDefinition 27: Consider the set of functions $h : \\X_1 \\times \\X_2 \\to \\C$ satisfying\n$$\\H = \\left\\{ h = \\sum_{i=1}^{n} u_i v_i \\, : \\, n \\in \\N \\text{ and } u_i \\in \\H_1, v_i \\in \\H_2 \\text{ for all } i \\in [n] \\right\\} $$\nWe define an inner product on $\\H$ as follows: for $h = \\sum_{i=1}^{n} u_i v_i$ and $g = \\sum_{j=1}^{m} w_j x_j$ in $\\H$ define\n$$ \\inner{h}{g} := \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\inner{u_i}{w_j}_{\\H_1} \\inner{v_i}{x_j}_{\\H_2} $$\nThen $\\H$ is a Hilbert space and is called the tensor product of $\\H_1$ and $\\H_2$. We denote it by $\\H = \\H_1 \\otimes \\H_2$.\nWe can now state the theorem for product of kernels.\nTheorem 7: Suppose that $\\H_1$ and $\\H_2$ are RKHSs of real-valued functions with domains $\\X_1$ and $\\X_2$, and equipped with kernels $K_1$ and $K_2$, respectively. Then the tensor product space $\\H = \\H_1 \\otimes \\H_2$ is an RKHS of functions with domain $\\X_1 \\times \\X_2$, and with kernel function $K : (\\X_1 \\times \\X_2) \\times (\\X_1 \\times \\X_2) \\to \\C$ defined by\n$$ K((x,s), (y,t)) := K_1(x,y) K_2(s,t) $$\n$K$ is called the tensor product of the kernels $K_1$ and $K_2$, and denoted by $K = K_1 \\otimes K_2$.\nOther operations We can similarly define more operations on kernels:\n If $K$ is a valid kernel and $\\alpha \\geq 0$, then $\\alpha K$ is a valid kernel. If $K$ is a valid kernel and $\\alpha \\geq 0$, then $K + \\alpha$ is a valid kernel. We can easily see from all these results that a linear combination or more generally for any polynomial $P$ with positive coefficients, the composition $P \\circ K$ is a valid kernel if $K$ is a valid kernel. If $K$ is a valid kernel, then $\\exp(K)$ is a valid kernel.  Representer theorem We are now at a stage where we can put all this theory to use in machine learning. Specifically we will develop Representer theorem which allows many optimization problems over the RKHS to be reduced to relatively simple calculations involving the gram matrix.\nLet us start with a functional analytic viewpoint of supervised learning. Suppose we are given empirical data\n$$ (x_1, y_1), \\ldots, (x_n, y_n) \\in \\X \\times \\Y $$\nwhere $\\X$ is a nonempty set. For now let $\\Y = \\R$. They are from an unknown function, $g : \\X \\to \\R$, i.e., we assume\n$$ y_i = g(x_i), \\quad (i \\in [n]) $$\nWe need to find some function $f^*$ which \u0026ldquo;best\u0026rdquo; approximates $g$. A natural way to formalize the notion of \u0026ldquo;best\u0026rdquo; is to limit ourselves to an RKHS $\\H$ which contains functions of the form $f : \\X \\to \\R$ and choose\n$$ f^* = \\argmin_{f \\in \\H} \\norm{f} \\quad \\text{ such that } f^*(x_i) = y_i \\text{ for } i \\in [n]$$\nThis optimization problem is feasible whenever there exists at least one function $f \\in \\H$ that fits the data exactly. Denote by $y$ the vector $[y_1, \\ldots, y_n]^T$. It can be shown that if $\\matr{K}$ is the gram matrix of the kernel $K$ with respect to $x_1, \\ldots, x_n$, then the feasibility is equivalent to $y \\in \\text{range}(\\matr{K})$. This is a special of the representer theorem.\nIn a realistic setting we assume that we have noisy observations, i.e.,\n$$y_i = g(x_i) + \\e_i, \\quad (i \\in [n])$$\nwhere $\\e_i$'s denote the noise. Then the constraint of exact fitting is no longer desirable, and we model the \u0026ldquo;best\u0026rdquo; approximation by introducing a loss function which represents how close our approximation is to the observed outputs. More concretely, let $L_y : \\R^n \\to \\R$ be a continuous function. Then we can define our cost function as\n$$ J(f) = \\norm{f}^2 + L_y(f(x_1), \\ldots, f(x_n)) $$\nTheorem 8 [Representer theorem]: If $f^*$ is a function such that\n$$ J(f^*) = \\inf_{f \\in \\H} J(f) $$\nthen $f^*$ is in the span of the functions $k_{x_1}, \\ldots, k_{x_n}$, i.e.,\n$$ f^*(\\cdot) = \\sum_{i=1}^{n} \\alpha_i k_{x_i}(\\cdot) \\quad \\text{for some } \\alpha_1, \\ldots, \\alpha_n \\in \\C$$\nI\u0026rsquo;ll skip the proof which can be found in Paulsen and Raghupathi\u0026rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces or in Sch√∂lkopf, Herbrich and Smola\u0026rsquo;s A Generalized Representer Theorem.\nAs an example $L$ could be the squared loss $L_y(f(x_1), \\ldots, f(x_n)) = \\sum_{i=1}^n (y_i - f(x_i))^2$. If we assume $L$ to be convex then the solution exists and is unique. Recall that $E \\subseteq \\R^k$ is a convex set if $\\lambda x + (1-\\lambda) y \\in E$ whenever $x,y \\in E$ and $0 \u0026lt; \\lambda \u0026lt; 1$. A real-valued function $L$ defined on a convex set $E$ is a convex function if $L(\\lambda x + (1-\\lambda) y) \\leq \\lambda L(x) + (1-\\lambda) L(y)$ whenever $x, y \\in E$ and $0 \u0026lt; \\lambda \u0026lt; 1$. Note that a convex function is continuous.\nThe Representer theorem allows us to reduce the infinite-dimensional problem of optimizing over an RKHS to an $n$-dimensional problem.\nEpilogue I barely scratched the surface of reproducing kernel Hilbert space theory. Some resources I recommend that do a much better job than me in explaining this theory and which I used as reference are:\n Paulsen V and Raghupathi M: An Introduction to the Theory of Reproducing Kernel Hilbert Spaces Wainwright M: High-Dimensional Statistics Sch√∂lkopf B, Herbrich R and Smola A.J: A Generalized Representer Theorem (COLT 2001) Aronszajn N: Theory of Reproducing Kernels (1950)  In the next article I will discuss kernel approximation methods. In particular I will focus on random Fourier Features developed by Rahimi and Recht which I am using in my work.\n","date":1594930823,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594930823,"objectID":"e6a68300e4479c6d14bd6375724ab9ea","permalink":"https://makkar.github.io/post/kernels1/","publishdate":"2020-07-16T16:20:23-04:00","relpermalink":"/post/kernels1/","section":"post","summary":"Some results in Reproducing Kernel Hilbert Spaces","tags":[],"title":"Kernels - Part 1","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\C}{\\mathbb{C}} \\newcommand{\\F}{\\mathbb{F}} \\newcommand{\\FF}{\\mathcal{F}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\X}{\\mathcal{X}} \\newcommand{\\H}{\\mathcal{H}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limn}{\\lim_{n \\to \\infty}} \\newcommand\\inner[2]{\\langle #1, #2 \\rangle} \\newcommand{\\norm}[1]{\\lVert#1\\rVert} \\newcommand{\\conj}[1]{\\overline{#1}} \\DeclareMathOperator{\\dx}{dx} \\DeclareMathOperator{\\dmu}{d\\mu} $$\nIntroduction The word \u0026ldquo;kernel\u0026rdquo; is heavily overloaded, but for our purposes it is, intuitively, a similarity measure that can be thought of as an inner product in some feature space. Kernel methods provide an elegant, theoretically well-founded, and powerful approach to solving many learning problems.\nWe usually have the following framework: The input space $\\X$ which contains our observations/inputs/features is either not rich enough (for example, if there is no linear boundary separating the two classes in a binary classification problem) or not convenient (for example, if our inputs are strings), and therefore we want to work is some other space we call feature space $\\H$. Suppose we have a map which takes our inputs from $\\X$ to $\\H$: $$ \\Phi : \\X \\to \\H $$ Then the class of kernels we are interested in are those for which is it possible to write $$ K(x,y) = \\langle \\Phi(x), \\Phi(y) \\rangle, \\quad x,y \\in \\X $$ What kind of functions, $K : \\H \\times \\H \\to \\C$ admit such a representation? We aim to be able to answer this question and many others in this series of articles on kernels.\nIn this article, I aim to introduce the necessary concepts from analysis, linear algebra, and functional analysis so as to understand Reproducing Kernel Hilbert Spaces (RKHS) and a few of their basic properties. In the next article I will discuss the kernel trick and some important theorems like Mercer\u0026rsquo;s theorem and Representer theorem.\nBackground A topic like this requires quite a bit of background before we can get to the interesting results like the one above. It\u0026rsquo;s always easy to get lazy and assume all the necessary background from the reader and get straight to the meat, but I want to write an article which I would have found useful had I had it when I started learning about kernel theory. With that being said, I am under no illusion and believe that a much better way to learn this background would be to read a functional analysis text if you have the time.\nLinear spaces Definition 1: A linear space, or alternatively a vector space, over a field $\\F$ ($\\F$ is $\\R$ or $\\C$ for our purposes) is a set $V$ of elements called vectors (the elements of $\\F$ are called scalars) satisfying:\n(A) To every pair, $x$ and $y$, of vectors in $V$ there corresponds a vector $x+y$, called the sum of $x$ and $y$, in such a way that\n addition is commutative, $x+y = y+x$, addition is associative, $x+(y+z) = (x+y)+z$, there exists in $V$ a unique vector $0$ such that $x+0=x$ for every vector $x$, and to every vector $x \\in V$ there corresponds a unique vector $-x$ such that $x+(-x)=0$.  (B) To every pair, $\\alpha \\in \\F$ and $x \\in V$, there corresponds a vector $\\alpha x \\in V$, called the product of $\\alpha$ and $x$, in such a way that\n multiplication by scalars is associative, $\\alpha(\\beta x) = (\\alpha \\beta)x$, and $1x = x$ for every vector $x$.  (C) Finally the distributive properties\n $\\alpha(x+y) = \\alpha x + \\alpha y$, and $(\\alpha + \\beta) x = \\alpha x + \\beta x$.  Examples  The Euclidean spaces $\\R^n$ are vector spaces over the real field. $\\C^n$ are vector spaces over $\\C$. The set of all polynomials, with complex coefficients, in a variable $t$ is a vector space over $\\C$. The set $C$ of all continuous complex functions on the unit interval $[0,1]$ is a vector space over $\\C$.  Definition 2: A linear transformation of a linear space $V$ into a linear space $W$ is a mapping $T: V \\to W$ such that $$ T(\\alpha x + \\beta y) = \\alpha T(x) + \\beta T(y), \\quad (x,y \\in V;\\; \\alpha, \\beta \\in \\F) $$\nDefinition 3: In the special case in which $W$ above is a field, $T$ is called a linear functional.\nNote that we often write $Tx$ instead of $T(x)$, if $T$ is linear, hinting that a linear transformation mapping a finite dimensional vector to another finite dimensional vector space is equivalent to a matrix vector product.\nDefinition 4: Let $\\mu$ be a positive measure on an arbitrary measurable space $X$. We define $L^1(\\mu)$ to be the collection of all complex measurable functions $f$ on $X$ for which $$ \\int_X |f| \\dmu \u0026lt; \\infty $$\nIt can be shown that for every $f, g \\in L^1(\\mu)$ and for every $\\alpha, \\beta \\in \\C$, we have $\\alpha f + \\beta g \\in L^1(\\mu)$, and $$ \\int_X (\\alpha f + \\beta g) \\dmu = \\alpha \\int_X f \\dmu + \\beta \\int_X g \\dmu $$\nThus, $L^1(\\mu)$ is a vector space, and the mapping $F: L^1(\\mu) \\to \\R$ defined by $$ F(f) = \\int_X |f| \\dmu, \\quad f \\in L^1(\\mu) $$ is a linear functional.\nInner products and norms Definition 5: If $V$ be a linear space over $\\C$, an inner product on $V$ is a function $\\langle \\cdot , \\cdot \\rangle: V \\times V \\to \\C$ such that for all $\\alpha, \\beta \\in \\C$, and all $x,y,z \\in V$, the following are satisfied:\n Linearity in the first argument: $\\langle \\alpha x + \\beta y, z \\rangle = \\alpha \\langle x,z \\rangle + \\beta \\langle y,z \\rangle$, Conjugate symmetry: $\\inner{x}{y} = \\conj{\\inner{y}{x}}$, Positivity: $\\inner{x}{x} \\geq 0$, If $\\inner{x}{x} = 0$, then $x=0$.  A function satisfying only the first three properties is called a semi-inner product on $V$.\nAn immediate consequences of this definition: For every $y \\in V$, the mapping $F: V \\to \\C$ defined by $$ F(x) = \\inner{x}{y}, \\quad (x \\in V) $$ is a linear functional on $V$.\nDefinition 6: If $V$ be a linear space over $\\C$, a norm on $V$ is a non-negative function $\\norm{\\cdot} : V \\to \\R$ such that for all $\\alpha \\in \\C$, and all $x,y \\in V$, the following are satisfied:\n Subadditivity: $\\norm{x+y} \\leq \\norm{x} + \\norm{y}$, Absolutely homogenous: $\\norm{\\alpha x} = |\\alpha| \\norm{x}$, Positive definite: $\\norm{x} = 0 \\implies x = 0$.  Given an inner product, we can define a norm as follows: $$ \\norm{x} = \\sqrt{\\inner{x}{x}} $$\nA classic result useful in many proofs:\nTheorem 1: Schwarz inequality $$|\\inner{x}{y}| \\leq \\norm{x} \\, \\lVert y \\rVert$$ Equality hold for $y = \\alpha x$ or $y=0$.\nThe proof is not too difficult.\nDefinition 7: The virtue of norm on a vector space $V$ is that $$ d(x,y) := \\norm{ x-y } $$ defines a metric on $V$ so that $V$ becomes a metric space.\nNote that technically the tuple $(V, d)$ defines a metric space, but I will often write just $V$ instead of $(V, d)$ when it\u0026rsquo;s clear from context what the metric $d$ is.\nDefinition 8: If $0 \u0026lt; p \u0026lt; \\infty$, $f$ is a complex measurable function on $X$, and $\\mu$ is a nonnegative measure on $X$, define $$ \\norm{f}_p := \\left( \\int_X |f|^p \\dmu \\right)^{1/p} $$ and let $L^p(\\mu)$ consist of all $f$ for which $\\norm{f}_p \u0026lt; \\infty$. We call $\\norm{f}_p$ the $L^p$-norm of $f$.\nHilbert spaces Definition 9: An inner product space, or alternatively a pre-Hilbert space, is a linear space with an inner product defined on it.\nWe need the concept of completeness to define Hilbert space. But before that let me define Cauchy sequences.\nDefinition 10: Given a metric space $(M, d)$, a sequence $(x_n)_{n \\in \\N}$ of elements in $M$ is called a Cauchy sequence if for every positive real number $\\e \u0026gt; 0$ there exists a positive integer $N \\in \\N$ such that $m, n \u0026gt; N$ implies that $d(x_m, x_n) \u0026lt; \\e$.\nRecall that we say a sequence $(x_n)_{n \\in \\N}$ in a metric space $(M, d)$ converges if there exists a point $x \\in M$ with the following property: for every $\\e \u0026gt; 0$ there exists a positive integer $N \\in \\N$ such that $n \u0026gt; N$ implies that $d(x_n, x) \u0026lt; \\e$.\nIt can be shown that every convergent sequence is a Cauchy sequence: Let the sequence $(x_n)_{n \\in \\N}$ in a metric space $(M, d)$ converge to $x \\in M$. If $\\e \u0026gt; 0$, there is an integer $N \\in \\N$ such that $d(x_n, x) \u0026lt; \\e$ for all $n \u0026gt; N$. Hence\n$$d(x_n, x_m) \\leq d(x_n, x) + d(x, x_m) \u0026lt; 2\\e$$\nif $n,m \u0026gt; N$. Thus $(x_n)_{n \\in \\N}$ is a Cauchy sequence.\nThe converse is not necessarily true. But if it holds in some space, we anoint the space with a special name.\nDefinition 11: A metric space $(M, d)$ is called complete if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.\nDefinition 12: A pre-Hilbert space $\\H$ is called a Hilbert space if it is complete in the metric $d$ (see Definition 7).\nExamples   The sets $\\R^n$ and $\\C^n$ are Hilbert spaces if we define $\\inner{x}{y} := \\sum_{i=1}^n x_i \\conj{y_i}$.\n  The set $C$ of all continuous complex functions on the unit interval $[0,1]$ defined above is an inner product space if we define $$ \\inner{f}{g} := \\int_0^1 f(x) \\conj{g(x)} \\dx $$ but is not a Hilbert space.\n  $L^2(\\mu)$ is a Hilbert space, with inner product $$ \\inner{f}{g} := \\int_X f \\, \\conj{g} \\dmu $$\n  The space of square-summable real-valued sequences, namely $$ \\ell^2(\\N) := \\left\\{ (x_n)_{n \\in \\N} \\; : \\; x_n \\in \\R,\\, \\sum_n x_n^2 \u0026lt; \\infty \\right\\} $$\nThis set, when endowed with the inner product $\\inner{x}{y} := \\sum_{n \\in \\N} x_n y_n$, defines a Hilbert space. It will play an important role in our discussion of eigenfunctions for Reproducing Kernel Hilbert spaces.\n  Definition 13: Consider a linear space $\\FF$ of functions each of which is a mapping from a set $X$ into $\\F$. For $x \\in X$, a linear evaluation functional is a linear functional $E_x$ that is defined as $$ E_x(f) = f(x), \\quad (f \\in \\FF) $$ In other words, a linear evaluation functional with respect to $x \\in X$ evaluates each function at $x$.\nIn general, the evaluation functional is not continuous. This means we can have $f_n \\to f$ but $E_x(f_n)$ does not converge to $E_x(f)$. Intuitively, this is because Hilbert spaces can contain very unsmooth functions. We will later consider a special type of Hilbert space, Reproducing Kernel Hilbert Space where evaluation functional is continuous.\nA lemma that will be useful later on:\nLemma 1: Let $\\H$ be a Hilbert space and $L:\\H \\to \\F$ a linear functional. The following statements are equivalent:\n $L$ is continuous. $L$ is continuous at $0$. $L$ is continuous at some point. $L$ is bounded, i.e., there is a constant $c \u0026gt; 0$ such that $|L(f)| \\leq c \\norm{f}$ for every $f \\in \\H$.  Proof: It is clear that $(1) \\implies (2) \\implies (3)$, and $(4) \\implies (2)$. Let\u0026rsquo;s show that $(3) \\implies (1)$, and $(2) \\implies (4)$.\n$(3) \\implies (1)$: Suppose $L$ is continuous at $f$ and $g$ is any point in $\\H$. If $g_n \\to g$ in $\\H$, then $g_n - g + f \\to f$. By assumption $L(f) = \\limn L(g_n - g + f) = \\limn L(g_n) - L(g) + L(f)$. Hence $L(g) = \\limn L(g_n)$.\n$(2) \\implies (4)$: The definition of continuity at $0$ implies that $L^{-1}(\\{\\alpha \\in \\F : |\\alpha| \u0026lt; 1\\})$ contains an open ball centered at $0$. Let $\\delta \u0026gt; 0$ be the radius of that open ball centered at $0$. Then for $f \\in \\H$ and $\\norm{f} \u0026lt; \\delta$ we have $|L(f)| \u0026lt; 1$. If $f$ is an arbitrary element of $\\H$ and $\\e \u0026gt; 0$, then $$ \\left\\lVert \\frac{\\delta f}{\\norm{f} + \\e} \\right\\rVert \u0026lt; \\delta $$ Hence, $$ 1 \u0026gt; \\left\\lvert L\\left( \\frac{\\delta f}{\\norm{f} + \\e} \\right) \\right\\rvert = \\frac{\\delta }{\\norm{f} + \\e} |L(f)| $$ Letting $\\e \\to 0$ we see that $(4)$ holds with $c = 1/\\delta$. $\\square$\nOrthonormal bases We generalize the idea of orthonormal basis to infinite dimensional case. This will be needed when we discuss Mercer\u0026rsquo;s theorem.\nDefinition 14: A collection of vectors $\\{v_{\\alpha} \\, : \\, \\alpha \\in A \\}$ in a Hilbert space $\\H$ for some index set $A$ is called orthonormal if it satisfies $\\inner{v_{\\alpha}}{v_{\\beta}} = \\delta_{\\alpha \\beta}$ where $\\delta_{\\alpha \\beta}$ is the Kronecker delta, which equals $1$ if $\\alpha = \\beta$ and $0$ otherwise.\nDefinition 15: A collection of vectors $\\{v_{\\alpha} \\, : \\, \\alpha \\in A \\}$ in a Hilbert space $\\H$ is complete if for any $u \\in \\H$, $\\inner{u}{v_{\\alpha}} = 0$ for all $\\alpha \\in A$ implies that $u = 0$.\nDefinition 16: An orthonormal basis a complete orthonormal system.\nNote, we can also define an orthonormal basis as a maximal orthonormal set in $\\H$. To say $\\{v_{\\alpha}\\}$ is maximal means that no vector of $\\H$ can be added to $\\{v_{\\alpha}\\}$ in such a way that the resulting set is still orthonormal. This happens precisely when there is no $u \\neq 0$ in $\\H$ that is orthogonal to every $v_{\\alpha}$.\nSeparable Hilbert spaces Another idea we need is separability. Let us define that now.\nDefinition 17: A topological space is called separable if it contains a countable, dense subset; that is, there exists a sequence $(x_{n})_{n=1}^{\\infty }$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.\nDefinition 18: A Hilbert space is separable if and only if it has a countable orthonormal basis. It follows that any separable, infinite-dimensional Hilbert space is isometric to the space $\\ell^2(\\N)$ of square-summable sequences.\nWe will be dealing with separable Hilbert spaces in our discussion.\nRiesz representation theorem We now come to a very important theorem called the Riesz representation theorem. The name Riesz has many theorems attached to it, but the one relevant to us is the following:\nTheorem 2: For each continuous linear functional $L$ on a Hilbert space $\\H$, there exists a unique $g \\in \\H$ such that $$L(f) = \\inner{f}{g}\\, , \\quad (f \\in \\H) $$\nI\u0026rsquo;ll skip the proof as it\u0026rsquo;s not easy and will unnecessarily make this article abstruse.\nSide-note: In the mathematical treatment of quantum mechanics, this theorem can be seen as a justification for the popular bra‚Äìket notation.\nReproducing Kernel Hilbert Spaces (RKHS) Definition 19: Let $\\X$ be a set. We will call a set $\\H$ of functions from $\\X$ to $\\F$ a Reproducing Kernel Hilbert Space (RKHS) on $\\X$ if\n $\\H$ is a vector space, $\\H$ is endowed with an inner product, $\\inner{\\cdot}{\\cdot}$, with respect to which $\\H$ is a Hilbert space, for every $x \\in \\X$, the linear evaluation functional $E_x : \\H \\to \\F$, is bounded (or continuous, as seen from Lemma-1).  If $\\H$ is an RKHS on $\\X$, then an application of the Riesz representation theorem shows that the linear evaluation functional is given by the inner product with a unique vector in $\\H$. Therefore, for each $x \\in \\X$, there exists a unique vector $k_x \\in \\H$, such that for every $f \\in \\H$, $$ f(x) = E_x(f) = \\inner{f}{k_x} $$\nDefinition 20: The function $k_x$ is called the reproducing kernel for the point $x$. The function $K: \\X \\times \\X \\to \\F$ defined by $$ K(x,y) = k_y(x) $$ is called the reproducing kernel for $\\H$.\nNote that we have $$ K(x,y) = k_y(x) = \\inner{k_y}{k_x} = \\conj{\\inner{k_x}{k_y}} = \\conj{K(y,x)}$$\nAlso, $$ \\norm{E_y}^2 = \\norm{k_y}^2 = \\inner{k_y}{k_y} = K(y,y) $$\nExample The first question that comes to mind is if any Reproducing Kernel Hilbert Spaces exist. The following example answers this question in the affirmative.\nWe saw before that $\\C^n$ is a Hilbert space. We can show that $\\C^n$ is in fact an RKHS. Let $\\X = \\{1, 2, \\ldots, n\\}$, then we can view $v \\in \\C$ as a function $V : \\X \\to \\C$, where $V(j) = v_j$. The linear evaluation functionals are of course bounded for every $x \\in \\X$ and we have $$ V(j) = v_j = \\inner{V}{e_j}\\,, \\quad (j \\in \\X) $$ where $e_j$ is a vector with $1$ at $j$th position and $0$ everywhere else. Therefore, the reproducing kernel for the point $x \\in \\X$ is $e_x$ and the reproducing kernel can be thought as the identity matrix.\nCan there be multiple reproducing kernels for an RKHS? The following theorem answers this question.\nTheorem 3: If an RKHS $\\H$ of functions on a set $\\X$ admits a reproducing kernel, $K$, then $K$ is uniquely determined by $\\H$.\nProof: Suppose that there exists another reproducing kernel $K'$ for $\\H$. Then $$ \\norm{k_y - k\u0026rsquo;_y} = \\inner{k_y - k\u0026rsquo;_y}{k_y - k\u0026rsquo;_y} = \\inner{k_y - k\u0026rsquo;_y}{k_y} - \\inner{k_y - k\u0026rsquo;_y}{k\u0026rsquo;_y} = (k_y - k\u0026rsquo;_y)(y) - (k_y - k\u0026rsquo;_y)(y) = 0 $$ for any $y \\in \\X$. In other words, $k_y(x) = k\u0026rsquo;_y(x)$ for every $x \\in \\X$ by the positive definite property of norms and hence the kernel is unique. $\\square$\nEpilogue We covered quite a lot of ground in this blog post but I didn\u0026rsquo;t even define a kernel as we commonly use in machine learning! In the next post I will do that and cover its fundamental properties.\nSome resources I recommend to go into more depth on what\u0026rsquo;s covered here are:\n Halmos, P: Finite-Dimensional Vector Spaces. Rudin, W: Real and Complex Analysis. (Chapter - 4) Rudin, W: Functional Analysis. Conway, J: A course in functional analysis.  ","date":1593826510,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593826510,"objectID":"0947cda7e02dd4e13d4c0959ccd45212","permalink":"https://makkar.github.io/post/kernels/","publishdate":"2020-07-03T21:35:10-04:00","relpermalink":"/post/kernels/","section":"post","summary":"Functional analysis background for kernel theory.","tags":[],"title":"Kernels - Part 0","type":"post"},{"authors":[],"categories":[],"content":"I want to show how these two concepts are a single mathematical idea.\nPartition A partition of a non-empty set, $X$, is a disjoint class $\\{X_i\\}_{i \\in I}$ of non-empty subsets of $X$ whose union is $X$. The $X_i$'s are called the partition sets.\nFor example, if $X = \\mathbb{R}$, then $X$ can be partitioned as $$ X = \\bigcup_{n \\in \\mathbb{Z}}[n, n+1) $$\nBinary Relation A binary relation, or simply relation, $R$, in the set $X$ is a subset of $X \\times X$.\nFor $x, y \\in X$, we denote the fact $(x,y) \\in R$ by writing $x R y$. A function may be defined as a special kind of binary relation.\nEquivalence relation Let\u0026rsquo;s assume that a partition of our non-empty set $X$ is given, and we associate with this partition a relation, $\\sim$, in $X$ defined as follows: $x \\sim y$ if $x$ and $y$ belong to the same partition set. It can easily checked that the relation $\\sim$ satisfies:\n $x \\sim x$ for every $x \\in X$ (reflexivity); $x \\sim y \\implies y \\sim x$ (symmetry); $x \\sim y$ and $y \\sim z$ $\\implies$ $x \\sim z$ (transitivity).  Any relation in $X$ which possesses these three properties is called an equivalence relation in $X$.\nExamples  Let $X = \\mathbb{Z}$ and let $x \\sim y$ if $2 | x-y$ for $x,y \\in X$. Then clearly $\\sim$ is an equivalence relation in $X$. Let $X$, $Y$ be any non-empty sets and $f$ be a mapping from $X$ onto $Y$. Let $x \\sim y$ if $f(x) = f(y)$ for $x,y \\in A$. This defines an equivalence relation in $X$. Indeed, $f(x) = f(x)$, and so $x \\sim x$. If $f(x) = f(y)$ then $f(y) = f(x)$, and so $x \\sim y \\implies y \\sim x$. Finally, if $f(x) = f(y)$ and $f(y) = f(z)$ then $f(x) = f(z)$, and so $x \\sim y$ and $y \\sim z$ $\\implies$ $x \\sim z$. The first example is a special case of this one if we take $X = \\mathbb{Z}$, $Y = \\{0,1\\}$ and $f(x) = x \\mod 2$.  \u0026ldquo;Relation\u0026rdquo; to partition We have just seen that each partition of $X$ has associated with it a natural equivalence relation in $X$. Let us now reverse the situation and show that a given equivalence relation in $X$ determines a natural partition of $X$.\nLet $\\sim$ be an equivalence relation in $X$. For every $x \\in X$ define the set $$ [x] := \\{ y \\in X : y \\sim x\\} $$ called the equivalence set of $x$. We show that the class of all distinct equivalence sets forms a partition of $X$.\nBy reflexivity, $x \\in [x]$ for every $x \\in X$, and thus each equivalence set is non-empty and their union is $X$. We now need to show that any two equivalence sets $[x_1]$ and $[x_2]$ are either disjoint or identical. We prove this by showing that if $[x_1]$ and $[x_2]$ are not disjoint then they are identical. To this end, let $z$ be a common element of $[x_1]$ and $[x_2]$. Let $y$ be any element of $[x_1]$. Using transitivity, $$ y \\sim x_1 \\sim z \\sim x_2 $$ Therefore, $y \\in [x_2]$. Since $y$ was an arbitrary element of $[x_1]$, we get $[x_1] \\subseteq [x_2]$. We can similarly show that $[x_2] \\subseteq [x_1]$. In short, $[x_1] = [x_2]$.\nWe have shown that there is no real distinction between partitions of a set and equivalence relations in the set. They are two \u0026ldquo;equivalent\u0026rdquo; approaches for the same mathematical idea. The approach we choose in an application depends entirely on our own convenience.\n","date":1592695005,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592695005,"objectID":"d1bbc0efd74ed683d7616f54612bc205","permalink":"https://makkar.github.io/post/equivalence-relations/","publishdate":"2020-06-20T19:16:45-04:00","relpermalink":"/post/equivalence-relations/","section":"post","summary":"I want to show how these two concepts are a single mathematical idea.","tags":[],"title":"Partitions and Equivalence Relations","type":"post"},{"authors":[],"categories":[],"content":"$$ \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\RR}{\\overline{\\mathbb{R}}} \\newcommand{\\Q}{\\mathbb{Q}} \\newcommand{\\N}{\\mathbb{N}} \\newcommand{\\Z}{\\mathbb{Z}} \\newcommand{\\e}{\\epsilon} \\newcommand{\\limm}{\\lim_{n \\to \\infty}} \\newcommand{\\lims}{\\limsup_{n \\to \\infty}} \\newcommand{\\limi}{\\liminf_{n \\to \\infty}} \\newcommand{\\sn}{\\{s_n\\}} \\newcommand{\\snk}{\\{s_{n_k}\\}} \\newcommand{\\supk}{\\sup_{k \\geq n}} \\newcommand{\\infk}{\\inf_{k \\geq n}} \\newcommand{\\sups}{\\sup_{k \\geq n} s_k} \\newcommand{\\infs}{\\inf_{k \\geq n} s_k} \\newcommand{\\cc}{\\mathsf{c}} $$\nMost of what follows is taken from Rudin\u0026rsquo;s Principles of Mathematical Analysis.\nIntroduction The concept of upper and lower limits (commonly denoted by $\\limsup$ and $\\liminf$ respectively) shows up routinely when discussing the limiting behaviour of a sequence. For example, consider the Big O notation, $\\mathcal{O}$, defined as $f(n) = \\mathcal{O}(g(n))$ iff there exists a positive real number $c$ and an integer $N$ such that for every $n \\in \\Z$, $n \u0026gt; N$, we have $|f(n)| \\leq c g(n)$. This can be succinctly written as $$ \\lims \\frac{|f(n)|}{g(n)} \u0026lt; \\infty $$\nIn this post, I aim to expound on the concept of upper and lower limits so that the second formulation of Big O notation above becomes easier to understand than the first one.\nDefinitions I start with some definitions.\nDefinition 1: A sequence is function defined on the set $\\N$ of positive integers. If $f(n) = x_n$, for $n \\in \\N$, we denote the sequence $f$ by the symbol $\\{x_n\\}$, or sometimes by $x_1, x_2, \\ldots$.\nDefinition 2: A sequence $\\{p_n\\}$ in a metric space $X$ is said to converge if there is a point $p \\in X$ with the following property: For every $\\e \u0026gt; 0$ there is an integer $N$ such that $n \\geq N$ implies that $d(p_n, p) \u0026lt; \\e$. We write this as $\\limm p_n = p$ or as $p_n \\to p$.\nDefinition 3: Let $\\{s_n\\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\geq M$. We then write $s_n \\to +\\infty$. Similarly for $s_n \\to -\\infty$.\nNote: The symbol $\\to$ is now used for certain type of divergent sequences as well.\nDefinition 4: Given a sequence $\\{p_n\\}$, consider a sequence $\\{n_k\\}$ of positive integers, such that $n_1 \u0026lt; n_2 \u0026lt; \\cdots$. Then the sequence $\\{p_{n_k}\\}$, which is a composition of the functions $\\{n_k\\}$ and $\\{p_n\\}$, is called a subsequence of ${p_n}$. If $\\{p_{n_k}\\}$ converges, its limit is called a subsequential limit of $\\{p_n\\}$.\nDefinition 5: Let $\\sn$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system, i.e., $x \\in \\RR := \\R \\cup \\{+\\infty, -\\infty\\}$) such that $s_{n_k} \\to x$ for some subsequence $\\{s_{n_k}\\}$. Therefore, this set contains all the subsequential limits of $\\sn$ plus possibly the numbers $+\\infty$ and $-\\infty$. We define $$ s^* = \\sup E $$ $$ s_* = \\inf E $$ The numbers $s^*$ and $s_*$ are called the upper and lower limits of $\\sn$. We use the notation $$ \\lims s_n = s^* $$ $$ \\limi s_n = s_* $$ It immediately follows that $s_* \\leq s^*$.\nThe fact that $E$ is non-empty (and thus taking $\\sup$ or $\\inf$ makes sense) follows from the observation that either $\\sn$ is bounded or unbounded. If it is bounded then it must contain a convergent subsequence (Bolzano‚ÄìWeierstrass theorem) and thus at least one element, or if it is unbounded then it must contain either $+\\infty$ or $-\\infty$.\nSome useful lemmas Now let us prove some interesting lemmas that will be useful later.\nLemma 1: The subsequential limits of a sequence $\\{p_n\\}$ in a metric space $X$ form a closed subset of $X$.\nProof: Let $E$ be the set of all subsequential limits of $\\{p_n\\}$ and let $q$ be a limit point of $E$. We have to show that $q \\in E$. To show this we will construct a subsequence of $\\{p_n\\}$ which converges to $q$.\nChoose $n_1$ so that $p_{n_1} \\neq q$. If no such $n_1$ exists, then $E$ has only one element, $q = p_1 = p_2 = \\cdots$, and there is nothing to prove. Define $\\delta = d(q, p_{n_1})$. Suppose $n_1, \\ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E$, there is an $x \\in E$ with $d(q, x) \u0026lt; \\frac{\\delta}{2^i}$. Since $x \\in E$, there is an $n_i \u0026gt; n_{i-1}$ such that $d(x, p_{n_i}) \u0026lt; \\frac{\\delta}{2^i}$. Thus $$ d(q, p_{n_i}) \\leq d(q, x) + d(x, p_{n_i}) \u0026lt; \\frac{\\delta}{2^{i-1}} \\quad \\text{for } i = 1,2,\\ldots $$ This implies $p_{n_k} \\to q$, and thus $q \\in E$. $\\square$\nLemma 2: Let $F$ be a nonempty closed set of real numbers which is bounded above. Let $\\alpha = \\sup F$. Then $\\alpha \\in F$.\nProof: Assume for the sake of the contradiction that $\\alpha \\notin F$. Then since $F^\\cc$ is an open set (because $F$ is closed) there exists an $\\e \u0026gt; 0$ such that $(\\alpha - \\e, \\alpha + \\e) \\subset F^\\cc$. But this implies $\\alpha - \\frac{\\e}{2}$ is an upper bound for $F$ which is lower that $\\alpha$. This gives us our required contradiction. $\\square$\nProperties We now have all the tools to prove a very useful characterization of upper and lower limits and the highlight of this post.\nTheorem 1: Let $\\sn$ be a sequence of real numbers. Let $E$ and $s^*$ have the same meaning as in Definition 5. Then $s^*$ has the following two properties:\n $s^* \\in E$. If $x \u0026gt; s^*$, there is an integer $N$ such that $n \\geq N$ implies $s_n \u0026lt; x$.  Moreover, $s^*$ is the only number with these two properties.\nOf course, an analogous result is true for $s_*$.\nProof: We start by showing the two properties.\n  We divide it into three cases depending on what value $s^*$ takes:\nIf $s^* = +\\infty$, then $E$ is not bounded above; hence $\\sn$ is not bounded above, and thus there is a subsequence $\\{s_{n_k}\\}$ such that $s_{n_k} \\to +\\infty$. Therefore, $+\\infty \\in E$ and thus $s^* \\in E$.\nIf $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists by the definition of $\\sup$. Therefore, $s^* \\in E$ follows from the Lemmas 1 and 2, and the fact that $s^* = \\sup E$.\nIf $s^* = -\\infty$, then $E$ contains only one element, namely $-\\infty$, and there is no subsequential limit. Thus, $s^* \\in E$.\n  Suppose for the sake of contradiction that there is a number $x \u0026gt; s^*$ such that $s_n \\geq x$ for infinitely many values of $n$. Let\u0026rsquo;s denote this set of $n$'s with $\\mathcal{K}$ and let $\\{s_k\\}$$_{k \\in \\mathcal{K}}$ be this subsequence. If $\\{s_k\\}_{k \\in \\mathcal{K}}$ is unbounded then $s^* = +\\infty$ contradicting the fact that there exists an $x \u0026gt; s^*$. And if $\\{s_k\\}_{k \\in \\mathcal{K}}$ is bounded that it contains a convergent subsequence (Bolzano‚ÄìWeierstrass theorem). Suppose this convergent subsequence converges to $y$. Then $y \\geq x \u0026gt; s^*$. This contradicts the definition of $s^*$.\n  To show the uniqueness, suppose there are two distinct numbers, $p$ and $q$, which satisfy the two properties, and suppose $p \u0026lt; q$. Choose $x$ such that $p \u0026lt; x \u0026lt; q$. Since $p$ satisfies the second property, we have $s_n \u0026lt; x$ for $n \\geq N$. But then $q$ cannot satisfy the second property. $\\square$\nAn intuitive theorem:\nTheorem 2: If $s_n \\leq t_n$ for $n \\geq N$, where $N \\in \\N$ is fixed, then $$\\limi s_n \\leq \\limi t_n,$$ $$\\lims s_n \\leq \\lims t_n$$\nProof: Let $s^* = \\lims s_n$ and $t^* = \\lims t_n$. Suppose for the sake of contradiction $t^* \u0026lt; s^*$. Choose $x$ such that $t^* \u0026lt; x \u0026lt; s^*$. Then by the second property of Theorem 1 there is an integer $N_1$ such that $n \\geq N_1$ implies $t_n \u0026lt; x$. Also by the first property there exists a subsequence $\\snk$ such that $s_{n_k} \\to s^*$. This implies that there exists an integer $N_2$ such that $n \\geq N_2$ implies $x \u0026lt; s_n$. But then for $n \\geq \\max\\{N_1, N_2\\}$ we have $t_n \u0026lt; x \u0026lt; s_n$. This gives us our required contradiction.\nA similar argument can be made for the $\\liminf$ case. $\\square$\nNext we give a necessary and sufficient condition for the convergence of a sequence in terms of its $\\liminf$ and $\\limsup$.\nTheorem 3: For a real-valued sequence $\\sn$, $\\limm s_n = s \\in \\RR$ if and only if $$\\lims s_n = \\limi s_n = s$$\nProof: We divide the analysis into three cases.\nFirst, let $s \\in \\R$. Then if $\\lims s_n = \\limi s_n = s$, Theorem 1 implies that for any $\\e \u0026gt; 0$ we have $s_n \\in (s-\\e, s+\\e)$ for all but finitely many $n$, which means $s_n \\to s$. On the other hand if $s_n \\to s$ then every subsequence $\\snk$ must converge to $s$ and hence $\\lims s_n = \\limi s_n = s$.\nNow let $s = +\\infty$. Then $s_n \\to s$, i.e., for every $M \\in \\R$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\geq M$ if and only $\\limi s_n = +\\infty$, and then $\\lims s_n = +\\infty$ since $\\limi s_n \\leq \\lims s_n$.\nLastly, let $s = -\\infty$. Then $s_n \\to s$, i.e., for every $M \\in \\R$ there is an integer $N$ such that $n \\geq N$ implies $s_n \\leq M$ if and only $\\lims s_n = -\\infty$, and then $\\limi s_n = -\\infty$ since $\\limi s_n \\leq \\lims s_n$. $\\square$\nUpper and lower limits - a reprise There is an equivalent way to express upper and lower limits.\nDefinition 6: Let $\\sn$ be a sequence of real numbers. We define the notation $$\\sups := \\sup \\{ s_k : k \\geq n\\}$$ $$\\infs := \\inf \\{ s_k : k \\geq n\\}$$\nWe note that the sequence $\\{ \\sups \\}$$_{n \\in \\N}$ is monotonically decreasing and the sequence $\\{ \\infs \\}$$_{n \\in \\N}$ is monotonically increasing, and thus their limits exist in $\\RR$.\nWe now show the equivalence of the two ways of looking at upper and lower limits.\nTheorem 4: Let $\\sn$ be a sequence of real numbers. Then $$\\limm \\sups = \\lims s_n$$ $$\\limm \\infs = \\limi s_n$$\nProof: We will prove the first equation. The proof for the second is similar. We prove the equation in two steps.\nLet $S = \\{ s_n : n \\in \\N \\}$. Let\u0026rsquo;s show that $\\sup S = +\\infty$ if and only if $\\lims s_n = + \\infty$. Suppose first that $\\sup S = + \\infty$. Then we construct a subsequence $\\snk$ as follows. We let $n_1 = 1.$ Suppose $n_1, \\ldots, n_{k}$ are chosen and let $$S_k = \\{ n \\in \\N : s_n \\geq \\max\\{s_{n_1}, \\ldots, s_{n_k}, k\\} + 1 \\}$$ Notice that $S_k$ is infinite as otherwise we can find an $M \\in \\R$ such that $s_n \\leq M$ for all $n \\geq 1$, contradicting the fact that $\\sup S = + \\infty$. We pick $n_{k+1}$ to be the smallest element of $S_k$ which is bigger than $n_k$. The resulting subsequence satisfies the condition that $s_{n_k} \\geq k$ for $k \\geq 2$ and thus we conclude that $s_{n_k} \\to +\\infty$ which gives $\\lims s_n = +\\infty$. Now suppose that $\\lims s_n = +\\infty$. From Theorem 1 we can conclude that there exists a subsequence $\\snk$ such that $s_{n_k} \\to +\\infty$. This immediately implies $\\sup S = +\\infty$.\nFor the second step, suppose $\\sup S \u0026lt; + \\infty$. Let $$a_n = \\sups$$ Notice that $a_1 \u0026lt; +\\infty$ and $\\{a_n\\}$ is a monotonically decreasing sequence. Therefore, we have that either $\\{a_n\\}$ is lower bounded in which case it converges to, say, $a$ or it is not in which case $\\limm a_n = -\\infty$. Since $s_n \\leq a_n$, by Theorem 2 we can conclude $$ \\lims s_n \\leq \\lims a_n = \\limm a_n $$ where the last equality follows from Theorem 3. We will now show that $$ \\lims s_n \\geq \\limm a_n $$ which will give us our required equality. If $\\limm a_n = -\\infty$ there is nothing to prove and so we assume that $a \u0026gt; -\\infty$. Let $\\e \u0026gt; 0$ be given and let $$ B = \\{ n \\in \\N : s_n \\geq a - \\e \\} $$ We claim that $B$ is infinite. Indeed, if $B$ were finite we can find $N \\in \\N$ so that $N \\geq \\max(B)$. This will imply that $s_n \\leq a - \\e$ for all $n \\geq N$ and so $a_n \\leq a - \\e$ for $n \\geq N$. But then by Theorem 2 we would conclude $a = \\limm a_n \\leq a-\\e$, which is absurd. Thus $B$ is infinite and we let $\\snk$ be a subsequence of $\\sn$ with $n_k \\in B$. Notice that $\\lims s_{n_k} \\leq \\lims s_n$, since any subsequential limit of $\\snk$ is also a subsequential limit of $\\sn$. This along with Theorem 2 give $$ a - \\e \\leq \\lims s_{n_k} \\leq \\lims s_n $$ Since $\\e$ was arbitrary we conclude that $\\lims s_n \\geq a$, which is what we wanted. $\\square$\nMore properties These theorems open new avenues to discover more properties of upper and lower limits.\nTheorem 5: Let $\\sn$ be a sequence of real numbers. Then $$\\limi s_n = - \\lims (-s_n)$$\nProof: TODO\nSubadditivity of $\\limsup$:\nTheorem 6: For any two real sequences $\\{a_n\\}$ and $\\{b_n\\}$, $$ \\lims (a_n + b_n) \\leq \\lims a_n + \\lims b_n $$ provided the sum on the right is not of the form $\\infty - \\infty$.\nProof: If $\\lims a_n = \\infty$ then as by assumption the right side is not $\\infty - \\infty$, it is $\\infty$ and there is nothing to prove. Similarly for the case $\\lims b_n = \\infty$. We may thus assume that $$ \\lims a_n = A \u0026lt; \\infty \\text{ and } \\lims b_n = B \u0026lt; \\infty $$ We note that $\\sup_{k \\geq 1} a_k \u0026lt; \\infty$, $\\sup_{k \\geq 1} b_k \u0026lt; \\infty$, and so TODO\nSuperadditivity of $\\liminf$:\nTheorem 7: For any two real sequences $\\{a_n\\}$ and $\\{b_n\\}$, $$ \\limi (a_n + b_n) \\geq \\limi a_n + \\limi b_n $$ provided the sum on the right is not of the form $\\infty - \\infty$.\nProof: TODO\n","date":1592426033,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592426033,"objectID":"4fb8d43db098b4713ceef892d1d72774","permalink":"https://makkar.github.io/post/upper-and-lower-limits/","publishdate":"2020-06-17T16:33:53-04:00","relpermalink":"/post/upper-and-lower-limits/","section":"post","summary":"A reference for a useful concept.","tags":[],"title":"Upper and Lower Limits","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"c7634463bc503571b81622dfbdaae48f","permalink":"https://makkar.github.io/hilbert/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/hilbert/","section":"","summary":"Blog","tags":null,"title":"Hilbert's Hotel","type":"widget_page"},{"authors":null,"categories":null,"content":"Title: Bayesian nonparametric ensemble Date: June 17, 2020 Tags: bayesian Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).\nIntroduction $$ \\mathbb{E}[X] = \\int_{\\Omega} X ;\\text{dP} $$\nProblem Setting BNE ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"79a589979f0bc90d50d8da1239fec4e1","permalink":"https://makkar.github.io/research/bayesian-ensemble/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/bayesian-ensemble/","section":"research","summary":"Title: Bayesian nonparametric ensemble Date: June 17, 2020 Tags: bayesian Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).","tags":null,"title":"","type":"research"}]