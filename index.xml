<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aditya Makkar</title>
    <link>https://makkar.github.io/</link>
      <atom:link href="https://makkar.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Aditya Makkar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 05 Feb 2021 17:24:39 -0500</lastBuildDate>
    <image>
      <url>https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_512x512_fill_lanczos_center_2.png</url>
      <title>Aditya Makkar</title>
      <link>https://makkar.github.io/</link>
    </image>
    
    <item>
      <title>A note on conditional probability</title>
      <link>https://makkar.github.io/post/regularcondprob/</link>
      <pubDate>Fri, 05 Feb 2021 17:24:39 -0500</pubDate>
      <guid>https://makkar.github.io/post/regularcondprob/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Prob}[1]{\PP \left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 \; : \; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 ; | ; #2 \right\}}
\newcommand{\Ex}[1]{\E\left(#1\right)}
\newcommand{\Exc}[2]{\E\left(#1 \mid #2\right)}
\newcommand{\Pc}[2]{\PP\left( \left. #1 \, \right\vert \, #2\right)}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ph}[1]{\varphi^{-1}\left(#1\right)}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathrm{Var}}
\DeclareMathOperator{\Log}{\mathrm{Log}}
\DeclareMathOperator{\O}{\mathcal{O}}
$$&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The concept of conditional probability is central to probability theory and excellent treatment of it can be found in many books. My aim with this blog post is to consolidate in one place some ideas around it which helped me form a better intuition. These ideas will be useful if you have already been exposed to this concept from a textbook and just want one more person&amp;rsquo;s ramblings about it.&lt;/p&gt;
&lt;p&gt;I will start by defining conditional expectation and stating some of its properties. It will be a grave injustice to claim my discussion of it is complete since I don&amp;rsquo;t even prove its existence; this section exists solely for establishing notation. I will then spend some time discussing conditional probability, relating it to the traditional notion of&lt;/p&gt;
&lt;p&gt;$$
\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
$$&lt;/p&gt;
&lt;p&gt;These discussions will naturally lead to the notions of regular conditional probability and regular conditional distribution which I discuss next.&lt;/p&gt;
&lt;h1 id=&#34;conditional-expectation&#34;&gt;Conditional expectation&lt;/h1&gt;
&lt;p&gt;Recall the concept of conditional expectation.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 1: &lt;/span&gt; Let $\probsp$ be a probability space, and $X$ a random variable with $\Ex{|X|} &amp;lt; \infty$. Let $\G$ be a sub-$\sigma$-algebra of $\F$. Then there exists a random variable $Y$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Y$ is $\G$ measurable,&lt;/li&gt;
&lt;li&gt;$\Ex{|Y|} &amp;lt; \infty$,&lt;/li&gt;
&lt;li&gt;$\int_G Y \dP = \int_G X \dP$ for every $G \in \G$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Remarks: &lt;/span&gt; (1.) It is easy to see from the $\pi-\lambda$ theorem that the last condition can be relaxed such that $\int_G Y \dP = \int_G X \dP$ for every $G$ in some $\pi$-system which contains $\Omega$ and generates $\G$. (2.) If $Y&#39;$ is another random variable with the three properties above then $Y&amp;rsquo; = Y$ a.s.. Therefore, $Y$ in the theorem above is called a &lt;em&gt;version&lt;/em&gt; of the conditional expectation. The notation $\Exc{X}{\G}$ is used to denote this unique (up to a.e. equivalence) random variable.&lt;/p&gt;
&lt;p&gt;The proof of this standard theorem can be found in any probability text. See [1,2] for example.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 1: &lt;/span&gt; In the setting of Theorem 1, if $Z$ is a random variable, we write $\Exc{X}{Z}$ for $\Exc{X}{\sigma(Z)}$.&lt;/p&gt;
&lt;p&gt;The fact that conditional expectation is defined as a random variable might come as surprising, but the correspondence with the traditional usage of conditional expectation as a number becomes clear once you realize that here we are conditioning on a $\sigma$-algebra instead of a single event. For example, consider the life expectancy of a new born baby conditioned on sex. This is a random variable that takes one value for males and another value for females.&lt;/p&gt;
&lt;h2 id=&#34;properties-of-conditional-expectation&#34;&gt;Properties of conditional expectation&lt;/h2&gt;
&lt;p&gt;For completeness I state some useful properties of conditional expectation. You can find the proofs in [1,2] for example. Most of them are parallels to well-known properties of (unconditional) expectation. Assume that all the $X$&#39;s satisfy $\Ex{|X|} &amp;lt; \infty$ and let $\G, \cH$ be sub-$\sigma$-algebras of $\F$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[Linearity] $\Exc{a_1 X_1 + a_2 X_2}{\G} = a_1 \Exc{X_1}{\G} + a_2 \Exc{X_2}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Positivity] If $X \ge 0$, then $\Exc{X}{\G} \ge 0$ a.s.&lt;/li&gt;
&lt;li&gt;[Monotone convergence theorem for conditional expectation] If $\Ex{|Y|} &amp;lt; \infty$ and $Y \le X_n \uparrow X$ a.s., then $\Exc{X_n}{\G} \uparrow \Exc{X}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Fatou&amp;rsquo;s lemma for conditional expectation] If $\Ex{|Y|} &amp;lt; \infty$ and $Y \le X_n$ for all $n \ge 1$ a.s., then $\Exc{\limi X_n}{\G} \le \limi \Exc{X_n}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Dominated convergence theorem for conditional expectation] If $|X_n| \le |Y|$ for all $n \ge 1$, $\Ex{Y} &amp;lt; \infty$, and $X_n \to X$ a.s., then $\Exc{X_n}{\G} \to \Exc{X}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Tower property] If $\cH \subseteq \G$, then $\Exc{\Exc{X}{\G}}{\cH} = \Exc{\Exc{X}{\cH}}{\G} = \Exc{X}{\cH}$ a.s.&lt;/li&gt;
&lt;li&gt;[Taking out what&amp;rsquo;s known] If $Y$ is $\G$-measurable and bounded, then
$$\Exc{Y X}{\G} = Y \Exc{X}{\G} \quad \text{a.s.}\tag{1} \label{eq1}$$
If $p &amp;gt; 1$, $1/p + 1/q = 1$, $X \in L^p(\Omega, \F, \PP)$ and $Y \in L^q(\Omega, \G, \PP)$, then \eqref{eq1} again holds. If $X$ is a nonnegative $\F$-measurable random variable, $Y$ is a nonnegative $\G$-measurable random variable, $\Ex{X} &amp;lt; \infty$ and $\Ex{XY} &amp;lt; \infty$, then \eqref{eq1} again holds.&lt;/li&gt;
&lt;li&gt;[Role of independence] If $\cH$ is independent of $\sigma(\sigma(X), \G)$, then $\Exc{X}{\sigma(\G, \cH)} = \Exc{X}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;conditional-probability&#34;&gt;Conditional probability&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 2: &lt;/span&gt; In the setting of Theorem 1, if $A \in \F$, we let $\Pc{A}{\G}$ to mean $\Exc{\ind{A}}{\G}$ and call it the &lt;em&gt;conditional probability of&lt;/em&gt; $A$ &lt;em&gt;given&lt;/em&gt; $\G.$ Here $\ind{A}$ is the indicator random variable. If $B \in \F$, we let $\Pc{A}{B}$ to mean $\Exc{\ind{A}}{\ind{B}}$.&lt;/p&gt;
&lt;p&gt;Just like conditional expectation, conditional probability, as defined above, is a random variable! Unlike conditional expectation this isn&amp;rsquo;t very palpable and deserves more rumination [3]. We have our probability space $\probsp$ and let $A,B \in \F$ be such that $\PP(B) \neq 0$ and $\PP(B^\comp) \neq 0$. Then our traditional notion of conditional probability tells us that the conditional probability of $A$ given $B$ is defined by&lt;/p&gt;
&lt;p&gt;$$
\PP_B(A) = \frac{\PP(A \cap B)}{\PP(B)}
$$&lt;/p&gt;
&lt;p&gt;Let us investigate how $\PP_B(A)$ depends on $B$. To this end, introduce the discrete measurable space $(\Lambda, 2^\Lambda)$ with $\Lambda = \se{\lambda_1, \lambda_2}$, and a measurable mapping $T \colon \Omega \to \Lambda$ such that&lt;/p&gt;
&lt;p&gt;$$
T(\omega) = \begin{cases}
\lambda_1 &amp;amp; \text{ if } \omega \in B   \\&lt;br&gt;
\lambda_2 &amp;amp; \text{ if } \omega \in B^\comp
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Define the two measures $\nu_A$ and $\nu$ on $(\Lambda, 2^\Lambda)$ as follows for any $E \subseteq \Lambda$&lt;/p&gt;
&lt;p&gt;$$
\nu_A(E) = \Prob{A \cap T^{-1}(E)}
$$&lt;/p&gt;
&lt;p&gt;$$
\nu(E) = \Prob{T^{-1}(E)}
$$&lt;/p&gt;
&lt;p&gt;Then it is easy to see that&lt;/p&gt;
&lt;p&gt;$$
\PP_B(A) = \frac{\nu_A(\se{\lambda_1})}{\nu(\se{\lambda_1})}
$$&lt;/p&gt;
&lt;p&gt;$$
\PP_{B^\comp}(A) = \frac{\nu_A(\se{\lambda_2})}{\nu(\se{\lambda_2})}
$$&lt;/p&gt;
&lt;p&gt;In other words conditional probability may be viewed as a measurable function on $\Lambda$. This can easily be generalized to any finite setting as follows. Let $\se{A_1, \ldots, A_n} \subseteq \F$ be a partition of $\Omega$, i.e., $A_i \cap A_j = \emp$ for $i \neq j$ and $\bigcup_i A_i = \Omega$. Introduce the discrete measurable space $(\Lambda, 2^\Lambda)$ with $\Lambda = \se{\lambda_1, \ldots, \lambda_n}$. Define a measurable mapping $T \colon \Omega \to \Lambda$ such that $T(\omega) = \lambda_i$ whenever $\omega \in A_i$. Define the measures $\nu_{A_1}, \ldots, \nu_{A_n}, \nu$ on $(\Lambda, 2^\Lambda)$ as follows for any $E \subseteq \Lambda$&lt;/p&gt;
&lt;p&gt;$$
\nu_{A_i}(E) = \Prob{A_i \cap T^{-1}(E)} \quad \text{for all } i = 1, \ldots, n
$$&lt;/p&gt;
&lt;p&gt;$$
\nu(E) = \Prob{T^{-1}(E)}
$$&lt;/p&gt;
&lt;p&gt;Then once again we have for any $A \in \F$&lt;/p&gt;
&lt;p&gt;$$
\PP_{A_i}(A) = \frac{\PP(A \cap A_i)}{\PP(A_i)} = \frac{\nu_{A_i}(\se{\lambda_i})}{\nu(\se{\lambda_i})} \quad \text{for all } i = 1, \ldots, n
$$&lt;/p&gt;
&lt;p&gt;These considerations are what motivated the definition of conditional probability in general cases as you see in Definition 2. If $T$ is any measurable mapping from $\probsp$ into a measurable space $(\Lambda, \LL)$, and if we write $\nu_A(E) = \Prob{A \cap T^{-1}(E)}$ where $A \in \F$ and $E \in \LL$, then it is clear that $\nu_E$ and $\PP \circ T^{-1}$ are measures on $\LL$ such that $\nu_A \ll \PP \circ T^{-1}$. Radon-Nikodym theorem now implies that there exists an $\PP \circ T^{-1}$-integrable function $p_A$, unique upto $\PP \circ T^{-1}$-a.e., such that&lt;/p&gt;
&lt;p&gt;$$
\Prob{A \cap T^{-1}(E)} = \int_E p_A(\lambda) \; \PP \circ T^{-1}(\dd \lambda) \quad \text{for all } E \in \LL
$$&lt;/p&gt;
&lt;p&gt;We anoint $p_A(\lambda)$ as the conditional probability of $A$ given $\lambda$ or the conditional probability of $A$ given that $T(\omega) = \lambda$. Note that here we are conditioning on a measurable mapping $T$ instead of a sub-$\sigma$-algebra, but this notion is related to conditioning on $\sigma(T)$ as will become clear ahead. Keep this &amp;ldquo;rumination&amp;rdquo; in mind when we discuss regular conditional distribution later.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at our definition of conditional probability from the other direction and show that $\Pc{A}{B}$ as defined in Definition 2 conforms to our traditional usage. To start, note that $\sigma(\ind{B}) = \se{\emp, B, B^\comp, \Omega}$, and since $\Pc{A}{B}$ is $\sigma(\ind{B})$ measurable, it must be constant on each of the sets $B, B^\comp$, thereby necessitating&lt;/p&gt;
&lt;p&gt;$$
\Pc{A}{B}(\omega) = \begin{cases}
\frac{\PP(A \cap B)}{\PP(B)} &amp;amp; \text{ if } \omega \in B   \\&lt;br&gt;
\frac{\PP(A \cap B^\comp)}{\PP(B^\comp)} &amp;amp; \text{ if } \omega \in B^\comp
\end{cases}
$$
because of property 3. of Theorem 1 by taking $G$ to be $B$ and $B^\comp$. Of course, if any of the sets $B$ or $B^\comp$ is of measure $0$, then you can take the corresponding value for $\Pc{A}{B}(\omega)$ to be anything in $[0,1]$ and it won&amp;rsquo;t matter since the concept of conditional probability is defined up to sets of measure $0$.&lt;/p&gt;
&lt;h2 id=&#34;properties-of-conditional-probability&#34;&gt;Properties of conditional probability&lt;/h2&gt;
&lt;p&gt;Positivity property (property 2. above) and monotone convergence property (property 3. above) imply $0 \le \Pc{A}{\G} \le 1$ a.s. for any $A \in \F$, $\Pc{A}{\G} = 0$ a.s. if and only if $\PP(A) = 0$, and $\Pc{A}{\G} = 1$ a.s. if and only if $\PP(A) = 1.$&lt;/p&gt;
&lt;p&gt;Let $A_1, A_2, \ldots \in \F$ be a sequence of disjoint sets. By linearity (property 1. above) and monotone convergence theorem for conditional expectation, we see that&lt;/p&gt;
&lt;p&gt;$$
\Pc{\bigcup_n A_n}{\G} = \sum_n \Pc{A_n}{\G} \quad \text{a.s.} \tag{2} \label{eq2}
$$&lt;/p&gt;
&lt;p&gt;If $A_n \in \F$, $n \ge 1$ and $\limn A_n = A$, then we also have $\limn \Pc{A_n}{\G} = \Pc{A}{G}$ a.s..&lt;/p&gt;
&lt;p&gt;It seems very tempting from the foregoing discussion to claim that $\Pc{\cdot}{\G}$ is a probability measure on $\F$ for almost all $\omega \in \Omega$, but except for some nice spaces, which we will discuss below, this isn&amp;rsquo;t true. Let us first try to see this intuitively [2]. Equation \eqref{eq2} holds for all $\omega \in \Omega$ EXCEPT for some null set which may well depend on the particular sequence $\se{A_n}$.  It does NOT stipulate that there exists a fixed null set $N \in \F$ such that&lt;/p&gt;
&lt;p&gt;$$
\Pc{\bigcup_n A_n}{\G}(\omega) = \sum_n \Pc{A_n}{\G}(\omega), \quad \omega \in N^\comp
$$&lt;/p&gt;
&lt;p&gt;for every disjoint sequence $\se{A_n} \subseteq \F$. Except in trivial cases, there are uncountably many disjoint sequences, and therefore we will need uncountable union of such null sets to be of measure $0$, which of course may not even be defined let alone be of measure $0$. To further drive this point home you can take a look at an explicit example of how this can fail in an exercise in [3] page 210.&lt;/p&gt;
&lt;h1 id=&#34;regular-conditional-probability&#34;&gt;Regular conditional probability&lt;/h1&gt;
&lt;p&gt;Motivated by our discussion above, we define regular conditional probability as follows.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 3: &lt;/span&gt; Let $\probsp$ be a probability space and $\G, \cH$ be sub-$\sigma$-algebras of $\F$. A &lt;em&gt;regular conditional probability&lt;/em&gt; on $\cH$ given $\G$ is a function $\PP(\cdot, \cdot) \colon \cH \times \Omega \to [0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;for a.e. $\omega \in \Omega$, $\PP(\cdot, \omega)$ is a probability measure on $\cH$,&lt;/li&gt;
&lt;li&gt;for each $A \in \cH$, $\PP(A, \cdot)$ is a $\G$-measurable function on $\Omega$ coinciding with the conditional probability of $A$ given $\G$, i.e., $\PP(A, \cdot) = \Pc{A}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s show that this definition is not outrageous by showing that it agrees with our traditional notion of conditional pdf and conditional expectation. So suppose that $X$ and $Y$ are random variables which have a joint probability density function $f_{X,Y}(x,y).$ This means that we are considering the probability space $(\R^2, \B(\R^2), \PP)$ with $X$ and $Y$ being the coordinate random variables, i.e. $(x,y) \mapsto x$ and $(x,y) \mapsto y$ respectively, and having an absolutely continuous distribution function $F_{X,Y}(x,y)$ such that&lt;/p&gt;
&lt;p&gt;$$
F_{X,Y}(x,y) = \int_{-\infty}^y \int_{\infty}^x f_{X,Y}(s,t) \; \dd s \, \dd t
$$&lt;/p&gt;
&lt;p&gt;We recall that $f_X(x) = \int_\R f_{X,Y}(x,y) \; \dd y$ and $f_Y(y) = \int_\R f_{X,Y}(x,y) \; \dd x$ act as probability density functions for $X$ and $Y$ respectively, and&lt;/p&gt;
&lt;p&gt;$$
f_{X \mid Y}(x \mid y) = \begin{cases}
\frac{f_{X,Y}(x,y)}{f_Y(y)} &amp;amp; \text{ if } f_Y(y) \neq 0   \\&lt;br&gt;
0 &amp;amp; \text{ otherwise}
\end{cases}
$$
defines the elementary conditional pdf $f_{X \mid Y}$ of $X$ given $Y$. By Fubini&amp;rsquo;s theorem $f_X$ and $f_Y$ are Borel functions on $\R$ and so $f_{X \mid Y}$ is a Borel function on $\R^2$. Let $\cH = \B(\R^2) = \sigma(X, Y)$ and $\G = \sigma(Y) = \R \times \B(\R)$. For $A \in \cH$ and $\omega = (x,y) \in \R^2$ we define&lt;/p&gt;
&lt;p&gt;$$
\PP(A, \omega) = \int_{\set{s}{(s,y) \in A}} f_{X \mid Y}(s \mid y) \; \dd s
$$&lt;/p&gt;
&lt;p&gt;Then for each $\omega \in \R^2$, $\PP(\cdot, \omega)$ is a probability measure on $\cH$, and for each $A \in \cH$, $\PP(A, \cdot)$ is a Borel function in $y$ and hence $\G$-measurable. To verify that $\PP(A, \cdot) = \Pc{A}{\G}$ for any $A \in \cH$ we just need to verify property 3. of Theorem 1. To this end, fix $A \in \cH$  and $G \in \G$, and note that $G$ must be of the form $G = \R \times B$ for $B \in \B(\R)$. Thus&lt;/p&gt;
&lt;p&gt;\begin{align*}
\int_G \PP(A, \omega) \, \dP(\omega) &amp;amp;= \int_B \int_R \PP(A, (s,t)) f_{X \mid Y}(s,t) \; \dd s \, \dd t \quad \text{(by absolute continuity and Fubini&amp;rsquo;s theorem)} \\&lt;br&gt;
&amp;amp;= \int_B \int_R \left[ \int_{\set{u}{(u,t) \in A}} f_{X \mid Y}(u \mid t) \; \dd u \right] f_{X \mid Y}(s,t) \; \dd s \, \dd t \\&lt;br&gt;
&amp;amp;= \int_B \left[ \int_{\set{u}{(u,t) \in A}} f_{X \mid Y}(u \mid t) \; \dd u \right] f_{Y}(t) \; \dd t \\&lt;br&gt;
&amp;amp;= \int_B \int_{\set{u}{(u,t) \in A}} f_{X, Y}(u, t) \; \dd u \, \dd t \\&lt;br&gt;
&amp;amp;= \int_B \int_{\R} \ind{A}(u,t) f_{X, Y}(u, t) \; \dd u \, \dd t \\&lt;br&gt;
&amp;amp;= \int_{G} \ind{A}(\omega) \; \dP(\omega)
\end{align*}&lt;/p&gt;
&lt;p&gt;and so $\PP(A, \cdot) = \Exc{\ind{A}}{\G} = \Pc{A}{\G}$. Hence, $\PP(A, \omega)$ is a regular probability measure on $\cH$ given $\G$.&lt;/p&gt;
&lt;p&gt;For the corresponding analysis for conditional expectation, let $h$ be a Borel function on $\R^2$ such that $\Ex{|h(X,Y)|} = \int_\R \int_\R |h(x,y)| f_{X,Y}(x,y) \; \dd x \, \dd y &amp;lt; \infty$. Set&lt;/p&gt;
&lt;p&gt;$$
g(y) = \int_\R h(s, y) f_{X \mid Y}(s \mid y) \; \dd s
$$&lt;/p&gt;
&lt;p&gt;$g(y)$ is the traditional conditional density of $h(X, Y)$ given $Y = y$.
Then the claim is that $g(Y) = \Exc{h(X,Y)}{\sigma(Y)}$ a.s.. The typical element of $\sigma(Y)$ has the form $\set{\omega \in \R^2}{Y(\omega) \in B}$, where $B \in \B(\R).$ Hence, we must show that&lt;/p&gt;
&lt;p&gt;$$
L = \Ex{h(X,Y) \ind{B}(Y)} = \Ex{g(Y) \ind{B}(Y)} = R
$$&lt;/p&gt;
&lt;p&gt;But we can write $L$ and $R$ as&lt;/p&gt;
&lt;p&gt;\begin{align*}
L &amp;amp;= \int \int h(x,y) \ind{B}(y) f_{X,Y}(x,y) \; \dd x \, \dd y \\&lt;br&gt;
R &amp;amp;= \int g(y) \ind{B}(y) f_Y(y) \; \dd y
\end{align*}&lt;/p&gt;
&lt;p&gt;and they are equal by Fubini&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;In general, we have the following useful theorem (taken from [2]) which allows us to view conditional expectations as ordinary expectations relative to the measure induced by regular conditional probability.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 2: &lt;/span&gt; Consider the setting of Definition 3 and denote $\PP_\omega(\cdot) = \PP(\cdot, \omega)$. Let $X$ be an $\cH$-measurable function with $\Ex{X} &amp;lt; \infty$. Then&lt;/p&gt;
&lt;p&gt;$$
\Exc{X}{\G}(\omega) = \int_\Omega X \; \dP_\omega \quad \text{a.s.} \tag{3} \label{eq3}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Recall the monotone class theorem for functions:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; Let $\mathscr{H}$ be a family of nonnegative functions on $\Omega$ which contains all indicators of sets of some class $\cH$ of subsets of $\Omega$. If either (i) $\cH$ is a $\pi$-class and $\mathscr{H}$ is a $\lambda$-system, or (ii) $\cH$ is a $\sigma$-algebra and $\mathscr{H}$ is a monotone system, then $\mathscr{H}$ contains all nonnegative $\sigma(\cH)$-measurable functions.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By separate considerations of $X^+$ and $X^-$, it may be supposed that $X \ge 0$. Let&lt;/p&gt;
&lt;p&gt;$$
\mathscr{H} = \set{X}{X \ge 0, X \text{ is } \cH \text{-measurable, and \eqref{eq3} holds for } X}
$$&lt;/p&gt;
&lt;p&gt;By the definition of regular conditional probability, $\ind{A} \in \mathscr{H}$ for $A \in \cH$. $\cH$ is a $\sigma$-algebra. Let&amp;rsquo;s show that $\mathscr{H}$ is a monotone system. If $X_1, X_2 \in \mathscr{H}$ and $c_1, c_2 \ge 0$, then $c_1 X_1 + c_2 X_2 \ge 0$, $c_1 X_1 + c_2 X_2$ is $\cH$-measurable and Equation \eqref{eq3} holds because of linearity of expectation and conditional expectation, and thus $c_1 X_1 + c_2 X_2 \in \mathscr{H}$. If $\se{X_n} \subseteq \mathscr{H}$ such that $X_n \uparrow X$, then $X \ge 0$, $X$ is $\cH$-measurable, and Equation \eqref{eq3} holds for $X$ because of monotone convergence theorem for expectation and conditional expectation, and thus $X \in \mathscr{H}$. Therefore, by the monotone class theorem $\mathscr{H}$ contains all nonnegative $\cH$-measurable functions. $\square$&lt;/p&gt;
&lt;h1 id=&#34;regular-conditional-distribution&#34;&gt;Regular conditional distribution&lt;/h1&gt;
&lt;p&gt;In some cases even the concept of regular conditional probability in inadequate, and that motivates the concept of regular conditional distributions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 4: &lt;/span&gt; Let $\probsp$ be a probability space, $\G \subseteq \F$ a $\sigma$-algebra, $(\Lambda, \LL)$ a measurable space, and $T \colon \Omega \to \Lambda$ a measurable mapping. A &lt;em&gt;regular conditional distribution&lt;/em&gt; for $T$ given $\G$ is a function $\PP_T \colon \LL \times \Omega \to [0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;for a.e. $\omega \in \Omega$, $\PP_T(\cdot, \omega)$ is a probability measure on $\LL$,&lt;/li&gt;
&lt;li&gt;for each $A \in \LL$, $\PP_T(A, \cdot)$ is a $\G$-measurable function on $\Omega$ coinciding with the conditional probability of $T^{-1}(A)$ given $\G$, i.e., $\PP_T(A, \cdot) = \Pc{T^{-1}(A)}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is clear that when $\Lambda = \Omega$, $\LL = \cH \subseteq \F$ and $T$ is the identity map, $\PP_T$ is exactly the regular conditional probability as defined in Definition 3.&lt;/p&gt;
&lt;p&gt;Now would be a good time to read the first &amp;ldquo;rumination&amp;rdquo; in the section Conditional Probability and realize that the definition of regular conditional distribution is in fact well motivated.&lt;/p&gt;
&lt;p&gt;A corresponding version of Theorem 2 exists, proof of which I&amp;rsquo;ll leave as an easy exercise:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 3: &lt;/span&gt; In the setting of Definition 4, if $\PP_T^\omega(A) = \PP_T(A, \omega)$ and $h \colon \Lambda \to \R$ is a Borel function with $\Ex{|h(T)|} &amp;lt; \infty$, then&lt;/p&gt;
&lt;p&gt;$$
\Exc{h(T)}{\G}(\omega) = \int_\Lambda h(\lambda) \; \PP_T^\omega(\dd \lambda)
$$&lt;/p&gt;
&lt;p&gt;To see the power of thinking about conditional probabilities like this, let&amp;rsquo;s give an unbelievably short proof of conditional Hölder&amp;rsquo;s inequality [2]. Contrast it with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality#Conditional_H%C3%B6lder_inequality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other proofs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 4: &lt;/span&gt; If $X,Y$ are random variables on $\probsp$, $\G \subseteq \F$ is a $\sigma-$algebra and $ 1 &amp;lt; p &amp;lt; \infty$, $1/p + 1/q = 1$, then&lt;/p&gt;
&lt;p&gt;$$
\Exc{|XY|}{\G} \le \left( \Exc{|X|^p}{\G} \right)^{1/p} \left( \Exc{|Y|^q}{\G} \right)^{1/q} \quad \text{a.s.}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; For $B \in \B(\R^2)$ and $\omega \in \Omega$, let $\PP_{X,Y}^\omega(B) = \PP_{X,Y}(B, \omega)$ be the regular conditional distribution for $(X,Y)$ given $\G$. Theorem 3 allows us to write&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Exc{|XY|}{\G}(\omega) &amp;amp;= \int_{\R^2} |x y| \; \PP_{X,Y}^\omega(\dd (x,y)) \\&lt;br&gt;
\left( \Exc{|X|^p}{\G} \right)^{1/p} &amp;amp;= \left( \int_{\R^2} |x|^p \; \PP_{X,Y}^\omega(\dd (x,y)) \right)^{1/p} \\&lt;br&gt;
\left( \Exc{|Y|^q}{\G} \right)^{1/q} &amp;amp;= \left( \int_{\R^2} |y|^q \; \PP_{X,Y}^\omega(\dd (x,y)) \right)^{1/q}
\end{align*}&lt;/p&gt;
&lt;p&gt;And now our desired inequality follows immediately from the ordinary Hölder&amp;rsquo;s inequality. $\square$&lt;/p&gt;
&lt;h1 id=&#34;existence-of-regular-conditional-distribution&#34;&gt;Existence of regular conditional distribution&lt;/h1&gt;
&lt;p&gt;Before we discuss their existence, let us define the concept of standard Borel space [6].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 5: &lt;/span&gt; Let $(X, \X)$ and $(Y, \Y)$ be measurable spaces. They are called &lt;em&gt;isomorphic&lt;/em&gt; is there exists a bijection $f \colon X \to Y$ such that $f$ and $f^{-1}$ are both measurable. The function $f$ is called an &lt;em&gt;isomorphism&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 6: &lt;/span&gt; A measurable space $(X, \X)$ is called a &lt;em&gt;standard Borel space&lt;/em&gt; if it satisfies any of the following equivalent conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$(X, \X)$ is isomorphic to some compact metric space with the Borel $\sigma$-algebra;&lt;/li&gt;
&lt;li&gt;$(X, \X)$ is isomorphic to some Polish space (i.e., a separable complete metric space) with the Borel $\sigma$-algebra;&lt;/li&gt;
&lt;li&gt;$(X, \X)$ is isomorphic to some Borel subset of some Polish space with the Borel $\sigma$-algebra.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As you can guess most spaces we deal with are standard Borel spaces. Durrett [5] calls these space &lt;em&gt;nice&lt;/em&gt; since we already have too many things named after Borel. I am not sure I agree with his reasoning but I like Durrett&amp;rsquo;s terminology.&lt;/p&gt;
&lt;p&gt;The next two theorems show the existence of regular conditional distribution and are taken from [5, Section 4.1.3]. See also [4, Section V.8].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 5: &lt;/span&gt; Regular conditional distribution exists if $(\Lambda, \LL)$ is nice.&lt;/p&gt;
&lt;p&gt;A generalization of the last theorem:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 6: &lt;/span&gt; Suppose $(\Lambda, \LL)$ is a nice space, $T$ and $S$ are measurable mappings from $\Omega$ to $\Lambda$, and $\G = \sigma(S)$. Then there exists a function $\mu \colon \Lambda \times \LL \to [0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;for a.e. $\omega \in \Omega$, $\mu(S(\omega), \cdot)$ is a probability measure on $\LL$,&lt;/li&gt;
&lt;li&gt;for each $A \in \LL$, $\mu(S(\cdot), A) = \Pc{T^{-1}(A)}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is instructive to prove Theorem 5 in the special case when $(\Lambda, \LL) = (\R^n, \B(\R^n))$. The theorem and the proof is taken from [2].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 7: &lt;/span&gt; In the setting of Definition 4, let $(\Lambda, \LL) = (\R^n, \B(\R^n))$ and $T = (T_1, \ldots, T_n) \colon \Omega \to \R^n$. Then there exists a regular conditional distribution for $T$ given $\G$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Let&amp;rsquo;s recall the definition of an $n$-dimensional distribution function on $\R^n$ (the Russian convention of left-continuous distribution function).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; An $n$-dimensional distribution function on $\R^n$ is a function $F$ satisfying:
$$\lim_{x_j \to - \infty} F(x_1, \ldots, x_n) = 0, \quad 1 \le j \le n \tag{i}$$
$$\lim_{\substack{x_j \to \infty \\ 1 \le j \le n}} F(x_1, \ldots, x_n) = 1 \tag{ii}$$
$$\lim_{y_j \uparrow x_j} F(x_1, \ldots, x_{j-1}, y_j, x_{j+1}, \ldots, x_n) = F(x_1, \ldots, x_j, \ldots, x_n), \quad 1 \le j \le n \tag{iii}$$
\begin{align*}\Delta_n^{a,b} &amp;amp;:= F(b_1, \ldots, b_n) - \sum_{j=1}^n F(b_1, \ldots, b_{j-1}, a_j, b_{j+1}, \ldots, b_n) \\ &amp;amp;+ \sum_{1 \le j &amp;lt; k \le n} F(b_1, \ldots, b_{j-1}, a_j, b_{j+1}, \ldots, b_{k-1}, a_k, b_{k+1}, \ldots, b_n) - \cdots (-1)^n F(a_1, \ldots, a_n) \\ &amp;amp;\ge 0 \tag{iv}\end{align*}
&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will try to construct a distribution function on $\R^n$. To this end, for any rational number $r_1, \ldots, r_n$ and $\omega \in \Omega$, define&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(r_1, \ldots, r_n) = \Pc{\bigcap_{i=1}^n \se{T_i &amp;lt; r_i}}{\G}(\omega) \tag{4} \label{eq4}
$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s evident that the properties of conditional probability discussed above imply there is a null set $N \in \G$ such that for $\omega \in N^\comp$ and all rational numbers $r_i, r_i&amp;rsquo;, q_{i,m}$ the following holds&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(r_1, \ldots, r_n) \ge F_n^\omega(r_1&amp;rsquo;, \ldots, r_n&amp;rsquo;) \text{ if } r_i &amp;gt; r_i&amp;rsquo;, \, 1 \le i \le n
$$&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(r_1, \ldots, r_n) = \lim_{\substack{q_{i,m} \uparrow r_i \\ 1 \le i \le n}} F_n^\omega(q_{1, m}, \ldots, q_{n, m})
$$&lt;/p&gt;
&lt;p&gt;$$
\lim_{r_i \to -\infty} F_n^\omega(r_1, \ldots, r_n) = 0, \quad 1 \le i \le n
$$&lt;/p&gt;
&lt;p&gt;$$
\lim_{\substack{r_i \to \infty \\ 1 \le i \le n}} F_n^\omega(r_1, \ldots, r_n) = 1
$$&lt;/p&gt;
&lt;p&gt;$$
\Delta_n^{r, r&amp;rsquo;} F_n^\omega \ge 0 \text{ if } r \le r&amp;rsquo;
$$&lt;/p&gt;
&lt;p&gt;where $r \le r&#39;$ means $r_i \le r_i&#39;$ for all $1 \le i \le n$. Having defined $F_n^\omega$ for rational values, define for any real numbers $x_1, \ldots, x_n$ as follows&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(x_1, \ldots, x_n) = \begin{cases}
\lim_{\substack{r_i \uparrow x_i \\ r_i \in \Q \\ 1 \le i \le n}} F_n^\omega(r_1, \ldots, r_n) &amp;amp; \text{ if } \omega \in N^\comp   \\&lt;br&gt;
\Prob{\bigcap_{i=1}^n \se{T_i &amp;lt; r_i}} &amp;amp; \text{ if } \omega \in N
\end{cases} \tag{5} \label{eq5}
$$&lt;/p&gt;
&lt;p&gt;Then for each $\omega \in \Omega$, $F_n^\omega(x_1, \ldots, x_n)$ is an $n$-dimensional distribution function and hence determines a Lebesgue-Stieltjes measure $\mu_\omega$ on $\B(\R^n)$ with $\mu_\omega(\R^n) = 1$. For $B \in \B(\R^n)$ and $\omega \in \Omega$ define&lt;/p&gt;
&lt;p&gt;$$
\PP_T(B, \omega) = \mu_\omega(B)
$$&lt;/p&gt;
&lt;p&gt;If&lt;/p&gt;
&lt;p&gt;\begin{align*}
\cH &amp;amp;= \set{B \in \B(\R^n)}{\PP_T(B, \cdot) = \Pc{T^{-1}(B)}{\G} \text{ a.s.} } \\&lt;br&gt;
\D &amp;amp;= \set{B \in \B(\R^n)}{B = [-\infty, r_1) \times \cdots \times [-\infty, r_n), \, r_i \in \Q}
\end{align*}&lt;/p&gt;
&lt;p&gt;then a moment&amp;rsquo;s reflection will convince you that that $\cH$ is a $\lambda-$class, $\D$ is a $\pi-$class, and $\cH \subseteq \D$. Hence, by the $\pi-\lambda$ theorem $\cH \subseteq \sigma(\D) = \B(\R^n)$, or in other words, $\PP_T(B, \omega)$ is a regular conditional distribution for $T$ given $\G$. $\square$&lt;/p&gt;
&lt;p&gt;In fact, this theorem is easily extended to $(\R^\infty, \B(\R^\infty))$ as follows: For all $n \ge 1$, define $F_n^\omega$ as in Equation \eqref{eq4}. Select the null set $N \in \G$ such that in addition to the conditions it satisfies above we also have the consistency condition&lt;/p&gt;
&lt;p&gt;$$
\lim_{r_{n+1} \to \infty} F_n^\omega(r_1, \ldots, r_n, r_{n+1}) = F_n^\omega(r_1, \ldots, r_n), \quad n \ge 1
$$&lt;/p&gt;
&lt;p&gt;For reals $x_1, \ldots, x_n$ define just like Equation \eqref{eq5}. Then for each $\omega \in \Omega$, $\se{F_n^\omega, \, n \ge 1}$ is a consistent family of distribution functions, and hence by the Kolmogorov extension theorem there exists a unique measure $\mu_\omega$ on $(\R^\infty, \B(\R^\infty))$ whose finite dimensional distributions are $\se{F_n^\omega, \, n \ge 1}$. Define $\PP_T(B, \omega) = \mu_\omega(B)$ for $B \in \B(\R^\infty)$. If&lt;/p&gt;
&lt;p&gt;\begin{align*}
\cH &amp;amp;= \set{B \in \B(\R^\infty)}{\PP_T(B, \cdot) = \Pc{T^{-1}(B)}{\G} \text{ a.s.} } \\&lt;br&gt;
\D &amp;amp;= \bigcup_{n=1}^\infty \set{B \in \B(\R^\infty)}{B = [-\infty, r_1) \times \cdots \times [-\infty, r_n) \times \R \times \R \times \cdots, \, r_i \in \Q}
\end{align*}&lt;/p&gt;
&lt;p&gt;then $\cH$ is a $\lambda-$class, $\D$ is a $\pi-$class, and $\cH \subseteq \D$. Hence, by the $\pi-\lambda$ theorem $\cH \subseteq \sigma(\D) = \B(\R^\infty)$.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Williams, David. &lt;em&gt;Probability with Martingales&lt;/em&gt;. Cambridge mathematical textbooks, Cambridge University Press, 1991.&lt;/li&gt;
&lt;li&gt;Chow, Yuan Shih and Teicher, Henry. &lt;em&gt;Probability Theory: Independence, Interchangeability, Martingales&lt;/em&gt;, 3rd edn. Springer-Verlag New York, 1997.&lt;/li&gt;
&lt;li&gt;Halmos, P. R.. &lt;em&gt;Measure Theory&lt;/em&gt;. Van Nostrand, Princeton, N. J., 1950; Springer-Verlag, Berlin and New York, 1974.&lt;/li&gt;
&lt;li&gt;Parthasarathy, K. R.. &lt;em&gt;Probability Measures on Metric Spaces&lt;/em&gt;. AMS Chelsea Publishing, 1967.&lt;/li&gt;
&lt;li&gt;Durrett, R.: &lt;em&gt;Probability: Theory and Examples&lt;/em&gt;, 5th edn. Cambridge University Press, 2019.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://encyclopediaofmath.org/wiki/Standard_Borel_space&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://encyclopediaofmath.org/wiki/Standard_Borel_space&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Longest Increasing Subsequence</title>
      <link>https://makkar.github.io/post/lisprob/</link>
      <pubDate>Fri, 25 Dec 2020 17:17:08 -0500</pubDate>
      <guid>https://makkar.github.io/post/lisprob/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Prob}[1]{\PP \left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 ; : ; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 ; | ; #2 \right\}}
\newcommand{\Ex}[1]{\E\left[#1\right]}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ph}[1]{\varphi^{-1}\left(#1\right)}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathrm{Var}}
\DeclareMathOperator{\Log}{\mathrm{Log}}
\DeclareMathOperator{\O}{\mathcal{O}}
$$&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Finding a longest increasing subsequence is a well-known problem in computer science (note that I use the article &amp;ldquo;a&amp;rdquo; instead of &amp;ldquo;the&amp;rdquo; because there could be multiple longest subsequences): Given a sequence $\se{a_1, \ldots, a_n}$ of real numbers, we want to find a subsequence $\se{a_{i_1}, \ldots, a_{i_k}}$ such that $0 \le i_1 &amp;lt; \cdots &amp;lt; i_k \le n$, $a_{i_1} \le \cdots \le a_{i_k}$, and the subsequence is as long as possible. It has a very easy 
&lt;a href=&#34;https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dynamic programming&lt;/a&gt; solution with a time complexity of $\O(n^2)$ and a slightly 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more involved solution&lt;/a&gt; with a time complexity of $\O(n \log n)$. But we are not interested in these algorithms in this blog post.&lt;/p&gt;
&lt;p&gt;We are interested in studying the asymptotics of the &lt;em&gt;length&lt;/em&gt; of the longest increasing subsequences of a sequence whose elements are coming from a random permutation. This simple to state problem will take us on a beautiful journey touching topics from combinatorics and probability theory. In particular, we will get to see the very elegant technique of Poissonization.&lt;/p&gt;
&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;Let us start by stating precisely what we are trying to prove. To do that we first define some notation. For any integer $n \ge 1$, let $S_n$ be the group of permutations of order $n$, i.e., it contains all permutations of $\se{1, 2, \ldots, n}$ and hence $S_n$ contains $n!$ elements.  If $\pi \in S_n$ then a subsequence of $\pi$ is a sequence $\se{\pi(i_1), \ldots, \pi(i_k)}$ such that $1 \leq i_1 &amp;lt; \cdots &amp;lt; i_k \leq n$. It is an increasing subsequence if $\pi(i_1) &amp;lt; \cdots &amp;lt; \pi(i_k)$ and similarly for the decreasing subsequence. Consider the uniform measure $\mu_n$ on the discrete measurable space $(S_n, 2^{S_n})$, i.e., $\mu_n(\pi) = 1 / n!$ for any $\pi \in S_n$. For $\pi \in S_n$ define $l_n(\pi)$ to be the maximal length of an increasing subsequence of $\pi$, i.e., $l_n(\pi)$ is the largest $k$ such that there are integers $1 \leq i_1 &amp;lt; \cdots &amp;lt; i_k \leq n$ so that $\pi(i_1) &amp;lt; \cdots &amp;lt; \pi(i_k)$. Similarly define $d_n(\pi)$ to be the maximal length of a decreasing subsequence of $\pi$.&lt;/p&gt;
&lt;p&gt;Since $l_n$ is a random variable we can consider its expectation $L_n = \E[l_n]$ on the probability space $(S_n, 2^{S_n}, \mu_n)$. It can be explicitly written as
$$
L_n = \frac{1}{n!} \sum_{\pi \in S_n} l_n(\pi) \tag{1} \label{eq1}
$$
We want to study the limiting properties of the sequence $\se{L_n}_{n \in \N}$. Specifically we will show that
$$
\frac{L_n}{\sqrt{n}} \to \gamma \text{ almost surely} \tag{2} \label{eq2}
$$
for some constant $\gamma$. It is known that $\gamma = 2$. We will not be showing this, but we will show that $1 \le \gamma \le e$.&lt;/p&gt;
&lt;h1 id=&#34;combinatorial-results&#34;&gt;Combinatorial results&lt;/h1&gt;
&lt;p&gt;We now prove some useful combinatorial results. The first result is called the Erdős–Szekeres theorem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 1 [Erdős–Szekeres theorem]: &lt;/span&gt; In any sequence $\se{a_1, a_2, \ldots, a_{mn+1}}$ of $mn+1$ distinct real numbers, there exists either an increasing subsequence $a_{i_1} &amp;lt; \cdots &amp;lt; a_{i_{m+1}}$ $(i_1 &amp;lt; \cdots &amp;lt; i_{m+1})$ of length $m+1$, or a decreasing subsequence $a_{j_1} &amp;gt; \cdots &amp;gt; a_{j_{n+1}}$ $(j_1 &amp;lt; \cdots &amp;lt; j_{n+1})$ of length $n+1$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; For $1 \leq i \leq mn+1$ define $t_i$ to be the length of a longest increasing subsequence starting at $a_i$, i.e., the first element of a longest increasing subsequence must be $a_i$ and the rest of the $a$&#39;s must have indices greater than $i$. If $t_i \geq m+1$ for some $i$ then we are done since we then have an increasing subsequence of length $m+1$, so assume $t_i \leq m$ for all $i$. Since $t_i \geq 1$, pigeonhole principle now implies that there is some integer $1 \leq k \leq m$ such that $t_i = k$ for at least $n+1$ $i$&#39;s. Let $t_i = k$ for all $i \in (j_1 &amp;lt; \cdots &amp;lt; j_{n+1})$. Now note that if $a_{j_l} &amp;lt; a_{j_{l+1}}$ for some $1 \leq l \leq n$, then we would obtain an increasing subsequence of length $k+1$ starting at $a_{j_l}$ because there is an increasing subsequence of length $k$ starting at $a_{j_{l+1}}$. But this contradicts the fact that $t_{j_l} = k,$ and thus $a_{j_l} &amp;gt; a_{j_{l+1}}$ for all $1 \leq l \leq n$. But now this gives us a decreasing subsequence $a_{j_1} &amp;gt; \cdots &amp;gt; a_{j_{n+1}}$ of length $n+1$. $\square$&lt;/p&gt;
&lt;p&gt;The next two theorems prove lower and upper bounds for $L_n / \sqrt{n}$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 2: &lt;/span&gt; $L_n / \sqrt{n}$ is lower bounded as follows
$$
L_n \geq \sqrt{n} \text{ for all } n \geq 1 \tag{3} \label{eq3}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt;
Theorem 1 implies $l_n(\pi) d_n(\pi) \geq n$ for all $\pi \in S_n$. Note that for every permutation $\pi \in S_n$ there exists an inverse permutation $\pi&#39;$ such that $l_n(\pi) = d_n(\pi&amp;rsquo;)$ and thus we can write $L_n$ also as
$$
L_n = \frac{1}{n!} \sum_{\pi \in S_n} d_n(\pi) \tag{4} \label{eq4}
$$
Therefore, averaging the two ways of computing the expectation $L_n$ (Equations \eqref{eq1} and \eqref{eq4}) and using the AM-GM inequality we have
$$
L_n = \frac{1}{n!} \sum_{\pi \in S_n} \frac{l(\pi) + d(\pi)}{2} \geq \frac{1}{n!} \sum_{\pi \in S_n} \sqrt{l(\pi) d(\pi)} \geq \frac{1}{n!} \sum_{\pi \in S_n} \sqrt{n} = \sqrt{n} \qquad \square
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 3: &lt;/span&gt; Upper bound:
$$
\lims \frac{L_n}{\sqrt{n}} \leq e \tag{5} \label{eq5}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt;
If $\pi \in S_n$ and $1 \leq k \leq n$ let $X_{n,k}(\pi)$ be the number of increasing subsequences of $\pi$ which are of length $k$. They are exactly those subsets $S \subseteq \se{1, \ldots, n}$ such that $|S| = k$ and if $S = \se{i_1 &amp;lt; \cdots &amp;lt; i_k}$ then $\pi(i_1) &amp;lt; \cdots &amp;lt; \pi(i_k)$. On the probability space $(S_n, 2^{S_n}, \mu_n)$, by the linearity of expectation, the expected value of $X_{n,k}$ is given by the number of ways to select $k$ subsets of $\se{1, \ldots, n}$ times the probability that a selection has all the elements in the increasing order. The number of ways is simply $\binom{n}{k}$ and the probability is $1/k!$ and thus
$$
\E[X_{n,k}] = \frac{1}{k!} \binom{n}{k}
$$
The Taylor expansion of $e^x$ implies $e^x \ge x^k / k!$. Substituting $x = k$ we get $k! \geq (k/e)^k$. Therefore, we get
$$
\E[X_{n,k}] = \frac{1}{k!} \binom{n}{k} = \frac{n(n-1) \cdots (n-k-1)}{(k!)^2} \leq \frac{n^k}{(k/e)^{2k}}
$$&lt;/p&gt;
&lt;p&gt;Now for a discrete random variable $X_{n,k}$ we can write $\E[X_{n,k}] = \sum_{i=0}^n \mu_n(X_{n,k} \geq i)$ and thus $\mu_n(X_{n,k} \geq 1) \leq \Ex{X_{n,k}}$. Also note that $l_n(\pi) \geq k$ if and only if $X_{n,k}(\pi) \geq 1$. We thus get
$$
\mu_n(l_n \geq k) = \mu_n(X_{n,k} \geq 1) \leq \E[X_{n,k}] \leq \frac{n^k}{(k/e)^{2k}}
$$
Fix an arbitrary $\delta &amp;gt; 0$ and let $k = \ceil{(1+\delta)e\sqrt{n}}$ in the inequality above to get
$$
\mu_n(l_n \geq k) \leq \frac{n^k}{(k/e)^{2k}} \leq \left( \frac{1}{1+\delta} \right)^{2k} \leq \left( \frac{1}{1+\delta} \right)^{2(1+\delta)e\sqrt{n}}
$$
Since $l_n \leq n$, we have
$$
L_n = \Ex{l_n} \leq \mu_n(l_n &amp;lt; k) (1+\delta)e\sqrt{n} + \mu_n(l_n \geq k) n \leq (1+\delta)e\sqrt{n} + O(e^{-c\sqrt{n}})
$$
where $c$ is some positive constant that depends on $\delta$.
Since $\delta$ was arbitrary, we can let $\delta \to 0$ and then take $\limsup$ to get Equation \eqref{eq5}. $\square$&lt;/p&gt;
&lt;h1 id=&#34;poissonization&#34;&gt;Poissonization&lt;/h1&gt;
&lt;p&gt;To be able to show \eqref{eq2} we will draw a correspondence between this problem of longest increasing subsequences and a seemingly unrelated problem called the Poissonized version. This Poissonized version will allow us to use the powerful Subadditive Ergodic Theorem (Theorem 5 below) to show \eqref{eq2}.&lt;/p&gt;
&lt;p&gt;To this end, assume an underlying probability space $\probsp$ and let $N$ be a Poisson random measure on $\mathbb{R} _ + ^2$ with mean measure given by the Lebesgue measure on $\mathbb{R} _ + ^2$. In other words $N:\Omega \times \B(\R _ + ^2) \to \bar{\R} _ +$ is a transition kernel from $(\Omega, \F)$ into $(\R_+^2, \B(\R_+^2))$ with $\int_\Omega \PP(\dd \omega) N(\omega, A) = \text{Leb}(A)$ for any $A \in \B(\R_+^2)$. We can view the random process as a sequence $\se{(X_i, Y_i)} _ {i \geq 1}$ of independent random variables taking values in $\R _ + ^2$ and having a uniform probability measure (more correctly, Lebesgue measure on $(\R_+^2, \B(\R_+^2))$). If we let $R_{s,t}$ denote the rectangle with vertices $(s,s), (s,t), (t,t)$ and $(t,s)$, then for each outcome $\omega \in \Omega$, we can think of having $\text{Poisson}(\text{Leb}(R_{s,t}(\omega)))$ distributed number of such points inside the rectangle $R_{s,t}(\omega).$&lt;/p&gt;
&lt;p&gt;For $s &amp;lt; t \in [0, \infty)$ let $Z_{s,t}$ be the random variable denoting the length of the longest increasing path lying in the rectangle $R_{s,t}$, i.e., $Z_{s,t}$ is the largest integer $k$ for which there are points $(X_1,Y_1), \dots, (X_k, Y_k)$ in the Poisson process with $s &amp;lt; X_1 &amp;lt; \cdots &amp;lt; X_k &amp;lt; t$ and $s&amp;lt; Y_1 &amp;lt; \cdots &amp;lt; Y_k &amp;lt; t$.&lt;/p&gt;
&lt;p&gt;Let $\tau(n)$ be the smallest value of $t \in [0, \infty)$ for which there are $n$ points in $R_{0,t}$. Let the $n$ points in $R_{0, \tau(n)}$ be written as $\se{(X_i, Y_i)}_{1 \leq i \leq n}$ such that $0 &amp;lt; X_1 &amp;lt; \cdots &amp;lt; X_n \leq \tau(n)$ (the inequalities are strict almost surely since they have continuous distributions). Let $\pi_n \in S_n$ be the unique permutation such that $Y_{\pi_n(1)} &amp;lt; \cdots &amp;lt; Y_{\pi_{n}(n)}$ (It is not difficult to see the existence and the uniqueness). Then
$$
\pi_n \text{ is a uniformly random sample of } S_n \text{ , and } Z_{0, \tau(n)} = l_n(\pi_n) \tag{6} \label{eq6}
$$
The second claim is obvious from the definition of $\pi_n$. The first claim is equivalent to showing that if $U_1, \ldots, U_n$ are independent random variables sampled from a uniform distribution on $[0,1]$ and if $U_{(1)}, \ldots, U_{(n)}$ are the order statistics, i.e., $U_{(k)}$ is the $k$&lt;sup&gt;th&lt;/sup&gt; smallest among $U_1, \ldots, U_n$, then the probability that $U_{(1)}, \ldots, U_{(n)}$ is same as $U_{\pi(1)}, \ldots, U_{\pi(n)}$ for any $\pi \in S_n$ is $1/n!$. But this is obvious from the independence of $U_i$&#39;s. The next theorem is from Durrett:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 4: &lt;/span&gt; $\tau(n) / \sqrt{n} \to 1$ almost surely.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt;
Let $S_n$ be the number of points in $R_{0,\sqrt{n}}$. Since $(R_{0, \sqrt{n}} \setminus R_{0, \sqrt{n-1}}) \cap (R_{0, \sqrt{m}} \setminus R_{0, \sqrt{m-1}}) = \emp$ for $n \neq m$, the definition of a Poisson random measure implies $\se{S_n - S_{n-1}}_{n \geq 1}$ are independent Poisson random variables with mean $1$. The strong law of large numbers now implies $S_n / n \to 1$ almost surely. For any $\e &amp;gt; 0$ we can find an $n$ large enough such that $S_{n(1-\e)} &amp;lt; n &amp;lt; S_{n(1+\e)}$ but then this means $\sqrt{n(1-\e)} \leq \tau(n) \leq \sqrt{n(1+\e)}$ which is same as the statement of the theorem since $\e$ was arbitrary. $\square$&lt;/p&gt;
&lt;p&gt;The last theorem along with \eqref{eq6} implies $Z_{0, \sqrt{n}} \to L_n$ almost surely and thus
$$
\frac{Z_{0,n}}{n} \to \frac{L_{n^2}}{n} \text{ almost surely} \tag{7} \label{eq7}
$$&lt;/p&gt;
&lt;h1 id=&#34;back-to-longest-increasing-subsequences&#34;&gt;Back to longest increasing subsequences&lt;/h1&gt;
&lt;p&gt;Having established the connection between the two ways of looking at the problem of longest increasing subsequences, we now freely jump between the two characterizations and use them to prove our results.&lt;/p&gt;
&lt;p&gt;Define $W_{s,t} = - Z_{s,t}$. We now check that $W_{m,n}, 0 \le m &amp;lt; n$ satisfies the conditions required for the Subadditive Ergodic Theorem. I state the theorem below for completeness. Check out Theorem 6.4.1 in 
&lt;a href=&#34;https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Durrett&lt;/a&gt; for a proof.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 5 [Subadditive Ergodic Theorem]: &lt;/span&gt; Suppose $W_{m,n}, 0 \le m &amp;lt; n$ satisfy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$W_{0,m} + W_{m,n} \ge W_{0,n}$,&lt;/li&gt;
&lt;li&gt;$\se{W_{nk, (n+1)k}, n \ge 1}$ is a stationary sequence for each $k$,&lt;/li&gt;
&lt;li&gt;The distribution of $\se{W_{m,m+k}, k \ge 1}$ does not depend on $m$,&lt;/li&gt;
&lt;li&gt;$\Ex{W_{0,1}^+} &amp;lt; \infty$ and $\inf_{n \ge 1} \frac{1}{n} \Ex{W_{0,1}} = \beta &amp;gt; - \infty$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\limn \frac{1}{n} \Ex{W_{0,1}} = \beta$,&lt;/li&gt;
&lt;li&gt;$W = \limn \frac{1}{n} W_{0,1}$ exists almost surely and in $L^1$, and $\Ex{X} = \beta$,&lt;/li&gt;
&lt;li&gt;If all stationary sequences in 2. are ergodic, then $W = \beta$ almost surely.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Coming back to the problem, let $0 &amp;lt; m &amp;lt; n$ then we claim $Z_{0,m} + Z_{m,n} \leq Z_{0,n}$. To see this fix $\omega \in \Omega$ and let $Z_{0,m}(\omega) = a$ and $Z_{m,n}(\omega) = b$. Then there exist $(X_1(\omega), Y_1(\omega)), \ldots (X_a(\omega), Y_a(\omega)), (X_{a+1}(\omega), Y_{a+1}(\omega)), \ldots, (X_{a+b}(\omega), Y_{a+b}(\omega))$ such that $0 &amp;lt; X_1(\omega) &amp;lt; \cdots &amp;lt; X_a(\omega) &amp;lt; m$, $0 &amp;lt; Y_1(\omega) &amp;lt; \cdots &amp;lt; Y_a(\omega) &amp;lt; m$ and $m &amp;lt; X_{a+1}(\omega) &amp;lt; \cdots &amp;lt; X_{a+b}(\omega) &amp;lt; n$, $m &amp;lt; Y_{a+1}(\omega) &amp;lt; \cdots &amp;lt; Y_{a+b}(\omega) &amp;lt; n$. But then it&amp;rsquo;s clear that $0 &amp;lt; X_1(\omega) &amp;lt; \cdots &amp;lt; X_{a+b}(\omega) &amp;lt; n$, $0 &amp;lt; Y_1(\omega) &amp;lt; \cdots &amp;lt; Y_{a+b}(\omega) &amp;lt; n$ and we have  $Z_{0,m}(\omega) + Z_{m,n}(\omega) \leq Z_{0,n}(\omega)$. Since $\omega$ was arbitrary equation our claim is true. Therefore, $W_{0,m} + W_{m,n} \ge W_{0,n}$ and condition 1. is true.&lt;/p&gt;
&lt;p&gt;For condition 2. we want to show that $\se{W_{nk, (n+1)k, n \geq 1}}$ is a stationary and ergodic sequence for all $k \geq 1$. This is clear from the observation that $Z_{ik, (i+1)k} \stackrel{d}{=} Y_{0,k}$ since $\text{Leb}(R_{ik, (i+1)k}) = \text{Leb}(R_{0,k})$ and thus by the definition of the Poisson random measure the number of points in each rectangle is an i.i.d. Poisson random variable. Checking the condition 3. is similar to condition 2..&lt;/p&gt;
&lt;p&gt;For condition 4. note that $W_{0,1}^+ = Z_{0,1}$ and $Z_{0,1} \leq \text{Poisson}(1)$ since there are $\text{Poisson}(1)$ number of points inside $[0,1]^2$ and at most all of them can be arranged in the increasing order. Thus, $\Ex{W_{0,1}^+} &amp;lt; \Ex{\text{Poisson}(1)} = 1 &amp;lt; \infty$. Now note that since $W_{0,n} = - Z_{0,n}$
$$
\inf_{n \geq 1} \frac{1}{n} \Ex{W_{0,n}} = - \sup_{n \geq 1} \frac{1}{n} \Ex{Z_{0,n}}
$$
and thus to show the second part of condition 4. we need to show that $\sup_{n \geq 1} \frac{1}{n} \Ex{Z_{0,n}} &amp;lt; \infty$. But this is immediate from Equation \eqref{eq5} in Theorem 3 and Equation \eqref{eq7}. Therefore, the subadditive ergodic theorem now implies
$$
\frac{Z_{0,n}}{n} \to \gamma \text{ a.s.}
$$
where $\gamma$ from Equations \eqref{eq3} and \eqref{eq5} lies in $[1,e]$ and we are done since this implies \eqref{eq2}.&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;I got introduced to this problem from an exam question in a math course I took recently. I recommend the book &amp;ldquo;The Surprising Mathematics of Longest Increasing Subsequences&amp;rdquo; by Dan Romik, which is 
&lt;a href=&#34;https://www.math.ucdavis.edu/~romik/book/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;freely available online&lt;/a&gt;, for a lot more content.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A coin tossing game</title>
      <link>https://makkar.github.io/post/infcoingame/</link>
      <pubDate>Sun, 11 Oct 2020 16:59:05 -0400</pubDate>
      <guid>https://makkar.github.io/post/infcoingame/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\se}[1]{\left{ #1 \right}}
\newcommand{\set}[2]{\left{ #1 ; : ; #2 \right}}
\newcommand{\sett}[2]{\left{ #1 ; | ; #2 \right}}
\newcommand{\Ex}[1]{\E\left[#1\right]}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dpr}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
$$&lt;/p&gt;
&lt;p&gt;I recently came across this simple to state puzzle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; Suppose you have a coin that has probabilities $p$ for heads and $1-p$ for tails. You play the following game with a friend. The first player picks one of the outcomes $HH$, $TH$, $HT$ and $TT$. The second player observes the choice of the first player and picks one of the remaining 3 outcomes. You then proceed to toss the coin infinitely many times independently constructing an infinite sequence of $H$’s and $T$’s. A player wins if their choice appears first in the sequence. For example, if player one chose $HH$ and player two chose $TT$ then $HTHTHTTHTHHH&amp;hellip;$ results in a victory for player two since $TT$ occurred before $HH$ in the sequence. When is it better to go first in this game?&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s make the conditions under which a player wins more concrete.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;The strategy: &lt;/span&gt; It’s better to go as the first player in the game if for some choice $a \in \{HH,TT,HT,TH\}$, the probability of winning the game is greater than $1/2$ no matter the choice of the second player. It’s better to go as the second player in the game if for every choice of the first player, we can find some element of the set $\{HH,TT,HT,TH\} \setminus \{a\}$, where $a$ is the choice of the first player, such that the probability of winning the game is greater than $1/2$.&lt;/p&gt;
&lt;p&gt;As an example, suppose $p = 3/4$, then if the first player chooses $HH$, then no matter the choice of the second player, the probability of the first player winning the game is greater than $1/2$. To see this note that the probability of getting two consecutive heads in the first two coin tosses itself is $9/16 &amp;gt; 1/2$.&lt;/p&gt;
&lt;p&gt;On first impression it seems as if it must be better to go as the first player, no matter the value of $p$, since the first player has more choices, but as we will see, the fact that the second player has the advantage of choosing the outcome &lt;em&gt;after&lt;/em&gt; observing the choice of the first player gives him an advantage for certain values of $p$.&lt;/p&gt;
&lt;p&gt;But before we do that, we should prove an implicit assumption: the game ends with a winner in a finite number of moves with probability $1$. But to do that we first need to define a probability space on which the game is played. In particular, does it even exist? Indeed it does as we see below.&lt;/p&gt;
&lt;h2 id=&#34;constructing-the-probability-space&#34;&gt;Constructing the probability space&lt;/h2&gt;
&lt;p&gt;To formalize our analysis we need to define a sequence of i.i.d. random variables, one for each coin toss.
However, the existence of a probability space on which we can define this sequence is not obvious at all.
For example, the following claim shows that we cannot always construct desired number of random variables on a measurable space.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Claim: &lt;/span&gt; There do not exist uncountably many independent, non-constant random variables on $([0,1], \B([0,1]), \lambda)$, where $\lambda$ is the Lebesgue measure on the Borel $\sigma$-algebra $\B([0,1])$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Assume that $\{X_i \}_{i \in I}$ is a collection of independent non-constant random variables. Define the collection $\{Y_i \} _{i \in I}$ by letting $Y_i = X_i \ind{|X_i| &amp;lt; C_i}$ where $C_i$ is large enough that $Y_i$ isn&amp;rsquo;t a constant. The collection formed by the random variables $Z_i = Y_i - \Ex{Y_i}$ is a collection of independent random variables. Note that the random variables $Z_i$ are in the separable Hilbert space $L^2([0,1], \lambda)$ and are orthogonal to each other. But a separable Hilbert can have only countably such elements. Thus, $I$ must be countable. $\square$&lt;/p&gt;
&lt;p&gt;Fortunately the situation isn&amp;rsquo;t so bleak for our case and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov extension theorem&lt;/a&gt; allows us to claim the existence of a probability space on which there exists our required sequence of i.i.d. random variables if we can show the existence of probability spaces $(\Omega_n, \F_n, \PP_n)$ for $n$ coin tosses that satisfy the consistency conditions. This is very easy: let $\Omega_n = \{H, T\}^n$, $\F_n = 2^{\Omega_n}$, $\PP({\omega}) = p^{m} (1-p)^{n-m}$ where $m = $ number of $H$ in $\omega \in \Omega_n$, and finally $\PP(A) = \sum_{\omega \in A} \PP({\omega})$ for any $A \in \F_n$. It is easy to see that this construction satisfies the consistency conditions and thus there exists a unique probability measure $\PP$ defined on the measurable space $(\Omega, \F)$ where $\Omega = \{H, T\}^{\N}$ and  $\F$ is the $\sigma$-algebra generated by the cylinder sets.&lt;/p&gt;
&lt;h2 id=&#34;the-game-has-a-winner-with-probability-1&#34;&gt;The game has a winner with probability $1$&lt;/h2&gt;
&lt;p&gt;We can now safely say the following statement: Let $\{X_n\}$ be a sequence of i.i.d. Bernoulli random variables such that $X_n = H$ or $T$ depending on the result of the $n$&lt;sup&gt;th&lt;/sup&gt; coin toss. To show that the game has a winner with probability $1$ we need to prove that in the sequence $X_1, X_2, \ldots$ all of the four choices $HH$, $TH$, $HT$ and $TT$ appear in a finite number of coin tosses with probability $1$.&lt;/p&gt;
&lt;p&gt;To that end, fix any of the four choices $HH$, $TH$, $HT$ and $TT$, and call it $XY$. Let $E_n$ be the event that $X_{2n-1} = X$ and $X_{2n} = Y$. Then $\PP(E_n) = \e &amp;gt; 0$ independent of $n$, where $\e$ is some positive real number dependent on $XY$ (for example, if $XY = HT$ then $\e = p(1-p)$). Now the events $\{E_n\}$ are independent and $\sum_{n=1}^\infty \PP(E_n) = \infty$, and thus we can apply the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma#Converse_result&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;second Borel-Cantelli lemma&lt;/a&gt;
to get $\PP(E_n \text{ i.o.}) = 1$. But this immediately implies that each of the four outcomes appear in that sequence infinitely many times with probability $1$.&lt;/p&gt;
&lt;h2 id=&#34;back-to-the-game&#34;&gt;Back to the game&lt;/h2&gt;
&lt;p&gt;Recall the strategy outlined above. Without loss of generality we may assume $p \geq 1/2$ because otherwise we can just flip the tags $H$ and $T$. From a first player perspective we just want to find one of the choices $HH$, $TH$, $HT$ or $TT$. Let&amp;rsquo;s calculate the minimum $p$ we get for each choice.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $HH$ and player $2$ chooses $TH$, then it&amp;rsquo;s better to be player $1$ if $p &amp;gt; 1/\sqrt{2}$. Checking for other choices of player $2$ we see that $TH$ is the optimal choice.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $TT$ and player $2$ chooses $HT$, then for no $p \geq 1/2$ it is better to be player $1$.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $HT$ and player $2$ chooses $HH$, then for no $p \geq 1/2$ it is better to be player $1$.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $TH$ and player $2$ chooses $HT$, then for no $p \geq 1/2$ it is better to be player $1$.&lt;/p&gt;
&lt;p&gt;Overall it is better to be player $1$ if $p &amp;gt; 1/\sqrt{2}$ or if $p &amp;lt; 1 - 1/\sqrt{2}$. And on the other hand it is better to be player $2$ if $1 - 1/\sqrt{2} &amp;lt; p &amp;lt; 1/\sqrt{2}$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernels - Part 2</title>
      <link>https://makkar.github.io/post/kernels2/</link>
      <pubDate>Mon, 07 Sep 2020 16:05:35 -0400</pubDate>
      <guid>https://makkar.github.io/post/kernels2/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\P}{\mathcal{P}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\L}{\mathcal{L}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\trans}{\intercal}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\O}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
$$&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We discussed 
&lt;a href=&#34;https://makkar.github.io/post/kernels/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;some functional analysis&lt;/a&gt; and the 
&lt;a href=&#34;https://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;basics of Reproducing Kernel Hilbert Space theory&lt;/a&gt; in the previous two articles. The aim of this article is to discuss the need for approximating the kernel matrix and one method in particular to do that, Random Fourier Features.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by discussing Kernel Ridge Regression, a simple application of kernel methods.&lt;/p&gt;
&lt;h1 id=&#34;kernel-ridge-regression&#34;&gt;Kernel Ridge Regression&lt;/h1&gt;
&lt;h2 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s recall the ridge regression model. We have some training data of the form:&lt;/p&gt;
&lt;p&gt;$$
(x_1, y_1), \ldots, (x_n, y_n) \in (\X, \Y)
$$
where $\X = \R^d$ and $\Y = \R$. We represent the training data more succinctly by letting $\matr{y} = [y_1, \ldots, y_n]^\trans$ and $\matr{X}$ be an $n \times d$ matrix whose $i$&lt;sup&gt;th&lt;/sup&gt; row is $x_i^\trans$. Then in ridge regression the likelihood model is&lt;/p&gt;
&lt;p&gt;$$
\matr{y} \sim \NN(\matr{X} \matr{w}, \sigma^2 \matr{I}_n)
$$
where $\matr{w}$ is a $d$-dimensional column vector representing the weights to learned, and $\sigma^2 &amp;gt; 0$. We also assume a Gaussian prior on $\matr{w}$&lt;/p&gt;
&lt;p&gt;$$
\matr{w} \sim \NN\left(0, \frac{1}{\lambda} \matr{I}_d\right)
$$&lt;/p&gt;
&lt;p&gt;Then the maximum a posteriori (MAP) estimate is given by&lt;/p&gt;
&lt;p&gt;\begin{align}
\matr{w}_{\text{MAP}} &amp;amp;= \argmax _{\matr{w}} \; \ln p(\matr{w} | \matr{y}, \matr{X}) \\
&amp;amp;= \argmax _{\matr{w}} \; \ln p(\matr{y} | \matr{w}, \matr{X}) + \ln p(\matr{w}) \\
&amp;amp;= \argmax _{\matr{w}} \; \underbrace{-\frac{1}{2\sigma^2} (\matr{y} - \matr{X} \matr{w})^\trans (\matr{y} - \matr{X} \matr{w}) - \frac{\lambda}{2} \matr{w}^\trans \matr{w}} _{\L}
\end{align}&lt;/p&gt;
&lt;p&gt;If we call this objective $\L$, then $\matr{w}_{\text{MAP}}$ is given by the solution of the equation formed by equating the gradient of $\L$ with respect to $\matr{w}$ to $0$. Thus,&lt;/p&gt;
&lt;p&gt;$$
0 = \nabla_{\matr{w}} \L = \frac{1}{\sigma^2} \matr{X}^\trans \matr{y} - \frac{1}{\sigma^2} \matr{X}^\trans \matr{X} \matr{w} - \lambda \matr{w}
$$&lt;/p&gt;
&lt;p&gt;and we get&lt;/p&gt;
&lt;p&gt;$$
\matr{w}_{\text{MAP}} = (\lambda \sigma^2 \matr{I} + \matr{X}^\trans \matr{X})^{-1} \matr{X}^\trans \matr{y}
$$&lt;/p&gt;
&lt;p&gt;which is called the ridge regression solution, and we denote it by $\matr{w}_{\text{RR}}$. We are inverting a $d \times d$ matrix taking $\O(d^3)$ time, and matrix multiplication of a $d \times d$ matrix with a $d \times n$ matrix taking $\O(d^2 n)$ time.&lt;/p&gt;
&lt;h2 id=&#34;kernel-ridge-regression-1&#34;&gt;Kernel Ridge Regression&lt;/h2&gt;
&lt;p&gt;The previous model suffers from limited expressiveness. As we have seen before, the idea of kernel methods is to define a map which takes our inputs from $\X$ to an RKHS $\H$:&lt;/p&gt;
&lt;p&gt;$$
\phi : \X \to \H
$$&lt;/p&gt;
&lt;p&gt;and then apply the linear model of ridge regression above to $\phi(x_i)$ instead of $x_i$. $\H$ could be an infinite-dimensional vector space, but for now suppose it is $D$-dimensional. Let $\matr{\Phi}$ represent the $n \times D$ matrix whose $i$&lt;sup&gt;th&lt;/sup&gt; row is $\phi(x_i)^\trans$. Then the ridge regression solution is given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
\matr{w}_{\text{RR}} = (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans \matr{y}
\end{equation}&lt;/p&gt;
&lt;p&gt;We immediately realize the problem here: we are inverting a $D \times D$ matrix and $D$ can be huge! Fortunately, we have a matrix trick that we can exploit, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Woodbury_matrix_identity#Discussion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;push-though identity&lt;/a&gt;. It looks like this&lt;/p&gt;
&lt;p&gt;\begin{align}
\matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) &amp;amp;= (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi}) \matr{\Phi}^\trans \quad \text{(can be seen by expanding)} \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) &amp;amp;= (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi}) \matr{\Phi}^\trans \quad \text{(left multiplying by } (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \text{ )} \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) &amp;amp;= \matr{\Phi}^\trans \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} &amp;amp;= \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \quad \text{(right multiplying by } (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \text{ )} \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans &amp;amp;= \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1}
\end{align}&lt;/p&gt;
&lt;p&gt;And thus we get
$$
\matr{w}_{\text{RR}} = \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \matr{y}
$$&lt;/p&gt;
&lt;p&gt;We are now inverting an $n \times n$ matrix taking $\O(n^3)$ time, and matrix multiplication of a $D \times n$ matrix with a $n \times n$ matrix taking $\O(D n^2)$ time.&lt;/p&gt;
&lt;p&gt;Notice that $\matr{\Phi} \matr{\Phi}^\trans$ is the gram matrix, and let&amp;rsquo;s denote it by $\matr{K}$. It&amp;rsquo;s $(i,j)$&lt;sup&gt;th&lt;/sup&gt; entry is $\matr{K}_{i,j} = \inner{\phi(x_i)}{\phi(x_j)} = K(x_i, x_j)$, where $K : \R^d \times \R^d \to \C$ is the kernel function corresponding to the RKHS $\H$.&lt;/p&gt;
&lt;p&gt;If we denote&lt;/p&gt;
&lt;p&gt;$$
\matr{\alpha} = (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \matr{y} = (\lambda \sigma^2 \matr{I}_n + \matr{K})^{-1} \matr{y}
$$&lt;/p&gt;
&lt;p&gt;then we have&lt;/p&gt;
&lt;p&gt;$$
\matr{w}_{\text{RR}} = \matr{\Phi}^\trans \matr{\alpha} = \sum _{i=1}^n \alpha_i \phi(x_i)
$$&lt;/p&gt;
&lt;p&gt;and prediction for a test data point, $x$, is&lt;/p&gt;
&lt;p&gt;$$
\matr{w}_{\text{RR}}^\trans \phi(x) = \sum _{i=1}^n \alpha_i \phi(x_i)^\trans \phi(x) = \sum _{i=1}^n \alpha_i k _{x_i}(x)
$$&lt;/p&gt;
&lt;p&gt;where $k_{x_i}$ denotes the reproducing kernel for the point $x_i$.&lt;/p&gt;
&lt;p&gt;If this looks suspiciously similar to the 
&lt;a href=&#34;https://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Representer theorem&lt;/a&gt; (Theorem-8), then that&amp;rsquo;s because it is an instance of it! So prediction takes $\O(n)$ time.&lt;/p&gt;
&lt;p&gt;The point of this discussion was to show that Kernel Ridge Regression is computationally expensive. Simply storing the gram matrix, $\matr{K}$, takes $\O(n^2)$ space. We need to find methods which can circumvent using $\matr{K}$ if we are to apply kernel methods to anything other than the smallest of the data sets.&lt;/p&gt;
&lt;h1 id=&#34;kernel-approximation&#34;&gt;Kernel approximation&lt;/h1&gt;
&lt;p&gt;The idea behind kernel approximation methods is to replace the gram matrix, $\matr{K}$, with a low rank approximation, i.e., find an $n \times S$ matrix, $\matr{Z}$, such that&lt;/p&gt;
&lt;p&gt;$$
\matr{K} \approx \matr{Z} \matr{Z}^\trans
$$&lt;/p&gt;
&lt;p&gt;We want $S \ll n$, and thus we significantly reduce our time and space complexity. $\matr{Z}$ takes $\O(nS)$ space. Inversion of $(\lambda \sigma^2 \matr{I}_n + \matr{Z} \matr{Z}^\trans)$ takes $\O(n S^2)$ time.&lt;/p&gt;
&lt;p&gt;One way to think about this is we want to find a mapping of $x_i$&#39;s to some latent space such that the inner product in this latent space approximates the kernel&lt;/p&gt;
&lt;p&gt;$$
\matr{K}_{i,j} = K(x_i, x_j) \approx z_i^\trans z_j
$$&lt;/p&gt;
&lt;p&gt;where $z_i$ denotes the $i$&lt;sup&gt;th&lt;/sup&gt; row of $\matr{Z}$.
One way to do that is Nyström approximation, which I won&amp;rsquo;t be discussing at all. The other method is to use Random Fourier Features. This approach was introduced by Rahimi and Recht in their seminal 2007 paper 
&lt;a href=&#34;https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random Features for Large-Scale Kernel Machines&lt;/a&gt;. It relies on a result from functional analysis called Bochner&amp;rsquo;s theorem, so let&amp;rsquo;s digress and discuss that first.&lt;/p&gt;
&lt;h1 id=&#34;bochners-theorem&#34;&gt;Bochner&amp;rsquo;s theorem&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Definition 28:&lt;/strong&gt; A function $K : \R^n \to \C$, is said to be &lt;em&gt;positive-definite&lt;/em&gt; if&lt;/p&gt;
&lt;p&gt;$$
\sum _{i,j = 1}^n \alpha_i \conj{\alpha_j} K(x_i - x_j) \geq 0
$$&lt;/p&gt;
&lt;p&gt;for every choice of $x_1, \ldots, x_n \in \R^n$ and for every choice of complex numbers $\alpha_1, \ldots, \alpha_n$.&lt;/p&gt;
&lt;p&gt;Notice how similar this definition is to the 
&lt;a href=&#34;https://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;definition of a kernel function&lt;/a&gt;. Positive-definite functions have some nice properties:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3:&lt;/strong&gt; If $K: \R^n \to \C$ is a positive-definite function, then $K(-x) = \conj{K(x)}$ for every $x \in \R^n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; It is easy to see that $K(0) \geq 0$. In the definition above, let $n = 2$, $x_1 = x$, $x_2 = 0$, $\alpha_1$ be an arbitrary complex number, and $\alpha_2 = 1$. Then applying the definition of a positive-definite function, we get&lt;/p&gt;
&lt;p&gt;$$
(1 + |\alpha_1|^2)K(0) + \alpha_1 K(x) + \conj{\alpha_1}K(-x) \geq 0
$$&lt;/p&gt;
&lt;p&gt;Let $\alpha_1 = 1$, then
$$
2K(0) + K(x) + K(-x) \geq 0
$$&lt;/p&gt;
&lt;p&gt;In particular, $K(x) + K(-x)$ is real, which implies&lt;/p&gt;
&lt;p&gt;$$
K(x) + K(-x) = \conj{K(x)} +\conj{K(-x)}
$$&lt;/p&gt;
&lt;p&gt;Similarly, letting $\alpha_1 = i$, we get $i(K(x) - K(-x))$ is real, which implies&lt;/p&gt;
&lt;p&gt;$$
K(x) - K(-x) = -\conj{K(x)} + \conj{K(-x)}
$$&lt;/p&gt;
&lt;p&gt;Adding these two equations, we get $K(-x) = \conj{K(x)}$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 4:&lt;/strong&gt; If $K: \R^n \to \C$ is a positive-definite function, then $K$ is bounded. In particular, $|K(x)| \leq K(0)$ for every $x \in \R^n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; From lemma-3, $K(0)$ must be real. In the definition above, let $n = 2$, $x_1 = 0$, $x_2 = x$, $\alpha_1 = |K(x)|$, and $\alpha_2 = -\conj{K(x)}$. Then applying the definition of a positive-definite function, we get&lt;/p&gt;
&lt;p&gt;$$
2K(0) |K(x)|^2 - |K(x)| K(x) K(-x) - \conj{K(x)} |K(x)| K(x) \geq 0
$$&lt;/p&gt;
&lt;p&gt;Now use lemma-3 to substitute $\conj{K(x)}$ for $K(-x)$ in the middle term to get&lt;/p&gt;
&lt;p&gt;$$
2K(0) |K(x)|^2 - 2 |K(x)|^3 \geq 0
$$&lt;/p&gt;
&lt;p&gt;If $|K(x)| = 0$, then we obviously have our result, since we can easily show $K(0) \geq 0$, otherwise we can divide by $2|K(x)|^2$ to get&lt;/p&gt;
&lt;p&gt;$$
K(0) - |K(x)| \geq 0
$$&lt;/p&gt;
&lt;p&gt;which is our desired result. $\square$&lt;/p&gt;
&lt;p&gt;The following theorem is the converse of Bochner&amp;rsquo;s theorem. We state it first since it is easier to prove.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 9 [Converse of Bochner&amp;rsquo;s theorem]:&lt;/strong&gt; The Fourier transform of every finite Borel measure on $\R^n$ is positive-definite.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let $\mu$ be a finite Borel measure on $\R^n$. Let $K : \R^n \to \C$ be the Fourier transform of $\mu$, i.e.,&lt;/p&gt;
&lt;p&gt;$$
K(x) = \int_{\R^n} \exp(-i \omega^\trans x) \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;Let $x_1, \ldots, x_n \in \R^n$ and $\alpha_1, \ldots, \alpha_n \in \C$ be arbitrary. Then&lt;/p&gt;
&lt;p&gt;\begin{align}
\sum _{i,j = 1}^n \alpha_i \conj{\alpha_j} K(x_i - x_j) &amp;amp;= \sum _{i,j = 1}^n \alpha_i \conj{\alpha_j} \int _{\R^n} \exp\{-i \omega^\trans (x_i - x_j)\} \dmu(\omega) \\&lt;br&gt;
&amp;amp;= \int _{\R^n} \left| \sum _{i = 1}^n \alpha_i \exp(-i \omega^\trans x_i) \right|^2 \dmu(\omega) \\&lt;br&gt;
&amp;amp;\geq 0
\end{align}&lt;/p&gt;
&lt;p&gt;Thus, $K$ is positive-definite. $\square$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 10 [Bochner&amp;rsquo;s theorem]:&lt;/strong&gt; If $K$ is continuous and positive-definite, then $K$ is the Fourier transform of a finite positive Borel measure.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof since it is beyond my understanding. The proof can be easily found on the internet, see 
&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.917.270&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; or 
&lt;a href=&#34;http://individual.utoronto.ca/jordanbell/notes/bochnertheorem.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for example.&lt;/p&gt;
&lt;p&gt;This finite positive Borel measure is often called as &lt;em&gt;spectral measure&lt;/em&gt;. It is easy to see that $K(0) = \mu(\R^n)$, and thus if we assume $K(0) = 1$ then the spectral measure is a probability measure.&lt;/p&gt;
&lt;h1 id=&#34;random-fourier-features&#34;&gt;Random Fourier Features&lt;/h1&gt;
&lt;p&gt;Picking up where we left: We want to find a low-dimensional mapping of $x_i$&#39;s into a latent space such that we can approximate the kernel computation with an inner product in this latent space. We will use Bochner&amp;rsquo;s theorem for this. To apply this theorem we need to limit ourselves to a special class of kernels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 29:&lt;/strong&gt; A kernel function $K : \R^d \times \R^d \to \C$ is called &lt;em&gt;shift-invariant&lt;/em&gt; if $K(x, y) = K(x-y, 0)$.&lt;/p&gt;
&lt;p&gt;Gaussian and Laplacian kernels are examples of shift-invariant kernels.
Note that the function $K&amp;rsquo;: \R^d \to \C$ defined by $K&amp;rsquo;(x) = K(x, 0)$ is positive-definite if $K$ is a shift-invariant kernel. Bochner&amp;rsquo;s theorem then implies the existence of a spectral measure, $\mu$, such that&lt;/p&gt;
&lt;p&gt;$$
K(x, y) = K(x-y, 0) = K&amp;rsquo;(x-y) = \int _{\R^d} \exp\{-i\omega^\trans (x-y)\} \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;Let $p$ denote the Radon-Nikodym derivative of $\mu$ with respect to the Lebesgue measure on $\R^d$, then we can write the equation above as&lt;/p&gt;
&lt;p&gt;\begin{equation}
K(x, y) = \int _{\R^d} \exp\{-i\omega^\trans (x-y)\} p(\omega) \dom \tag{$\star$}
\end{equation}&lt;/p&gt;
&lt;p&gt;For example, if we take the Gaussian kernel&lt;/p&gt;
&lt;p&gt;$$
K(x,y) = \exp\left(-\frac{\norm{x-y}^2_2}{2 \sigma^2}\right)
$$&lt;/p&gt;
&lt;p&gt;then since $K(0,0) = 1$, $p$ is a probability density, and can be easily shown to be Gaussian&lt;/p&gt;
&lt;p&gt;$$
p \sim \NN\left(0, \frac{1}{\sigma^2} \matr{I}_d\right)
$$&lt;/p&gt;
&lt;p&gt;We now see an obvious way to approximate the kernel from equation $(\star)$: use Monte Carlo approximation. If we take $S$ samples, $\omega_1, \ldots, \omega_S \sim p(\omega)$,&lt;/p&gt;
&lt;p&gt;\begin{align}
K(x, y) &amp;amp;= \int _{\R^d} \exp\{-i\omega^\trans (x-y)\} p(\omega) \dom \\&lt;br&gt;
&amp;amp;\approx \frac{1}{S} \sum _{s=1}^S \exp\{-i\omega^\trans_s (x-y)\} \\&lt;br&gt;
&amp;amp;= \inner{z(x)}{z(y)}
\end{align}&lt;/p&gt;
&lt;p&gt;where $z : \R^d \to \R^S$ is the latent mapping given by&lt;/p&gt;
&lt;p&gt;$$
z(x) = \frac{1}{\sqrt{S}}[\exp(-i \omega ^\trans _1 x), \ldots, \exp(-i \omega ^\trans _S x)]^\trans
$$&lt;/p&gt;
&lt;p&gt;We can get another mapping by noting that if the kernel is real, then from $(\star)$ we can see that the RHS must also be real and we can thus write it as&lt;/p&gt;
&lt;p&gt;\begin{equation}
K(x, y) = \int _{\R^d} \cos\{\omega^\trans (x-y)\} p(\omega) \dom
\end{equation}&lt;/p&gt;
&lt;p&gt;Using the fact that $\cos(a - b) = \cos(a) \cos(b) + \sin(a) \sin(b)$, we can write $\cos\{\omega^\trans (x-y)\} = \cos(\omega^\trans x) \cos(\omega^\trans y) + \sin(\omega^\trans x) \sin(\omega^\trans y)$&lt;/p&gt;
&lt;p&gt;Thus, if we define $z_1 : \R^d \to \R^{2S}$ by&lt;/p&gt;
&lt;p&gt;$$
z_1(x) = \frac{1}{\sqrt{S}}[\cos(\omega^\trans_1 x), \ldots, \cos(\omega^\trans_S x), \sin(\omega^\trans_1 x), \ldots, \sin(\omega^\trans_S x)]^\trans
$$&lt;/p&gt;
&lt;p&gt;we have $\inner{z_1(x)}{z_1(y)} \approx K(x, y)$.&lt;/p&gt;
&lt;p&gt;Another mapping that is used is $z_2 : \R^d \to \R^{S}$ defined by&lt;/p&gt;
&lt;p&gt;$$
z_2(x) = \sqrt{\frac{2}{S}}[\cos(\omega^\trans_1 x + b_1), \ldots, \cos(\omega^\trans_S x + b_S)]^\trans
$$&lt;/p&gt;
&lt;p&gt;where $b_1, \ldots, b_S \sim \text{Unif}[0, 2\pi]$. It is not too difficult to show that&lt;/p&gt;
&lt;p&gt;$$
\E_{\omega, b}[z_2(x)^\trans z_2(y)] = \E_{\omega}[z_1(x)^\trans z_1(y)]
$$&lt;/p&gt;
&lt;p&gt;There exist many modifications to the basic RFF approach described here. For example, we can do Quasi-Monte Carlo approximations instead of Monte Carlo approximations. See the paper 
&lt;a href=&#34;https://jmlr.org/papers/volume17/14-538/14-538.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels&lt;/a&gt; by Avron, Sindhwani, Yang and Mahoney (2016). Or we could sample from a modified distribution in Fourier space, given by the leverage function of the kernel. See the paper 
&lt;a href=&#34;https://arxiv.org/pdf/1804.09893.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees&lt;/a&gt; by Avron et al (2017).&lt;/p&gt;
&lt;h1 id=&#34;extension-to-a-more-general-class-of-kernels&#34;&gt;Extension to a more general class of kernels&lt;/h1&gt;
&lt;p&gt;I have often seen kernels being represented as an integral of a product of two functions neatly separating the dependence on $x$ and $y$:&lt;/p&gt;
&lt;p&gt;$$
K(x,y) = \int_{\Omega} \varphi(\omega, x) \varphi(\omega, y) \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;for some function $\varphi(\cdot, \cdot)$ and measure $\mu$ on $\Omega$. I found a good explanation of when this is possible in the papers by Bach (2017) 
&lt;a href=&#34;https://jmlr.org/papers/volume18/14-546/14-546.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Breaking the Curse of Dimensionality with Convex Neural Networks&lt;/a&gt; and  Li et al (2019) 
&lt;a href=&#34;https://arxiv.org/pdf/1806.09178.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards a Unified Analysis of Random Fourier Features&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let $\mu$ be a Borel probability measure on the compact space $\Omega$, and $\varphi: \Omega \times \X \to \R$ be a function such that the functions $\varphi(\cdot, x) : \Omega \to \R$ are measurable for all $x \in \X$, i.e., they are random variables. Now define the set $\H$ to consist of all functions $f$ that can written as&lt;/p&gt;
&lt;p&gt;$$
f(x) = \int_{\Omega} h(\omega) \varphi(\omega, x) \dmu(\omega) \quad \text{for all } x \in \X
$$&lt;/p&gt;
&lt;p&gt;for some $h: \Omega \to \R$ such that $\int_{\Omega} h^2 \dmu &amp;lt; \infty$. Let us define the squared norm, $\norm{f}_{\H}^2$, as the infimum of $\int_{\Omega} h^2 \dmu$ over all functions $h$ for which $f$ can be decomposed as above. Then it can be shown that $\H$ is an RKHS with the kernel&lt;/p&gt;
&lt;p&gt;$$
K(x,y) = \int_{\Omega} \varphi(\omega, x) \varphi(\omega, y) \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can again approximate this kernel with a Monte Carlo approximation:&lt;/p&gt;
&lt;p&gt;\begin{align}
K(x,y) &amp;amp;= \int_{\Omega} \varphi(\omega, x) \varphi(\omega, y) \dmu(\omega) \\&lt;br&gt;
&amp;amp;\approx \frac{1}{S} \sum_{s=1}^S \varphi(\omega_s, x) \varphi(\omega_s, y)
\end{align}&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;Random Fourier Features is an easy to implement approach that allows us to apply kernel methods on large data sets. I haven&amp;rsquo;t discussed the number of samples, $S$, needed to approximate the kernel function within an error bound. You can find such results in the papers linked above.&lt;/p&gt;
&lt;p&gt;With this article I conclude the series on kernels. I had initially set out to write a short piece on Random Fourier Features but the subject of kernel theory is vast and beautiful, and that short piece metastasised to three articles.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zorn&#39;s lemma</title>
      <link>https://makkar.github.io/post/zorn/</link>
      <pubDate>Fri, 07 Aug 2020 18:59:12 -0400</pubDate>
      <guid>https://makkar.github.io/post/zorn/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\P}{\mathcal{P}}
\newcommand{\Ch}{\mathcal{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I have been self-studying some functional analysis recently, and Zorn&amp;rsquo;s lemma comes up often in proofs. Since I have no formal background in mathematics (my undergrad was in Mechanical engineering), it was my first time hearing about Zorn&amp;rsquo;s lemma. I read its statement from the Wikipedia article, naively assuming I understood this extremely powerful tool. But every usage of Zorn&amp;rsquo;s lemma seemed contrived to me, and it was only in retrospect that I could see how Zorn&amp;rsquo;s lemma seems like an obvious tool to apply. An excellent article which helped me understand Zorn&amp;rsquo;s lemma is 
&lt;a href=&#34;https://gowers.wordpress.com/2008/08/12/how-to-use-zorns-lemma/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to use Zorn&amp;rsquo;s lemma&lt;/a&gt; by Timothy Gowers. If you didn&amp;rsquo;t know about Gowers&amp;rsquo;s article, then mentioning it is the biggest contribution of my article and I recommend you read it.&lt;/p&gt;
&lt;p&gt;In this article, I want to show how to use Zorn&amp;rsquo;s lemma by stating a theorem and discussing its proof. Things &amp;ldquo;clicked&amp;rdquo; for me when I proved that theorem. But before we do that let me define some important concepts.&lt;/p&gt;
&lt;h1 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h1&gt;
&lt;p&gt;Recall the concept of &lt;em&gt;relation&lt;/em&gt; I defined 
&lt;a href=&#34;https://makkar.github.io/post/equivalence-relations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; Let $P$ be a nonempty set. A &lt;em&gt;partial order relation&lt;/em&gt; in $P$ is a relation which is symbolized by $\leq$ and that satisfies the following properties for all $x,y,z \in P$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reflexivity: $x \leq x$;&lt;/li&gt;
&lt;li&gt;Antisymmetry: $x \leq y$ and $y \leq x$ implies $x = y$;&lt;/li&gt;
&lt;li&gt;Transitivity: $x \leq y$ and $y \leq z$ implies $x \leq z$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The set $P$ is then called a &lt;em&gt;partially ordered set&lt;/em&gt;. If in addition to these three properties, the set $P$ also satisfies&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;any two elements are comparable, i.e., either $x \leq y$ or $y \leq x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;then the relation is called a &lt;em&gt;total order relation&lt;/em&gt; and the set $P$ is called a &lt;em&gt;totally ordered set&lt;/em&gt; or a &lt;em&gt;chain&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Let $P$ be the set of all positive integers, and let $m \leq n$ mean that $m$ divides $n$. Then $P$ is a partially ordered set. It is not a chain as $2$ and $3$ are not comparable, for example.&lt;/li&gt;
&lt;li&gt;Let $P$ be the set of all real numbers, and let $x \leq y$ mean that $y-x$ is nonnegative. Then $P$ is a chain.&lt;/li&gt;
&lt;li&gt;Let $P$ be the class of all subsets of some universal set $U$, and let $A \leq B$ for $A,B \in P$ mean that $A \subseteq B$. Then $P$ is a partially ordered set. It is not a chain because if $U$ contains at least two elements, then we can find two subsets of $U$ neither of which is a subset of the other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; If $P$ is a partially ordered set, an element $x \in P$ is said to be &lt;em&gt;maximal&lt;/em&gt; if for any $y \in P$ for which $x \leq y$, we must have $x = y$.&lt;/p&gt;
&lt;p&gt;Note that a maximal element does not have to be bigger than everything else: it just must not be smaller than anything else. A maximal element may not exist, and if it exists it may not be unique.
Examples 1 and 2 above have no maximal elements. Example 3 has one maximal element: $U$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3:&lt;/strong&gt; Let $Q$ be a nonempty subset of a partially ordered set $P$. An element $x \in P$ is called an &lt;em&gt;upper bound&lt;/em&gt; of $Q$ if $y \leq x$ for every $y \in Q$. An upper bound of $Q$ is called a &lt;em&gt;least upper bound&lt;/em&gt; of $Q$ if it is less than or equal to every upper bound of $Q$.&lt;/p&gt;
&lt;p&gt;Similarly for &lt;em&gt;lower bound&lt;/em&gt; and &lt;em&gt;greatest lower bound&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note how similar these definitions are to the definitions of supremum and infimum for real numbers. That&amp;rsquo;s because in that case they are the same. In Example 1 above, if we take $Q$ to be any finite subset of $P$, then the greatest lower bound is the greatest common divisor of all the elements of $Q$ and the least upper bound is the least common multiple. In Example 3 above, let $Q$ be any nonempty subset of $P$. Then the least upper bound is the union of all the sets in $Q$, and the greatest lower bound is the intersection of all the sets in $Q$.&lt;/p&gt;
&lt;p&gt;We are now ready to state the Zorn&amp;rsquo;s lemma.&lt;/p&gt;
&lt;h1 id=&#34;zorns-lemma&#34;&gt;Zorn&amp;rsquo;s lemma&lt;/h1&gt;
&lt;p&gt;If $P$ is a partially ordered set in which every chain has an upper bound, then $P$ possesses a maximal element.&lt;/p&gt;
&lt;h1 id=&#34;example-application&#34;&gt;Example application&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Theorem:&lt;/strong&gt; Any infinite set $X$ can be represented as a union of a disjoint class of countably infinite subsets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; The theorem is trivial if $X$ is countably infinite, so assume that it is uncountably infinite.&lt;/p&gt;
&lt;p&gt;The first thing that we need to do is realize Zorn&amp;rsquo;s lemma can be applied here. I use the following description taken from Gowers&amp;rsquo;s article as a guiding principle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you are building a mathematical object in stages and find that (i) you have not finished even after infinitely many stages, and (ii) there seems to be nothing to stop you continuing to build, then Zorn’s lemma may well be able to help you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The fact that we need to build $X$ by taking a union of some sets, and it could be an uncountable union and therefore we might not finish even in countably infinite number of stages, suggests Zorn&amp;rsquo;s lemma might be helpful here. Zorn&amp;rsquo;s lemma will prove the existence of a maximal element in a partially ordered set, therefore, we want to construct a partially ordered set such that we can prove that its maximal element is $X$.&lt;/p&gt;
&lt;p&gt;We want to build our partially ordered set to be consisting of elements of this form: disjoint class of countably infinite subsets of $X$. The reason we want to define our partially ordered set this way is because, first, the subset operation makes it a partially ordered set, and second, it seems intuitively true that the maximal element of this partially ordered set, if it exists, will be such that the union of its elements is $X$.&lt;/p&gt;
&lt;p&gt;To this end, let $\P$ be the set of all disjoint classes of countably infinite subsets of $X$. $\P$ is partially ordered by $\subseteq$ as we saw in Example 3 above.&lt;/p&gt;
&lt;p&gt;To be able to apply Zorn&amp;rsquo;s lemma we need to show that every chain in $\P$ has an upper bound. Let $\Ch$ be a chain in $\P$. A natural guess for the upper bound of $\Ch$ is $U = \bigcup _{S \in \Ch} S$. To show that $U$ is an upper bound of $\Ch$ we need to show that $U \in \P$ and $S \subseteq U$ for every $S \in \Ch$. The second claim is trivial from $U$&#39;s definition. It is easy to see that $U$ is a class of countably infinite subsets of $X$ because this is true for each $S \in \Ch$. To show that $U$ is a disjoint class, let $A, B \in U$ be distinct elements of $U$. There exist elements $S_A, S_B \in \Ch$ such that $A \in S_A$ and $B \in S_B$. Since $\Ch$ is a chain, either $S_A \subseteq S_B$ or $S_B \subseteq S_A$. Without loss of generality, $S_A \subseteq S_B$. Then $A, B \in S_B$ and this implies $A$ and $B$ are disjoint because this is true for the elements of our chain. Thus $U$ is a disjoint class of countably infinite subsets and hence belongs to $\P$.&lt;/p&gt;
&lt;p&gt;By the Zorn&amp;rsquo;s lemma, $\P$ possesses a maximal element, $M$. If $\bigcup _{E \in M} E = X$ then we are done because $M$ is the required disjoint class of countably infinite subsets whose union is $X$. But it can still happen that some elements of $X$ are not present in this union. In this case we can show that these leftover elements form a finite set and we can get our required disjoint class by adding these leftover elements to any element of $M$. To see that the leftover elements must be finite, denote the set of leftover elements by $Y = X \setminus \bigcup _{E \in M} E$. If $Y$ is infinite it must contain a countably infinite subset $Z \subseteq Y$. Consider the class $M \cup \{Z\}$. It is an element of $\P$ because it is a disjoint class of countably infinite subsets of $X$. But $M$ is a strict subset of $M \cup \{Z\}$ which contradicts the fact that $M$ is a maximal element of $\P$.&lt;/p&gt;
&lt;p&gt;We are done!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernels - Part 1</title>
      <link>https://makkar.github.io/post/kernels1/</link>
      <pubDate>Thu, 16 Jul 2020 16:20:23 -0400</pubDate>
      <guid>https://makkar.github.io/post/kernels1/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is the second blog post in the series of blog posts on kernels. In the 
&lt;a href=&#34;http://makkar.github.io/post/kernels/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first part&lt;/a&gt; I introduced the functional analysis background for kernel theory. I highly recommend you read it before continuing. I will frequently refer to it and use the same notation. In this blog post I aim to introduce the fundamental theorems like Mercer&amp;rsquo;s theorem and Representer theorem.&lt;/p&gt;
&lt;h1 id=&#34;characterization-of-reproducing-kernels&#34;&gt;Characterization of reproducing kernels&lt;/h1&gt;
&lt;p&gt;We already defined the notion of reproducing kernels for an RKHS (Definition 20). We now turn our attention to obtaining necessary and sufficient conditions for a function $K : \X \times \X \to \C$ to be the reproducing kernel for some RKHS. But first we must define kernel functions.&lt;/p&gt;
&lt;h2 id=&#34;kernel-functions&#34;&gt;Kernel functions&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start by recalling a basic definition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 21:&lt;/strong&gt; Let $A = (a_{i,j})$ be an $n \times n$ complex matrix. Then $A$ is called &lt;em&gt;positive&lt;/em&gt; if for every $\alpha_1, \ldots, \alpha_n \in \C$ we have
$$ \sum_{i,j = 1}^{n} \conj{\alpha_i}\alpha_j a_{i,j} \geq 0 $$
We denote this by $A \geq 0$.&lt;/p&gt;
&lt;p&gt;Note that if we define a vector $x \in \C^n$ to be such that its $i$&lt;sup&gt;th&lt;/sup&gt; component is $\alpha_i$, then the condition above can rewritten as
$$ \inner{Ax}{x} \geq 0 $$&lt;/p&gt;
&lt;p&gt;Also note that if $A \geq 0$ then $A = A^* $, where $A^* $ denotes the Hermitian matrix (also known as the self-adjoint matrix), $\conj{A^T}$. Therefore, positivity gives self-adjoint property for free if we are dealing with complex matrices. Things aren&amp;rsquo;t so elegant for real matrices. For the real case we need to explicitly state that the matrix $A$ is also symmetric apart from what&amp;rsquo;s stated in the definition above. Therefore, we often use the following as the definition of positive matrices:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 21&amp;rsquo;:&lt;/strong&gt; An $n \times n$ matrix $A$ is positive, in symbols $A \geq 0$, if it is self-adjoint and if $\inner{Ax}{x} \geq 0$ for all $x \in \C^n$.&lt;/p&gt;
&lt;p&gt;Positive matrices are also alternatively called &lt;em&gt; positive semidefinite&lt;/em&gt; or &lt;em&gt;nonnegative&lt;/em&gt; matrices.&lt;/p&gt;
&lt;p&gt;The following lemma connects the concept of positive matrices to its eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2:&lt;/strong&gt; A matrix $A \geq 0$ if and only if $A = A^*$ and every eigenvalue of $A$ is nonnegative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let us first suppose $A \geq 0$, then $A = A^*$ by definition. Now if $\lambda$ is an eigenvalue of $A$ and $v$ is an eigenvector of $A$ corresponding to $\lambda$, we have
$$ 0 \leq \inner{Av}{v} = \inner{\lambda v}{v} = \lambda \inner{v}{v} $$
Thus, $\lambda$ is a nonnegative number.&lt;/p&gt;
&lt;p&gt;For the other side, find an orthonormal basis $v_1, \ldots, v_n$ consisting of eigenvectors of $A$ (it exists by the Spectral theorem). Let $v_i$ be the eigenvector corresponding to the eigenvalue $\lambda_i$. Then for any $x = \sum_i \alpha_i v_i$ we have $\inner{Ax}{x} = \sum_i \lambda_i \modu{\alpha_i}^2$. Since $\lambda_i \geq 0$, $\inner{Ax}{x} \geq 0$ and $A$ must be positive. $\square$&lt;/p&gt;
&lt;p&gt;We are now ready to define kernel function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 22:&lt;/strong&gt; Let $\X$ be a set, then $K : \X \times \X \to \C$ is called a &lt;em&gt;kernel function&lt;/em&gt; if for every $n \in \N$ and for every choice of $\{x_1, \ldots, x_n\} \subseteq \X$, the matrix $(K(x_i, x_j)) \geq 0$. We will use the notation $K \geq 0$ to denote that the function $K$ is a kernel function.&lt;/p&gt;
&lt;p&gt;Kernel functions are alternatively called &lt;em&gt;positive definite functions&lt;/em&gt; or &lt;em&gt;positive semidefinite functions&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 23:&lt;/strong&gt; Given a kernel function $K : \X \times \X \to \C$ and points $x_1, \ldots, x_n \in \X$, the $n \times n$ matrix $(K(x_i, x_j))$ is called the Gram matrix of $K$ with respect to $x_1, \ldots, x_n$.&lt;/p&gt;
&lt;p&gt;Some examples follow:&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Linear kernels&lt;/em&gt;: When $\X = \R^d$, we can define the linear kernel function as
$$K(x, y) = \inner{x}{y} $$
It is clearly a symmetric function of its arguments, and hence self-adjoint. To prove positivity, let $x_1, \ldots, x_n \in \R^d$ be an arbitrary collection of points, and consider its gram matrix $\matr{K}$, i.e., $\matr{K}_{i,j} = K(x_i, x_j) = \inner{x_i}{x_j}$. Then for any $\alpha \in \R^n$, we have&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ \inner{\matr{K} \alpha}{\alpha} = \alpha^T \matr{K}^T \alpha = \alpha^T \matr{K} \alpha = \sum_{i,j = 1}^n \alpha_i \alpha_j \inner{x_i}{x_j} = \left\lVert \sum_{i=1}^n \alpha_i x_i \right\rVert^2 \geq 0 $$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Polynomial kernels&lt;/em&gt;: A natural generalization of the linear kernel on $\R^d$ is the homogeneous polynomial kernel
$$ K(x, y) = (\inner{x}{y})^m $$
of degree $m \geq 2$, also defined on $\R^d$. It is clearly a symmetric function. To prove positivity, note that&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \left( \sum_{i=1}^d x_i y_i \right)^m $$&lt;/p&gt;
&lt;p&gt;This will have $D = \binom{m+d-1}{m}$ monomials, so to simplify the analysis let&amp;rsquo;s take $m=2$. Then&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \sum_{i=1}^d x_i^2 y_i^2 + 2 \sum_{i &amp;lt; j} x_i x_j y_i y_j $$
In this case $D = \binom{d+1}{d} = d + \binom{d}{2}$. Define a mapping $\Phi: \R^d \to \R^D$ such that&lt;/p&gt;
&lt;p&gt;$$ \Phi(x) = [x_1^2, \ldots, x_d^2, \sqrt{2}x_1 x_2, \ldots, \sqrt{2} x_{d-1} x_d ]^T $$&lt;/p&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \inner{\Phi(x)}{\Phi(y)} $$&lt;/p&gt;
&lt;p&gt;Following the same argument as the first example, we can verify that the gram matrix thus formed is positive.&lt;/p&gt;
&lt;p&gt;The mapping $x \mapsto \Phi(x)$ is often referred to as a &lt;em&gt;feature map&lt;/em&gt;. We see that dealing with elements in    the feature space, i.e. the range of $\Phi$, is computationally expensive. The relation $K(x,y) = \inner{\Phi(x)}{\Phi(y)}$ allows us compute the inner products using the kernel function instead of actually taking the inner product in a very high dimensional space. We will see that this &amp;ldquo;kernel trick&amp;rdquo; holds for very many kernel functions when we discuss Mercer&amp;rsquo;s theorem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Gaussian kernels&lt;/em&gt;: Given some compact subset $\X \subseteq \R^d$, consider the Gaussian kernel&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \exp{\left( -\frac{1}{2 \sigma^2} \norm{x-y}^2 \right)} $$&lt;/p&gt;
&lt;p&gt;It is not obvious why this is a kernel function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;equivalence-between-kernel-function-and-reproducing-kernel&#34;&gt;Equivalence between kernel function and reproducing kernel&lt;/h2&gt;
&lt;p&gt;Let us return to the characterization of reproducing kernels. We will now prove that a function is a kernel function if and only if there is an RKHS for which it is the reproducing kernel. At this point recall Theorem-3 which states that an RKHS admits a unique reproducing kernel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 4:&lt;/strong&gt; Let $\X$ be a set and let $\H$ be an RKHS on $\X$ with reproducing kernel $K$. Then $K$ is a kernel function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; For some arbitrary $n \in \N$ fix some arbitrary collection $x_1, \ldots, x_n \in \X$ and $\alpha \in \C^n$. Then if we denote by $\matr{K}$ the gram matrix of $K$ with respect to $x_1, \ldots, x_n$, we have
$$\inner{\matr{K} \alpha}{\alpha} =  \sum_{i,j = 1}^n \conj{\alpha_i}\alpha_j K(x_i, x_j) = \sum_{i,j = 1}^n \conj{\alpha_i}\alpha_j \inner{k_{x_j}}{k_{x_i}} = \left\lVert \sum_{i=1}^n \alpha_i k_{x_i} \right\rVert^2 \geq 0 \quad$$&lt;/p&gt;
&lt;p&gt;And thus $K$ is a kernel function. $\square$&lt;/p&gt;
&lt;p&gt;What does it mean in the above proof if we have an equality? That is, if $\inner{\matr{K} \alpha}{\alpha} = 0$? This happens if and only if $\left\lVert \sum_{i=1}^n \alpha_i k_{x_i} \right\rVert = 0$. But this means that for every $f \in \H$ we have $\sum_{i=1}^n \conj{\alpha_i} f(x_i) = \inner{f}{\sum_i \alpha_i k_{x_i}} = 0$. Thus, in this case there is an equation of linear dependence between the values of every function in $\H$ at this finite set of points.&lt;/p&gt;
&lt;p&gt;Now let us state the converse of Theorem-4. It is a deep result in RKHS theory known as the Moore–Aronszajn theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 5 [Moore–Aronszajn theorem]:&lt;/strong&gt; Let $\X$ be a set and let $K : \X \times \X \to \C$ be a kernel function, then there exists a reproducing kernel Hilbert space $\H$ of functions on $\X$ such that $K$ is the reproducing kernel of $\H$.&lt;/p&gt;
&lt;p&gt;For a proof see 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#Moore%E2%80%93Aronszajn_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here on Wikipedia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In light of these two theorems we have the following notation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 24:&lt;/strong&gt; Given a kernel function $K : \X \times \X \to \C$, we let $\H(K)$ denote the unique RKHS with the reproducing kernel $K$.&lt;/p&gt;
&lt;p&gt;It is not an easy problem to start with a kernel function $K$ on some set $\X$ and give a concrete description of $\H(K)$. I will not be discussing this here, but there are plenty of resources available where you can see this problem getting discussed. I recommend Paulsen and Raghupathi&amp;rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces.&lt;/p&gt;
&lt;h1 id=&#34;mercers-theorem&#34;&gt;Mercer&amp;rsquo;s theorem&lt;/h1&gt;
&lt;p&gt;Recall the Spectral theorem for finite-dimensional vector spaces:  A linear operator $T: V \to V$ for some finite dimensional vector space $V$ on $\C$ is normal, i.e., $T T^* = T^* T$, if and only if $V$ has an orthonormal basis consisting of eigenvectors of $T$. This implies that if $\matr{U} = [v_1, \ldots, v_n]$ is a unitary matrix containing the $i$&lt;sup&gt;th&lt;/sup&gt; eigenvector in its $i$&lt;sup&gt;th&lt;/sup&gt; column and $\matr{\Lambda} = \text{diag}(\lambda_1, \ldots, \lambda_n)$ is a diagonal matrix containing the corresponding eigenvalues then if $\matr{K}$ is a normal matrix then it can written as
$$\matr{K} = \matr{U} \matr{\Lambda} \matr{U}^T = \sum_{i=1}^n \lambda_i v_i v_i^T$$&lt;/p&gt;
&lt;p&gt;Mercer&amp;rsquo;s theorem generalizes this decomposition to kernel functions. Let us start by defining a special type of kernel function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 25:&lt;/strong&gt; Let $\X$ be a compact metric space. A function $K : \X \times \X \to \C$ is called a &lt;em&gt;Mercer kernel&lt;/em&gt; if it is a continuous kernel function.&lt;/p&gt;
&lt;p&gt;Recall the space $L^2(\mu)$ from Definition-8 where we now take $\X$ (written as $X$ there) to be a compact metric space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 26:&lt;/strong&gt; Given a Mercer kernel $K : \X \times \X \to \C$, we define a linear operator $T_{K} : L^2(\mu) \to L^2(\mu)$ as
$$ T_K(f)(x) := \int_{\X} K(x, y) f(y) \dmu(y), \quad (x \in \X) $$&lt;/p&gt;
&lt;p&gt;We assume that the Mercer kernel satisfies the &lt;em&gt;Hilbert-Schmidt condition&lt;/em&gt;, stated as&lt;/p&gt;
&lt;p&gt;$$ \int_{\X \times \X} \left\lvert K(x,y) \right\rvert^2 \dmu(x) \dmu(y) &amp;lt; \infty $$&lt;/p&gt;
&lt;p&gt;which ensures that $T_K$ is a bounded linear operator on $L^2(\mu)$. Indeed, we have&lt;/p&gt;
&lt;p&gt;$$ \lVert T_K(f) \rVert^{2} = \int_{\X} \left\lvert \int_{\X} K(x, y) f(y) \dmu(y) \right\rvert^2 \dmu(x) \leq \norm{f}^2 \int_{\X \times \X} \left\lvert K(x,y) \right\rvert^2 \dmu(x) \dmu(y) $$&lt;/p&gt;
&lt;p&gt;where we have applied Schwarz inequality (Theorem-1) as follows
$$ \left\lvert \int_{\X} K(x, y) f(y) \dmu(y) \right\rvert^2 = \left\lvert T_K(f)(x) \right\rvert^2 = \left\lvert \inner{K(x, \cdot)}{f} \right\rvert^2 \leq \norm{K(x, \cdot)}^2 \norm{f}^2$$&lt;/p&gt;
&lt;p&gt;Operators of this type are known as &lt;em&gt;Hilbert-Schmidt operators&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We are now ready to state the Mercer&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 5 [Mercer&amp;rsquo;s theorem]:&lt;/strong&gt; Suppose that $\X$ is a compact metric space, and $K : \X \times \X \to \C$ is a Mercer&amp;rsquo;s kernel that satisfies the Hilbert-Schmidt condition.
Then there exists an at most countable set of eigenfunctions $ (e_{i})_{i} $ for $ T_K $ that form an orthonormal basis of $L^2(\mu)$, and a corresponding set of non-negative eigenvalues $ (\lambda_{i})_{i} $ such that&lt;/p&gt;
&lt;p&gt;$$ T_K(e_i) = \lambda_i e_i, \quad (i \in \N) $$&lt;/p&gt;
&lt;p&gt;Moreover, $K$ has the expansion&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \sum_{i} \lambda_i e_i(x) e_i(y), \quad (x,y \in \X) $$&lt;/p&gt;
&lt;p&gt;where the convergence of the series above holds absolutely and uniformly.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof.&lt;/p&gt;
&lt;p&gt;Among other things, Mercer&amp;rsquo;s theorem provides a framework for embedding an element of $\X$ into an element of $\ell^2(\N)$ (for its definition, see Example-4 in the section on Hilbert spaces in the 
&lt;a href=&#34;http://makkar.github.io/post/kernels/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first part&lt;/a&gt;). More concretely, given the eigenfunctions and eigenvalues guaranteed by Mercer&amp;rsquo;s theorem, we may define a mapping $\Phi : \X \to \ell^2(\N)$ as follows&lt;/p&gt;
&lt;p&gt;$$ x \mapsto \left( \sqrt{\lambda_i} e_i(x) \right)_{i \in \N} $$&lt;/p&gt;
&lt;p&gt;Therefore, we have&lt;/p&gt;
&lt;p&gt;$$\inner{\Phi(x)}{\Phi(y)} = \sum_{i=1}^{\infty} \lambda_i e_i(x) e_i(y) = K(x,y) $$&lt;/p&gt;
&lt;p&gt;This is the well-known &amp;ldquo;kernel trick&amp;rdquo;. Let us connect Mercer&amp;rsquo;s theorem to the Spectral theorem.&lt;/p&gt;
&lt;p&gt;Let $\X = [d] := \{1, 2, \ldots, d\}$ along with the Hamming metric be our compact metric space. Let $\mu(\{i\}) = 1$ for all $i \in [d]$ be the counting measure on $\X$. Any function $f : \X \to \C$ is equivalent to the $d$-dimensional vector $[f(1), \ldots, f(d)]$, and any kernel function $K : \X \times \X \to \C$ is continuous, satisfies the Hilbert-Schmidt condition, and is equivalent to a $d \times d$ normal matrix $\matr{K}$ where $\matr{K}_{i,j} = K(i, j)$. The Hilbert-Schmidt operator reduces to&lt;/p&gt;
&lt;p&gt;$$ T_K(f)(x) = \int_{\X} K(x, y) f(y) \dmu(y) = \sum_{i=1}^{d} K(x,y) f(y) $$&lt;/p&gt;
&lt;p&gt;Mercer&amp;rsquo;s theorem then states that there exists a set of eigenfunctions $v_1, \ldots, v_d$ (for our $\X$ they are equivalent to vectors) and the corresponding eigenvalues $\lambda_1, \ldots, \lambda_d$ such that&lt;/p&gt;
&lt;p&gt;$$ \matr{K} = \sum_{i=1}^{d} \lambda_i v_i v_i^T $$&lt;/p&gt;
&lt;p&gt;which is exactly the spectral theorem.&lt;/p&gt;
&lt;h1 id=&#34;operations-on-kernels&#34;&gt;Operations on kernels&lt;/h1&gt;
&lt;p&gt;Let us now consider how various algebraic operations on kernels affect the corresponding Hilbert spaces. All this (and a lot more) can be found in the seminal paper by Aronszajn &amp;ldquo;Theory of Reproducing Kernels&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;I state the following theorems without proof to illustrate how operations on kernels are done.&lt;/p&gt;
&lt;h2 id=&#34;sums-of-kernels&#34;&gt;Sums of kernels&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem 6:&lt;/strong&gt; Suppose that $\H_1$ and $\H_2$ are both RKHSs with kernels $K_1$ and $K_2$, respectively. Then the space&lt;/p&gt;
&lt;p&gt;$$\H = \H_1 + \H_2 := \{f_1 + f_2 \, : \, f_1 \in \H_1 \text{ and } f_2 \in \H_2 \}$$&lt;/p&gt;
&lt;p&gt;with the norm&lt;/p&gt;
&lt;p&gt;$$ \norm{f}^{2}_{\H} := \inf \left\{ \lVert f_1 \rVert^{2} _ {\H_1} + \norm{f_2}^{2} _{\H_2} \, : \, f = f_1 + f_2, f_1 \in \H_1, f_2 \in \H_2 \right\} $$&lt;/p&gt;
&lt;p&gt;is an RKHS with the kernel $K = K_1 + K_2$.&lt;/p&gt;
&lt;h2 id=&#34;products-of-kernels&#34;&gt;Products of kernels&lt;/h2&gt;
&lt;p&gt;Let us first define the notion of tensor product of two (separable) Hilbert spaces $\H_1$ and $\H_2$ of functions, say with domains $\X_1$ and $\X_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 27:&lt;/strong&gt; Consider the set of functions $h : \X_1 \times \X_2 \to \C$ satisfying&lt;/p&gt;
&lt;p&gt;$$\H = \left\{ h = \sum_{i=1}^{n} u_i v_i \, : \, n \in \N \text{ and } u_i \in \H_1, v_i \in \H_2 \text{ for all } i \in [n] \right\} $$&lt;/p&gt;
&lt;p&gt;We define an inner product on $\H$ as follows: for $h = \sum_{i=1}^{n} u_i v_i$ and $g = \sum_{j=1}^{m} w_j x_j$ in $\H$ define&lt;/p&gt;
&lt;p&gt;$$ \inner{h}{g} := \sum_{i=1}^{n} \sum_{j=1}^{m} \inner{u_i}{w_j}_{\H_1} \inner{v_i}{x_j}_{\H_2} $$&lt;/p&gt;
&lt;p&gt;Then $\H$ is a Hilbert space and is called the &lt;em&gt;tensor product&lt;/em&gt; of $\H_1$ and $\H_2$. We denote it by $\H = \H_1 \otimes \H_2$.&lt;/p&gt;
&lt;p&gt;We can now state the theorem for product of kernels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 7:&lt;/strong&gt; Suppose that $\H_1$ and $\H_2$ are RKHSs of real-valued functions with domains $\X_1$ and $\X_2$, and equipped with kernels $K_1$ and $K_2$, respectively. Then the tensor product space $\H = \H_1 \otimes \H_2$ is an RKHS of functions with domain $\X_1 \times \X_2$, and with kernel function $K : (\X_1 \times \X_2) \times (\X_1 \times \X_2) \to \C$ defined by&lt;/p&gt;
&lt;p&gt;$$ K((x,s), (y,t)) := K_1(x,y) K_2(s,t) $$&lt;/p&gt;
&lt;p&gt;$K$ is called the tensor product of the kernels $K_1$ and $K_2$, and denoted by $K = K_1 \otimes K_2$.&lt;/p&gt;
&lt;h2 id=&#34;other-operations&#34;&gt;Other operations&lt;/h2&gt;
&lt;p&gt;We can similarly define more operations on kernels:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If $K$ is a valid kernel and $\alpha \geq 0$, then $\alpha K$ is a valid kernel.&lt;/li&gt;
&lt;li&gt;If $K$ is a valid kernel and $\alpha \geq 0$, then $K + \alpha$ is a valid kernel.&lt;/li&gt;
&lt;li&gt;We can easily see from all these results that a linear combination or more generally for any polynomial $P$ with positive coefficients, the composition $P \circ K$ is a valid kernel if $K$ is a valid kernel.&lt;/li&gt;
&lt;li&gt;If $K$ is a valid kernel, then $\exp(K)$ is a valid kernel.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;representer-theorem&#34;&gt;Representer theorem&lt;/h1&gt;
&lt;p&gt;We are now at a stage where we can put all this theory to use in machine learning. Specifically we will develop Representer theorem which allows many optimization problems over the RKHS to be reduced to relatively simple calculations involving the gram matrix.&lt;/p&gt;
&lt;p&gt;Let us start with a functional analytic viewpoint of supervised learning. Suppose we are given empirical data&lt;/p&gt;
&lt;p&gt;$$ (x_1, y_1), \ldots, (x_n, y_n) \in \X \times \Y $$&lt;/p&gt;
&lt;p&gt;where $\X$ is a nonempty set. For now let $\Y = \R$. They are from an unknown function, $g : \X \to \R$, i.e., we assume&lt;/p&gt;
&lt;p&gt;$$ y_i = g(x_i), \quad (i \in [n]) $$&lt;/p&gt;
&lt;p&gt;We need to find some function $f^*$ which &amp;ldquo;best&amp;rdquo; approximates $g$. A natural way to formalize the notion of &amp;ldquo;best&amp;rdquo; is to limit ourselves to an RKHS $\H$ which contains functions of the form $f : \X \to \R$ and choose&lt;/p&gt;
&lt;p&gt;$$ f^* = \argmin_{f \in \H} \norm{f} \quad \text{ such that } f^*(x_i) = y_i \text{ for } i \in [n]$$&lt;/p&gt;
&lt;p&gt;This optimization problem is feasible whenever there exists at least one function $f \in \H$ that fits the data exactly. Denote by $y$ the vector $[y_1, \ldots, y_n]^T$. It can be shown that if $\matr{K}$ is the gram matrix of the kernel $K$ with respect to $x_1, \ldots, x_n$, then the feasibility is equivalent to $y \in \text{range}(\matr{K})$. This is a special of the representer theorem.&lt;/p&gt;
&lt;p&gt;In a realistic setting we assume that we have noisy observations, i.e.,&lt;/p&gt;
&lt;p&gt;$$y_i = g(x_i) + \e_i, \quad (i \in [n])$$&lt;/p&gt;
&lt;p&gt;where $\e_i$&#39;s denote the noise. Then the constraint of exact fitting is no longer desirable, and we model the &amp;ldquo;best&amp;rdquo; approximation by introducing a loss function which represents how close our approximation is to the observed outputs. More concretely, let $L_y : \R^n \to \R$ be a continuous function. Then we can define our cost function as&lt;/p&gt;
&lt;p&gt;$$ J(f) = \norm{f}^2 + L_y(f(x_1), \ldots, f(x_n)) $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 8 [Representer theorem]:&lt;/strong&gt; If $f^*$ is a function such that&lt;/p&gt;
&lt;p&gt;$$ J(f^*) = \inf_{f \in \H} J(f) $$&lt;/p&gt;
&lt;p&gt;then $f^*$ is in the span of the functions $k_{x_1}, \ldots, k_{x_n}$, i.e.,&lt;/p&gt;
&lt;p&gt;$$ f^*(\cdot) = \sum_{i=1}^{n} \alpha_i k_{x_i}(\cdot) \quad \text{for some } \alpha_1, \ldots, \alpha_n \in \C$$&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof which can be found in Paulsen and Raghupathi&amp;rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces or in Schölkopf, Herbrich and Smola&amp;rsquo;s A Generalized Representer Theorem.&lt;/p&gt;
&lt;p&gt;As an example $L$ could be the squared loss $L_y(f(x_1), \ldots, f(x_n)) = \sum_{i=1}^n (y_i - f(x_i))^2$. If we assume $L$ to be convex then the solution exists and is unique. Recall that $E \subseteq \R^k$ is a &lt;em&gt;convex set&lt;/em&gt; if $\lambda x + (1-\lambda) y \in E$ whenever $x,y \in E$ and $0 &amp;lt; \lambda &amp;lt; 1$. A real-valued function $L$ defined on a convex set $E$ is a &lt;em&gt;convex function&lt;/em&gt; if $L(\lambda x + (1-\lambda) y) \leq \lambda L(x) + (1-\lambda) L(y)$ whenever $x, y \in E$ and $0 &amp;lt; \lambda &amp;lt; 1$. Note that a convex function is continuous.&lt;/p&gt;
&lt;p&gt;The Representer theorem allows us to reduce the infinite-dimensional problem of optimizing over an RKHS to an $n$-dimensional problem.&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;I barely scratched the surface of reproducing kernel Hilbert space theory. Some resources I recommend that do a much better job than me in explaining this theory and which I used as reference are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Paulsen V and Raghupathi M: An Introduction to the Theory of Reproducing Kernel Hilbert Spaces&lt;/li&gt;
&lt;li&gt;Wainwright M: High-Dimensional Statistics&lt;/li&gt;
&lt;li&gt;Schölkopf B, Herbrich R and Smola A.J: A Generalized Representer Theorem (COLT 2001)&lt;/li&gt;
&lt;li&gt;Aronszajn N: Theory of Reproducing Kernels (1950)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the 
&lt;a href=&#34;https://makkar.github.io/post/kernels2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next article&lt;/a&gt; I will discuss kernel approximation methods. In particular I will focus on random Fourier Features developed by Rahimi and Recht which I am using in my work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kernels - Part 0</title>
      <link>https://makkar.github.io/post/kernels/</link>
      <pubDate>Fri, 03 Jul 2020 21:35:10 -0400</pubDate>
      <guid>https://makkar.github.io/post/kernels/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\conj}[1]{\overline{#1}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
$$&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The word &amp;ldquo;kernel&amp;rdquo; is 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;heavily overloaded&lt;/a&gt;, but for our purposes it is, intuitively, a similarity measure that can be thought of as an inner product in some feature space. Kernel methods provide an elegant, theoretically well-founded, and powerful approach to solving many learning problems.&lt;/p&gt;
&lt;p&gt;We usually have the following framework: The input space $\X$ which contains our observations/inputs/features is either not rich enough (for example, if there is no linear boundary separating the two classes in a binary classification problem) or not convenient (for example, if our inputs are strings), and therefore we want to work is some other space we call feature space $\H$. Suppose we have a map which takes our inputs from $\X$ to $\H$:
$$ \Phi : \X \to \H $$
Then the class of kernels we are interested in are those for which is it possible to write
$$ K(x,y) = \langle \Phi(x), \Phi(y) \rangle, \quad x,y \in \X $$
What kind of functions, $K : \H \times \H \to \C$ admit such a representation? We aim to be able to answer this question and many others in this series of articles on kernels.&lt;/p&gt;
&lt;p&gt;In this article, I aim to introduce the necessary concepts from analysis, linear algebra, and functional analysis so as to understand Reproducing Kernel Hilbert Spaces (RKHS) and a few of their basic properties. In the 
&lt;a href=&#34;http://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next article&lt;/a&gt; I will discuss the kernel trick and some important theorems like Mercer&amp;rsquo;s theorem and Representer theorem.&lt;/p&gt;
&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;p&gt;A topic like this requires quite a bit of background before we can get to the interesting results like the one above. It&amp;rsquo;s always easy to get lazy and assume all the necessary background from the reader and get straight to the meat, but I want to write an article which I would have found useful had I had it when I started learning about kernel theory. With that being said, I am under no illusion and believe that a much better way to learn this background would be to read a functional analysis text if you have the time.&lt;/p&gt;
&lt;h2 id=&#34;linear-spaces&#34;&gt;Linear spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; A &lt;em&gt;linear space&lt;/em&gt;, or alternatively a &lt;em&gt;vector space&lt;/em&gt;, over a field $\F$ ($\F$ is $\R$ or $\C$ for our purposes) is a set $V$ of elements called &lt;em&gt;vectors&lt;/em&gt; (the elements of $\F$ are called &lt;em&gt;scalars&lt;/em&gt;) satisfying:&lt;/p&gt;
&lt;p&gt;(A) To every pair, $x$ and $y$, of vectors in $V$ there corresponds a vector $x+y$, called the sum of $x$ and $y$, in such a way that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;addition is commutative, $x+y = y+x$,&lt;/li&gt;
&lt;li&gt;addition is associative, $x+(y+z) = (x+y)+z$,&lt;/li&gt;
&lt;li&gt;there exists in $V$ a unique vector $0$ such that $x+0=x$ for every vector $x$, and&lt;/li&gt;
&lt;li&gt;to every vector $x \in V$ there corresponds a unique vector $-x$ such that $x+(-x)=0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(B) To every pair, $\alpha \in \F$ and $x \in V$, there corresponds a vector $\alpha x \in V$, called the product of $\alpha$ and $x$, in such a way that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;multiplication by scalars is associative, $\alpha(\beta x) = (\alpha \beta)x$, and&lt;/li&gt;
&lt;li&gt;$1x = x$ for every vector $x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(C) Finally the distributive properties&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha(x+y) = \alpha x + \alpha y$, and&lt;/li&gt;
&lt;li&gt;$(\alpha + \beta) x = \alpha x + \beta x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The Euclidean spaces $\R^n$ are vector spaces over the real field.&lt;/li&gt;
&lt;li&gt;$\C^n$ are vector spaces over $\C$.&lt;/li&gt;
&lt;li&gt;The set of all polynomials, with complex coefficients, in a variable $t$ is a vector space over $\C$.&lt;/li&gt;
&lt;li&gt;The set $C$ of all continuous complex functions on the unit interval $[0,1]$ is a vector space over $\C$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; A &lt;em&gt;linear transformation&lt;/em&gt; of a linear space $V$ into a linear space $W$ is a mapping $T: V \to W$ such that
$$ T(\alpha x + \beta y) = \alpha T(x) + \beta T(y), \quad  (x,y \in V;\; \alpha, \beta \in \F) $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3:&lt;/strong&gt; In the special case in which $W$ above is a field, $T$ is called a &lt;em&gt;linear functional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note that we often write $Tx$ instead of $T(x)$, if $T$ is linear, hinting that a linear transformation mapping a finite dimensional vector to another finite dimensional vector space is equivalent to a matrix vector product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 4:&lt;/strong&gt; Let $\mu$ be a positive measure on an arbitrary measurable space $X$. We define $L^1(\mu)$ to be the collection of all complex measurable functions $f$ on $X$ for which
$$ \int_X |f| \dmu &amp;lt; \infty $$&lt;/p&gt;
&lt;p&gt;It can be shown that for every $f, g \in L^1(\mu)$ and for every $\alpha, \beta \in \C$, we have $\alpha f + \beta g \in L^1(\mu)$, and
$$ \int_X (\alpha f + \beta g) \dmu = \alpha \int_X f \dmu + \beta \int_X g \dmu $$&lt;/p&gt;
&lt;p&gt;Thus, $L^1(\mu)$ is a vector space, and the mapping $F: L^1(\mu) \to \R$ defined by
$$ F(f) = \int_X |f| \dmu, \quad f \in L^1(\mu) $$
is a linear functional.&lt;/p&gt;
&lt;h2 id=&#34;inner-products-and-norms&#34;&gt;Inner products and norms&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition 5:&lt;/strong&gt; If $V$ be a linear space over $\C$, an &lt;em&gt;inner product&lt;/em&gt; on $V$ is a function $\langle \cdot , \cdot \rangle: V \times V \to \C$ such that for all $\alpha, \beta \in \C$, and all $x,y,z \in V$, the following are satisfied:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Linearity in the first argument: $\langle \alpha x + \beta y, z \rangle = \alpha \langle x,z \rangle + \beta \langle y,z \rangle$,&lt;/li&gt;
&lt;li&gt;Conjugate symmetry: $\inner{x}{y} = \conj{\inner{y}{x}}$,&lt;/li&gt;
&lt;li&gt;Positivity: $\inner{x}{x} \geq 0$,&lt;/li&gt;
&lt;li&gt;If $\inner{x}{x} = 0$, then $x=0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A function satisfying only the first three properties is called a &lt;em&gt;semi-inner product&lt;/em&gt; on $V$.&lt;/p&gt;
&lt;p&gt;An immediate consequences of this definition: For every $y \in V$, the mapping $F: V \to \C$ defined by
$$ F(x) = \inner{x}{y}, \quad (x \in V) $$
is a linear functional on $V$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 6:&lt;/strong&gt; If $V$ be a linear space over $\C$, a &lt;em&gt;norm&lt;/em&gt; on $V$ is a non-negative function $\norm{\cdot} : V \to \R$ such that for all $\alpha \in \C$, and all $x,y \in V$, the following are satisfied:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Subadditivity: $\norm{x+y} \leq \norm{x} + \norm{y}$,&lt;/li&gt;
&lt;li&gt;Absolutely homogenous: $\norm{\alpha x} = |\alpha| \norm{x}$,&lt;/li&gt;
&lt;li&gt;Positive definite: $\norm{x} = 0 \implies x = 0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given an inner product, we can define a &lt;em&gt;norm&lt;/em&gt; as follows:
$$ \norm{x} = \sqrt{\inner{x}{x}} $$&lt;/p&gt;
&lt;p&gt;A classic result useful in many proofs:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1:&lt;/strong&gt; Schwarz inequality
$$|\inner{x}{y}| \leq \norm{x} \, \lVert y \rVert$$
Equality hold for $y = \alpha x$ or $y=0$.&lt;/p&gt;
&lt;p&gt;The proof is not too difficult.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 7:&lt;/strong&gt; The virtue of norm on a vector space $V$ is that
$$ d(x,y) := \norm{ x-y } $$
defines a &lt;em&gt;metric&lt;/em&gt; on $V$ so that $V$ becomes a metric space.&lt;/p&gt;
&lt;p&gt;Note that technically the tuple $(V, d)$ defines a metric space, but I will often write just $V$ instead of $(V, d)$ when it&amp;rsquo;s clear from context what the metric $d$ is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 8:&lt;/strong&gt; If $0 &amp;lt; p &amp;lt; \infty$, $f$ is a complex measurable function on $X$, and $\mu$ is a nonnegative measure on $X$, define
$$ \norm{f}_p := \left( \int_X |f|^p \dmu \right)^{1/p} $$
and let $L^p(\mu)$ consist of all $f$ for which $\norm{f}_p &amp;lt; \infty$. We call $\norm{f}_p$ the $L^p$-norm of $f$.&lt;/p&gt;
&lt;h2 id=&#34;hilbert-spaces&#34;&gt;Hilbert spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition 9:&lt;/strong&gt; An &lt;em&gt;inner product space&lt;/em&gt;, or alternatively a &lt;em&gt;pre-Hilbert space&lt;/em&gt;, is a linear space with an inner product defined on it.&lt;/p&gt;
&lt;p&gt;We need the concept of completeness to define Hilbert space. But before that let me define Cauchy sequences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 10:&lt;/strong&gt; Given a metric space $(M, d)$, a sequence $(x_n)_{n \in \N}$ of elements in $M$ is called a &lt;em&gt;Cauchy sequence&lt;/em&gt; if for every positive real number $\e &amp;gt; 0$ there exists a positive integer $N \in \N$ such that $m, n &amp;gt; N$ implies that $d(x_m, x_n) &amp;lt; \e$.&lt;/p&gt;
&lt;p&gt;Recall that we say a sequence $(x_n)_{n \in \N}$ in a metric space $(M, d)$ &lt;em&gt;converges&lt;/em&gt; if there exists a point $x \in M$ with the following property: for every $\e &amp;gt; 0$ there exists a positive integer $N \in \N$ such that $n &amp;gt; N$ implies that $d(x_n, x) &amp;lt; \e$.&lt;/p&gt;
&lt;p&gt;It can be shown that every convergent sequence is a Cauchy sequence: Let the sequence $(x_n)_{n \in \N}$ in a metric space $(M, d)$ converge to $x \in M$. If $\e &amp;gt; 0$, there is an integer $N \in \N$ such that $d(x_n, x) &amp;lt; \e$ for all $n &amp;gt; N$. Hence&lt;/p&gt;
&lt;p&gt;$$d(x_n, x_m) \leq d(x_n, x) + d(x, x_m) &amp;lt; 2\e$$&lt;/p&gt;
&lt;p&gt;if $n,m &amp;gt; N$. Thus $(x_n)_{n \in \N}$ is a Cauchy sequence.&lt;/p&gt;
&lt;p&gt;The converse is not necessarily true. But if it holds in some space, we anoint the space with a special name.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 11:&lt;/strong&gt; A metric space $(M, d)$ is called &lt;em&gt;complete&lt;/em&gt; if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 12:&lt;/strong&gt; A pre-Hilbert space $\H$ is called a &lt;em&gt;Hilbert space&lt;/em&gt; if it is complete in the metric $d$ (see Definition 7).&lt;/p&gt;
&lt;h3 id=&#34;examples-1&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The sets $\R^n$ and $\C^n$ are Hilbert spaces if we define $\inner{x}{y} := \sum_{i=1}^n x_i \conj{y_i}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The set $C$ of all continuous complex functions on the unit interval $[0,1]$ defined above is an inner product space if we define
$$ \inner{f}{g} := \int_0^1 f(x) \conj{g(x)} \dx $$
but is not a Hilbert space.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$L^2(\mu)$ is a Hilbert space, with inner product
$$ \inner{f}{g} := \int_X f \, \conj{g} \dmu $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The space of square-summable real-valued sequences, namely
$$ \ell^2(\N) := \left\{ (x_n)_{n \in \N} \; : \; x_n \in \R,\, \sum_n x_n^2 &amp;lt; \infty \right\} $$&lt;/p&gt;
&lt;p&gt;This set, when endowed with the inner product $\inner{x}{y} := \sum_{n \in \N} x_n y_n$, defines a Hilbert space. It will play an important role in our discussion of eigenfunctions for Reproducing Kernel Hilbert spaces.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Definition 13:&lt;/strong&gt; Consider a linear space $\FF$ of functions each of which is a mapping from a set $X$ into $\F$. For $x \in X$, a &lt;em&gt;linear evaluation functional&lt;/em&gt; is a linear functional $E_x$ that is defined as
$$ E_x(f) = f(x), \quad (f \in \FF) $$
In other words, a linear evaluation functional with respect to $x \in X$ evaluates each function at $x$.&lt;/p&gt;
&lt;p&gt;In general, the evaluation functional is not continuous. This means we can have $f_n \to f$ but $E_x(f_n)$ does not converge to $E_x(f)$. Intuitively, this is because Hilbert spaces can contain very unsmooth functions. We will later consider a special type of Hilbert space, Reproducing Kernel Hilbert Space where evaluation functional is continuous.&lt;/p&gt;
&lt;p&gt;A lemma that will be useful later on:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt; Let $\H$ be a Hilbert space and $L:\H \to \F$ a linear functional. The following statements are equivalent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$L$ is continuous.&lt;/li&gt;
&lt;li&gt;$L$ is continuous at $0$.&lt;/li&gt;
&lt;li&gt;$L$ is continuous at some point.&lt;/li&gt;
&lt;li&gt;$L$ is bounded, i.e., there is a constant $c &amp;gt; 0$ such that $|L(f)| \leq c \norm{f}$ for every $f \in \H$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; It is clear that $(1) \implies (2) \implies (3)$, and $(4) \implies (2)$. Let&amp;rsquo;s show that $(3) \implies (1)$, and $(2) \implies (4)$.&lt;/p&gt;
&lt;p&gt;$(3) \implies (1)$: Suppose $L$ is continuous at $f$ and $g$ is any point in $\H$. If $g_n \to g$ in $\H$, then $g_n - g + f \to f$. By assumption $L(f) = \limn L(g_n - g + f) = \limn L(g_n) - L(g) + L(f)$. Hence $L(g) = \limn L(g_n)$.&lt;/p&gt;
&lt;p&gt;$(2) \implies (4)$: The definition of continuity at $0$ implies that $L^{-1}(\{\alpha \in \F : |\alpha| &amp;lt; 1\})$ contains an open ball centered at $0$. Let $\delta &amp;gt; 0$ be the radius of that open ball centered at $0$. Then for $f \in \H$ and $\norm{f} &amp;lt; \delta$ we have $|L(f)| &amp;lt; 1$. If $f$ is an arbitrary element of $\H$ and $\e &amp;gt; 0$, then
$$ \left\lVert \frac{\delta f}{\norm{f} + \e} \right\rVert &amp;lt; \delta $$
Hence,
$$ 1 &amp;gt; \left\lvert L\left( \frac{\delta f}{\norm{f} + \e} \right) \right\rvert = \frac{\delta }{\norm{f} + \e} |L(f)| $$
Letting $\e \to 0$ we see that $(4)$ holds with $c = 1/\delta$. $\square$&lt;/p&gt;
&lt;h3 id=&#34;orthonormal-bases&#34;&gt;Orthonormal bases&lt;/h3&gt;
&lt;p&gt;We generalize the idea of orthonormal basis to infinite dimensional case. This will be needed when we discuss Mercer&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 14:&lt;/strong&gt; A collection of vectors $\{v_{\alpha} \, : \, \alpha \in A \}$ in a Hilbert space $\H$ for some index set $A$ is called &lt;em&gt;orthonormal&lt;/em&gt; if it satisfies $\inner{v_{\alpha}}{v_{\beta}} = \delta_{\alpha \beta}$ where $\delta_{\alpha \beta}$ is the Kronecker delta, which equals $1$ if $\alpha = \beta$ and $0$ otherwise.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 15:&lt;/strong&gt; A collection of vectors $\{v_{\alpha} \, : \, \alpha \in A \}$ in a Hilbert space $\H$ is &lt;em&gt;complete&lt;/em&gt; if for any $u \in \H$, $\inner{u}{v_{\alpha}} = 0$ for all $\alpha \in A$ implies that $u = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 16:&lt;/strong&gt; An &lt;em&gt;orthonormal basis&lt;/em&gt; a complete orthonormal system.&lt;/p&gt;
&lt;p&gt;Note, we can also define an orthonormal basis as a maximal orthonormal set in $\H$. To say $\{v_{\alpha}\}$ is maximal means that no vector of $\H$ can be added to $\{v_{\alpha}\}$ in such a way that the resulting set is still orthonormal. This happens precisely when there is no $u \neq 0$ in $\H$ that is orthogonal to every $v_{\alpha}$.&lt;/p&gt;
&lt;h3 id=&#34;separable-hilbert-spaces&#34;&gt;Separable Hilbert spaces&lt;/h3&gt;
&lt;p&gt;Another idea we need is separability. Let us define that now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 17:&lt;/strong&gt; A topological space is called &lt;em&gt;separable&lt;/em&gt; if it contains a countable, dense subset; that is, there exists a sequence $(x_{n})_{n=1}^{\infty }$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 18:&lt;/strong&gt; A Hilbert space is separable if and only if it has a countable orthonormal basis. It follows that any separable, infinite-dimensional Hilbert space is isometric to the space $\ell^2(\N)$ of square-summable sequences.&lt;/p&gt;
&lt;p&gt;We will be dealing with separable Hilbert spaces in our discussion.&lt;/p&gt;
&lt;h2 id=&#34;riesz-representation-theorem&#34;&gt;Riesz representation theorem&lt;/h2&gt;
&lt;p&gt;We now come to a very important theorem called the Riesz representation theorem. The name Riesz has 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Riesz_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many theorems&lt;/a&gt; attached to it, but the one relevant to us is the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2:&lt;/strong&gt; For each continuous linear functional $L$ on a Hilbert space $\H$, there exists a unique $g \in \H$ such that
$$L(f) = \inner{f}{g}\, , \quad (f \in \H) $$&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof as it&amp;rsquo;s not easy and will unnecessarily make this article abstruse.&lt;/p&gt;
&lt;p&gt;Side-note: In the mathematical treatment of quantum mechanics, this theorem can be seen as a justification for the popular bra–ket notation.&lt;/p&gt;
&lt;h1 id=&#34;reproducing-kernel-hilbert-spaces-rkhs&#34;&gt;Reproducing Kernel Hilbert Spaces (RKHS)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Definition 19:&lt;/strong&gt; Let $\X$ be a set. We will call a set $\H$ of functions from $\X$ to $\F$ a &lt;em&gt;Reproducing Kernel Hilbert Space (RKHS)&lt;/em&gt; on $\X$ if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\H$ is a vector space,&lt;/li&gt;
&lt;li&gt;$\H$ is endowed with an inner product, $\inner{\cdot}{\cdot}$, with respect to which $\H$ is a Hilbert space,&lt;/li&gt;
&lt;li&gt;for every $x \in \X$, the linear evaluation functional $E_x : \H \to \F$, is bounded (or continuous, as seen from Lemma-1).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If $\H$ is an RKHS on $\X$, then an application of the Riesz representation theorem shows that the linear evaluation functional is given by the inner product with a unique vector in $\H$. Therefore, for each $x \in \X$, there exists a unique vector $k_x \in \H$, such that for every $f \in \H$,
$$ f(x) = E_x(f) = \inner{f}{k_x} $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 20:&lt;/strong&gt; The function $k_x$ is called the &lt;em&gt;reproducing kernel for the point $x$&lt;/em&gt;. The function $K: \X \times \X \to \F$ defined by
$$ K(x,y) = k_y(x) $$
is called the &lt;em&gt;reproducing kernel for $\H$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note that we have
$$ K(x,y) = k_y(x) = \inner{k_y}{k_x} = \conj{\inner{k_x}{k_y}} = \conj{K(y,x)}$$&lt;/p&gt;
&lt;p&gt;Also,
$$ \norm{E_y}^2 = \norm{k_y}^2 = \inner{k_y}{k_y} = K(y,y) $$&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;The first question that comes to mind is if any Reproducing Kernel Hilbert Spaces exist. The following example answers this question in the affirmative.&lt;/p&gt;
&lt;p&gt;We saw before that $\C^n$ is a Hilbert space. We can show that $\C^n$ is in fact an RKHS. Let $\X = \{1, 2, \ldots, n\}$, then we can view $v \in \C$ as a function $V : \X \to \C$, where $V(j) = v_j$. The linear evaluation functionals are of course bounded for every $x \in \X$ and we have
$$ V(j) = v_j = \inner{V}{e_j}\,, \quad (j \in \X) $$
where $e_j$ is a vector with $1$ at $j$&lt;sup&gt;th&lt;/sup&gt; position and $0$ everywhere else. Therefore, the reproducing kernel for the point $x \in \X$ is $e_x$ and the reproducing kernel can be thought as the identity matrix.&lt;/p&gt;
&lt;p&gt;Can there be multiple reproducing kernels for an RKHS? The following theorem answers this question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3:&lt;/strong&gt; If an RKHS $\H$ of functions on a set $\X$ admits a reproducing kernel, $K$, then  $K$ is uniquely determined by $\H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose that there exists another reproducing kernel $K&#39;$ for $\H$. Then
$$
\norm{k_y - k&amp;rsquo;_y} = \inner{k_y - k&amp;rsquo;_y}{k_y - k&amp;rsquo;_y} = \inner{k_y - k&amp;rsquo;_y}{k_y} - \inner{k_y - k&amp;rsquo;_y}{k&amp;rsquo;_y} = (k_y - k&amp;rsquo;_y)(y) - (k_y - k&amp;rsquo;_y)(y) = 0
$$
for any $y \in \X$. In other words, $k_y(x) = k&amp;rsquo;_y(x)$ for every $x \in \X$ by the positive definite property of norms and hence the kernel is unique. $\square$&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;We covered quite a lot of ground in this blog post but I didn&amp;rsquo;t even define a kernel as we commonly use in machine learning!
In the 
&lt;a href=&#34;http://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next post&lt;/a&gt; I will do that and cover its fundamental properties.&lt;/p&gt;
&lt;p&gt;Some resources I recommend to go into more depth on what&amp;rsquo;s covered here are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Halmos, P: Finite-Dimensional Vector Spaces.&lt;/li&gt;
&lt;li&gt;Rudin, W: Real and Complex Analysis. (Chapter - 4)&lt;/li&gt;
&lt;li&gt;Rudin, W: Functional Analysis.&lt;/li&gt;
&lt;li&gt;Conway, J: A course in functional analysis.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Partitions and Equivalence Relations</title>
      <link>https://makkar.github.io/post/equivalence-relations/</link>
      <pubDate>Sat, 20 Jun 2020 19:16:45 -0400</pubDate>
      <guid>https://makkar.github.io/post/equivalence-relations/</guid>
      <description>&lt;p&gt;I want to show how these two concepts are a single mathematical idea.&lt;/p&gt;
&lt;h2 id=&#34;partition&#34;&gt;Partition&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;partition&lt;/em&gt; of a non-empty set, $X$, is a disjoint class $\{X_i\}_{i \in I}$ of non-empty subsets of $X$ whose union is $X$. The $X_i$&#39;s are called the &lt;em&gt;partition sets&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For example, if $X = \mathbb{R}$, then $X$ can be partitioned as
$$
X = \bigcup_{n \in \mathbb{Z}}[n, n+1)
$$&lt;/p&gt;
&lt;h2 id=&#34;binary-relation&#34;&gt;Binary Relation&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;binary relation&lt;/em&gt;, or simply &lt;em&gt;relation&lt;/em&gt;, $R$, in the set $X$ is a subset of $X \times X$.&lt;/p&gt;
&lt;p&gt;For $x, y \in X$, we denote the fact $(x,y) \in R$ by writing $x R y$. A function may be defined as a special kind of binary relation.&lt;/p&gt;
&lt;h2 id=&#34;equivalence-relation&#34;&gt;Equivalence relation&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume that a partition of our non-empty set $X$ is given, and we associate with this partition a relation, $\sim$, in $X$ defined as follows: $x \sim y$ if $x$ and $y$ belong to the same partition set. It can easily checked that the relation $\sim$ satisfies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$x \sim x$ for every $x \in X$ (&lt;em&gt;reflexivity&lt;/em&gt;);&lt;/li&gt;
&lt;li&gt;$x \sim y \implies y \sim x$ (&lt;em&gt;symmetry&lt;/em&gt;);&lt;/li&gt;
&lt;li&gt;$x \sim y$ and $y \sim z$ $\implies$ $x \sim z$ (&lt;em&gt;transitivity&lt;/em&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Any relation in $X$ which possesses these three properties is called an &lt;em&gt;equivalence relation&lt;/em&gt; in $X$.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Let $X = \mathbb{Z}$ and let $x \sim y$ if $2 | x-y$ for $x,y \in X$. Then clearly $\sim$ is an equivalence relation in $X$.&lt;/li&gt;
&lt;li&gt;Let $X$, $Y$ be any non-empty sets and $f$ be a mapping from $X$ onto $Y$. Let $x \sim y$ if $f(x) = f(y)$ for $x,y \in A$. This defines an equivalence relation in $X$. Indeed, $f(x) = f(x)$, and so $x \sim x$. If $f(x) = f(y)$ then $f(y) = f(x)$, and so $x \sim y \implies y \sim x$. Finally, if $f(x) = f(y)$ and $f(y) = f(z)$ then $f(x) = f(z)$, and so $x \sim y$ and $y \sim z$ $\implies$ $x \sim z$. The first example is a special case of this one if we take $X = \mathbb{Z}$, $Y = \{0,1\}$ and $f(x) = x \mod 2$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;relation-to-partition&#34;&gt;&amp;ldquo;Relation&amp;rdquo; to partition&lt;/h3&gt;
&lt;p&gt;We have just seen that each partition of $X$ has associated with it a natural equivalence relation in $X$. Let us now reverse the situation and show that a given equivalence relation in $X$ determines a natural partition of $X$.&lt;/p&gt;
&lt;p&gt;Let $\sim$ be an equivalence relation in $X$. For every $x \in X$ define the set
$$
[x] := \{ y \in X : y \sim x\}
$$
called the &lt;em&gt;equivalence set&lt;/em&gt; of $x$. We show that the class of all distinct equivalence sets forms a partition of $X$.&lt;/p&gt;
&lt;p&gt;By reflexivity, $x \in [x]$ for every $x \in X$, and thus each equivalence set is non-empty and their union is $X$. We now need to show that any two equivalence sets $[x_1]$ and $[x_2]$ are either disjoint or identical. We prove this by showing that if $[x_1]$ and $[x_2]$ are not disjoint then they are identical. To this end, let $z$ be a common element of $[x_1]$ and $[x_2]$. Let $y$ be any element of $[x_1]$. Using transitivity,
$$
y \sim x_1 \sim z \sim x_2
$$
Therefore, $y \in [x_2]$. Since $y$ was an arbitrary element of $[x_1]$, we get $[x_1] \subseteq [x_2]$. We can similarly show that $[x_2] \subseteq [x_1]$. In short, $[x_1] = [x_2]$.&lt;/p&gt;
&lt;p&gt;We have shown that there is no real distinction between partitions of a set and equivalence relations in the set. They are two &amp;ldquo;equivalent&amp;rdquo; approaches for the same mathematical idea. The approach we choose in an application depends entirely on our own convenience.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Upper and Lower Limits</title>
      <link>https://makkar.github.io/post/upper-and-lower-limits/</link>
      <pubDate>Wed, 17 Jun 2020 16:33:53 -0400</pubDate>
      <guid>https://makkar.github.io/post/upper-and-lower-limits/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\e}{\epsilon}
\newcommand{\limm}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sn}{\{s_n\}}
\newcommand{\snk}{\{s_{n_k}\}}
\newcommand{\supk}{\sup_{k \geq n}}
\newcommand{\infk}{\inf_{k \geq n}}
\newcommand{\sups}{\sup_{k \geq n} s_k}
\newcommand{\infs}{\inf_{k \geq n} s_k}
\newcommand{\cc}{\mathsf{c}}
$$&lt;/p&gt;
&lt;p&gt;Most of what follows is taken from Rudin&amp;rsquo;s Principles of Mathematical Analysis.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The concept of upper and lower limits (commonly denoted by $\limsup$ and $\liminf$ respectively) shows up routinely when discussing the limiting behaviour of a sequence. For example, consider the Big O notation, $\mathcal{O}$, defined as $f(n) = \mathcal{O}(g(n))$ iff there exists a positive real number $c$ and an integer $N$ such that for every $n \in \Z$, $n &amp;gt; N$, we have $|f(n)| \leq c g(n)$. This can be succinctly written as
$$
\lims \frac{|f(n)|}{g(n)} &amp;lt; \infty
$$&lt;/p&gt;
&lt;p&gt;In this post, I aim to expound on the concept of upper and lower limits so that the second formulation of Big O notation above becomes easier to understand than the first one.&lt;/p&gt;
&lt;h2 id=&#34;definitions&#34;&gt;Definitions&lt;/h2&gt;
&lt;p&gt;I start with some definitions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; A &lt;em&gt;sequence&lt;/em&gt; is function defined on the set $\N$ of positive integers. If $f(n) = x_n$, for $n \in \N$, we denote the sequence $f$ by the symbol $\{x_n\}$, or sometimes by $x_1, x_2, \ldots$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; A sequence $\{p_n\}$ in a metric space $X$ is said to &lt;em&gt;converge&lt;/em&gt; if there is a point $p \in X$ with the following property: For every $\e &amp;gt; 0$ there is an integer $N$ such that $n \geq N$ implies that $d(p_n, p) &amp;lt; \e$. We write this as $\limm p_n = p$ or as $p_n \to p$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3:&lt;/strong&gt; Let $\{s_n\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$. We then write $s_n \to +\infty$. Similarly for $s_n \to -\infty$.&lt;/p&gt;
&lt;p&gt;Note: The symbol $\to$ is now used for certain type of divergent sequences as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 4:&lt;/strong&gt; Given a sequence $\{p_n\}$, consider a sequence $\{n_k\}$ of positive integers, such that $n_1 &amp;lt; n_2 &amp;lt; \cdots$. Then the sequence $\{p_{n_k}\}$, which is a composition of the functions $\{n_k\}$ and $\{p_n\}$, is called a &lt;em&gt;subsequence&lt;/em&gt; of ${p_n}$. If $\{p_{n_k}\}$ converges, its limit is called a &lt;em&gt;subsequential limit&lt;/em&gt; of $\{p_n\}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 5:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system, i.e., $x \in \RR := \R \cup \{+\infty, -\infty\}$) such that $s_{n_k} \to x$ for some subsequence $\{s_{n_k}\}$. Therefore, this set contains all the subsequential limits of $\sn$ plus possibly the numbers $+\infty$ and $-\infty$. We define
$$
s^* = \sup E
$$
$$
s_* = \inf E
$$
The numbers $s^*$ and $s_*$ are called the &lt;em&gt;upper&lt;/em&gt; and &lt;em&gt;lower limits&lt;/em&gt; of $\sn$. We use the notation
$$
\lims s_n = s^*
$$
$$
\limi s_n = s_*
$$
It immediately follows that $s_* \leq s^*$.&lt;/p&gt;
&lt;p&gt;The fact that $E$ is non-empty (and thus taking $\sup$ or $\inf$ makes sense) follows from the observation that either $\sn$ is bounded or unbounded. If it is bounded then it must contain a convergent subsequence (Bolzano–Weierstrass theorem) and thus at least one element, or if it is unbounded then it must contain either $+\infty$ or $-\infty$.&lt;/p&gt;
&lt;h2 id=&#34;some-useful-lemmas&#34;&gt;Some useful lemmas&lt;/h2&gt;
&lt;p&gt;Now let us prove some interesting lemmas that will be useful later.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt; The subsequential limits of a sequence $\{p_n\}$ in a metric space $X$ form a closed subset of $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let $E$ be the set of all subsequential limits of $\{p_n\}$ and let $q$ be a limit point of $E$. We have to show that $q \in E$.
To show this we will construct a subsequence of $\{p_n\}$ which converges to $q$.&lt;/p&gt;
&lt;p&gt;Choose $n_1$ so that $p_{n_1} \neq q$. If no such $n_1$ exists, then $E$ has only one element, $q = p_1 = p_2 = \cdots$, and there is nothing to prove. Define $\delta = d(q, p_{n_1})$. Suppose $n_1, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E$, there is an $x \in E$ with $d(q, x) &amp;lt; \frac{\delta}{2^i}$. Since $x \in E$, there is an $n_i &amp;gt; n_{i-1}$ such that $d(x, p_{n_i}) &amp;lt; \frac{\delta}{2^i}$. Thus
$$
d(q, p_{n_i}) \leq d(q, x) + d(x, p_{n_i}) &amp;lt; \frac{\delta}{2^{i-1}} \quad \text{for } i = 1,2,\ldots
$$
This implies $p_{n_k} \to q$, and thus $q \in E$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2:&lt;/strong&gt; Let $F$ be a nonempty closed set of real numbers which is bounded above. Let $\alpha = \sup F$. Then $\alpha \in F$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Assume for the sake of the contradiction that $\alpha \notin F$. Then since $F^\cc$ is an open set (because $F$ is closed) there exists an $\e &amp;gt; 0$ such that $(\alpha - \e, \alpha + \e) \subset F^\cc$. But this implies $\alpha - \frac{\e}{2}$ is an upper bound for $F$ which is lower that $\alpha$. This gives us our required contradiction. $\square$&lt;/p&gt;
&lt;h2 id=&#34;properties&#34;&gt;Properties&lt;/h2&gt;
&lt;p&gt;We now have all the tools to prove a very useful characterization of upper and lower limits and the highlight of this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Let $E$ and $s^*$ have the same meaning as in Definition 5. Then $s^*$ has the following two properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$s^* \in E$.&lt;/li&gt;
&lt;li&gt;If $x &amp;gt; s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n &amp;lt; x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Moreover, $s^*$ is the only number with these two properties.&lt;/p&gt;
&lt;p&gt;Of course, an analogous result is true for $s_*$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We start by showing the two properties.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We divide it into three cases depending on what value $s^*$ takes:&lt;/p&gt;
&lt;p&gt;If $s^* = +\infty$, then $E$ is not bounded above; hence $\sn$ is not bounded above, and thus there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k} \to +\infty$. Therefore, $+\infty \in E$ and thus $s^* \in E$.&lt;/p&gt;
&lt;p&gt;If $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists by the definition of $\sup$. Therefore, $s^* \in E$ follows from the Lemmas 1 and 2, and the fact that $s^* = \sup E$.&lt;/p&gt;
&lt;p&gt;If $s^* = -\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Thus, $s^* \in E$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose for the sake of contradiction that there is a number $x &amp;gt; s^*$ such that $s_n \geq x$ for infinitely many values of $n$. Let&amp;rsquo;s denote this set of $n$&#39;s with $\mathcal{K}$ and let $\{s_k\}$$_{k \in \mathcal{K}}$ be this subsequence. If $\{s_k\}_{k \in \mathcal{K}}$ is unbounded then $s^* = +\infty$ contradicting the fact that there exists an $x &amp;gt; s^*$. And if $\{s_k\}_{k \in \mathcal{K}}$ is bounded that it contains a convergent subsequence (Bolzano–Weierstrass theorem). Suppose this convergent subsequence converges to $y$. Then $y \geq x &amp;gt; s^*$. This contradicts the definition of $s^*$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To show the uniqueness, suppose there are two distinct numbers, $p$ and $q$, which satisfy the two properties, and suppose $p &amp;lt; q$. Choose $x$ such that $p &amp;lt; x &amp;lt; q$. Since $p$ satisfies the second property, we have $s_n &amp;lt; x$ for $n \geq N$. But then $q$ cannot satisfy the second property. $\square$&lt;/p&gt;
&lt;p&gt;An intuitive theorem:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2:&lt;/strong&gt; If $s_n \leq t_n$ for $n \geq N$, where $N \in \N$ is fixed, then
$$\limi s_n \leq \limi t_n,$$
$$\lims s_n \leq \lims t_n$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let $s^* = \lims s_n$ and $t^* = \lims t_n$. Suppose for the sake of contradiction $t^* &amp;lt; s^*$. Choose $x$ such that $t^* &amp;lt; x &amp;lt; s^*$. Then by the second property of Theorem 1 there is an integer $N_1$ such that $n \geq N_1$ implies $t_n &amp;lt; x$. Also by the first property there exists a subsequence $\snk$ such that $s_{n_k} \to s^*$. This implies that there exists an integer $N_2$ such that $n \geq N_2$ implies $x &amp;lt; s_n$. But then for $n \geq \max\{N_1, N_2\}$ we have $t_n &amp;lt; x &amp;lt; s_n$.
This gives us our required contradiction.&lt;/p&gt;
&lt;p&gt;A similar argument can be made for the $\liminf$ case. $\square$&lt;/p&gt;
&lt;p&gt;Next we give a necessary and sufficient condition for the convergence of a sequence in terms of its $\liminf$ and $\limsup$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3:&lt;/strong&gt; For a real-valued sequence $\sn$, $\limm s_n = s \in \RR$ if and only if
$$\lims s_n = \limi s_n = s$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;
We divide the analysis into three cases.&lt;/p&gt;
&lt;p&gt;First, let $s \in \R$. Then if $\lims s_n = \limi s_n = s$, Theorem 1 implies that for any $\e &amp;gt; 0$ we have $s_n \in (s-\e, s+\e)$ for all but finitely many $n$, which means $s_n \to s$. On the other hand if $s_n \to s$ then every subsequence $\snk$ must converge to $s$ and hence $\lims s_n = \limi s_n = s$.&lt;/p&gt;
&lt;p&gt;Now let $s = +\infty$. Then $s_n \to s$, i.e., for every $M \in \R$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$ if and only $\limi s_n = +\infty$, and then $\lims s_n = +\infty$ since $\limi s_n \leq \lims s_n$.&lt;/p&gt;
&lt;p&gt;Lastly, let $s = -\infty$. Then $s_n \to s$, i.e., for every $M \in \R$ there is an integer $N$ such that $n \geq N$ implies $s_n \leq M$ if and only $\lims s_n = -\infty$, and then $\limi s_n = -\infty$ since $\limi s_n \leq \lims s_n$. $\square$&lt;/p&gt;
&lt;h2 id=&#34;upper-and-lower-limits---a-reprise&#34;&gt;Upper and lower limits - a reprise&lt;/h2&gt;
&lt;p&gt;There is an equivalent way to express upper and lower limits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 6:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. We define the notation
$$\sups := \sup \{ s_k : k \geq n\}$$
$$\infs := \inf \{ s_k : k \geq n\}$$&lt;/p&gt;
&lt;p&gt;We note that the sequence $\{ \sups \}$$_{n \in \N}$ is monotonically decreasing and the sequence $\{ \infs \}$$_{n \in \N}$ is monotonically increasing, and thus their limits exist in $\RR$.&lt;/p&gt;
&lt;p&gt;We now show the equivalence of the two ways of looking at upper and lower limits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 4:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Then
$$\limm \sups = \lims s_n$$
$$\limm \infs = \limi s_n$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;
We will prove the first equation. The proof for the second is similar. We prove the equation in two steps.&lt;/p&gt;
&lt;p&gt;Let $S = \{ s_n : n \in \N \}$. Let&amp;rsquo;s show that $\sup S = +\infty$ if and only if $\lims s_n = + \infty$. Suppose first that $\sup S = + \infty$. Then we construct a subsequence $\snk$ as follows. We let $n_1 = 1.$ Suppose $n_1, \ldots, n_{k}$ are chosen and let
$$S_k = \{ n \in \N : s_n \geq \max\{s_{n_1}, \ldots, s_{n_k}, k\} + 1 \}$$
Notice that $S_k$ is infinite as otherwise we can find an $M \in \R$ such that $s_n \leq M$ for all $n \geq 1$, contradicting the fact that $\sup S = + \infty$. We pick $n_{k+1}$ to be the smallest element of $S_k$ which is bigger than $n_k$. The resulting subsequence satisfies the condition that $s_{n_k} \geq k$ for $k \geq 2$ and thus we conclude that $s_{n_k} \to +\infty$ which gives $\lims s_n = +\infty$. Now suppose that $\lims s_n = +\infty$. From Theorem 1 we can conclude that there exists a subsequence $\snk$ such that $s_{n_k} \to +\infty$. This immediately implies $\sup S = +\infty$.&lt;/p&gt;
&lt;p&gt;For the second step, suppose $\sup S &amp;lt; + \infty$. Let
$$a_n = \sups$$
Notice that $a_1 &amp;lt; +\infty$ and $\{a_n\}$ is a monotonically decreasing sequence. Therefore, we have that either $\{a_n\}$ is lower bounded in which case it converges to, say, $a$ or it is not in which case $\limm a_n = -\infty$. Since $s_n \leq a_n$, by Theorem 2 we can conclude
$$
\lims s_n \leq \lims a_n = \limm a_n
$$
where the last equality follows from Theorem 3. We will now show that
$$
\lims s_n \geq \limm a_n
$$
which will give us our required equality. If $\limm a_n = -\infty$ there is nothing to prove and so we assume that $a &amp;gt; -\infty$. Let $\e &amp;gt; 0$ be given and let
$$
B = \{ n \in \N : s_n  \geq a - \e \}
$$
We claim that $B$ is infinite. Indeed, if $B$ were finite we can find $N \in \N$ so that $N \geq \max(B)$. This will imply that $s_n \leq a - \e$ for all $n \geq N$ and so $a_n \leq a - \e$ for $n \geq N$. But then by Theorem 2 we would conclude $a = \limm a_n \leq a-\e$, which is absurd. Thus $B$ is infinite and we let $\snk$ be a subsequence of $\sn$ with $n_k \in B$. Notice that $\lims s_{n_k} \leq \lims s_n$, since any subsequential limit of $\snk$ is also a subsequential limit of $\sn$. This along with Theorem 2 give
$$
a - \e \leq \lims s_{n_k} \leq \lims s_n
$$
Since $\e$ was arbitrary we conclude that  $\lims s_n \geq a$, which is what we wanted. $\square$&lt;/p&gt;
&lt;h2 id=&#34;more-properties&#34;&gt;More properties&lt;/h2&gt;
&lt;p&gt;These theorems open new avenues to discover more properties of upper and lower limits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 5:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Then
$$\limi s_n = - \lims (-s_n)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; TODO&lt;/p&gt;
&lt;p&gt;Subadditivity of $\limsup$:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 6:&lt;/strong&gt; For any two real sequences $\{a_n\}$ and $\{b_n\}$,
$$
\lims (a_n + b_n) \leq \lims a_n + \lims b_n
$$
provided the sum on the right is not of the form $\infty - \infty$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; If $\lims a_n = \infty$ then as by assumption the right side is not $\infty - \infty$, it is $\infty$ and there is nothing to prove. Similarly for the case $\lims b_n = \infty$. We may thus assume that
$$
\lims a_n = A &amp;lt; \infty \text{ and } \lims b_n = B &amp;lt; \infty
$$
We note that $\sup_{k \geq 1} a_k &amp;lt; \infty$, $\sup_{k \geq 1} b_k &amp;lt; \infty$, and so TODO&lt;/p&gt;
&lt;p&gt;Superadditivity of $\liminf$:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 7:&lt;/strong&gt; For any two real sequences $\{a_n\}$ and $\{b_n\}$,
$$
\limi (a_n + b_n) \geq \limi a_n + \limi b_n
$$
provided the sum on the right is not of the form $\infty - \infty$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; TODO&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hilbert&#39;s Hotel</title>
      <link>https://makkar.github.io/hilbert/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://makkar.github.io/hilbert/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://makkar.github.io/research/bayesian-ensemble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://makkar.github.io/research/bayesian-ensemble/</guid>
      <description>&lt;p&gt;Title:  Bayesian nonparametric ensemble
Date: June 17, 2020
Tags:  bayesian
Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;$$
\mathbb{E}[X] = \int_{\Omega} X ;\text{dP}
$$&lt;/p&gt;
&lt;h1 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h1&gt;
&lt;h2 id=&#34;bne&#34;&gt;BNE&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
