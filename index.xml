<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aditya Makkar</title>
    <link>https://makkar.github.io/</link>
      <atom:link href="https://makkar.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Aditya Makkar</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 15 May 2021 20:34:25 -0400</lastBuildDate>
    <image>
      <url>https://makkar.github.io/images/icon_hu6726f8512450ca078cf8b558674bfa85_44401_512x512_fill_lanczos_center_2.png</url>
      <title>Aditya Makkar</title>
      <link>https://makkar.github.io/</link>
    </image>
    
    <item>
      <title>General Theory of Processes - Part 2</title>
      <link>https://makkar.github.io/post/grlthprcs2/</link>
      <pubDate>Sat, 15 May 2021 20:34:25 -0400</pubDate>
      <guid>https://makkar.github.io/post/grlthprcs2/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\cJ}{\mathcal{J}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\LO}{\mathbb{L}^0}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\Prob}[1]{\bP\left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\K}{\mathscr{K}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\PO}{\mathfrak{P}}
\newcommand{\fS}{\mathfrak{S}}
\newcommand{\fSP}{\fS^{(\sP)}}
\newcommand{\fA}{\mathfrak{A}}
\newcommand{\f}{\mathfrak{f}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 \; : \; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 \; | \; #2 \right\}}
\newcommand{\Ex}[1]{\bE\left[#1\right]}
\newcommand{\Excc}[2]{\bE\left( \left. #1 \, \right\vert \, #2\right)}
\newcommand{\Exc}[2]{\bE\left[ \left. #1 \, \right\vert \, #2\right]}
\newcommand{\Pc}[2]{\bP\left( \left. #1 \, \right\vert \, #2\right)}
\newcommand{\Prob}[1]{\bP\left(#1\right)}
\newcommand{\Probb}[1]{\bP\left[#1\right]}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\PM}{\widehat{\otimes}_p}
\newcommand{\nn}{{n \in \N}}
\newcommand{\oi}{[0, \infty)}
\newcommand{\oio}{{[0, \infty) \times \Omega}}
\newcommand{\gr}[1]{[\![ #1 ]\!]}
\newcommand{\grr}[1]{]\!] #1 ]\!]}
\newcommand{\grl}[1]{[\![ #1 [\![}
\newcommand{\grrl}[1]{]\!] #1 [\![}
\newcommand{\oX}{\sideset{^o}{}X}
\newcommand{\pX}{\sideset{^p}{}X}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator{\dQ}{d\QQ}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\bE}{\mathbb{E}}
\DeclareMathOperator{\bP}{\mathbb{P}}
\DeclareMathOperator*{\esssup}{ess\,sup}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;This is part 2 of the sequence of blog posts on general theory of processes. For part 1 
&lt;a href=&#34;https://makkar.github.io/post/grlthprcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see here&lt;/a&gt;. I will be building upon the content presented there.&lt;/p&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Stochastic Processes&lt;/li&gt;
&lt;li&gt;Stopping Times
&lt;ul&gt;
&lt;li&gt;Predictable Stopping Times&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Debut of a Progressive Set&lt;/li&gt;
&lt;li&gt;Optional and Predictable Processes&lt;/li&gt;
&lt;li&gt;Properties of Debuts&lt;/li&gt;
&lt;li&gt;Section Theorems&lt;/li&gt;
&lt;li&gt;Applications of Section Theorems&lt;/li&gt;
&lt;li&gt;Projection Theorems&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; &amp;ldquo;Mathematics exists solely for the honour of the human mind.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ndash; Jacobi in a letter to Legendre after the death of Fourier. Fourier had the opinion that the principal aim of mathematics was public utility and explanation of natural phenomena. &lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We previously discussed Choquet&amp;rsquo;s theory of capacities and its applications in measure theory. The goal of this blog post is to discuss the debut, section and projection theorems in stochastic processes. These theorems form the core of the &amp;ldquo;general theory of processes&amp;rdquo;. At the risk of being overly simplistic, general theory of processes is the study of filtrations and stopping times.&lt;/p&gt;
&lt;p&gt;Unlike part 1 where there were no prerequisites other than basic measure theory, this part assumes a good amount of familiarity with stochastic processes, at the level of [1,2]. Without that the material here will feel unmotivated and difficult. On the other hand, the theorems proved here are skipped even in advanced courses in stochastic processes as they aren&amp;rsquo;t the most useful for applications, but rather are necessary to fill in gaps if a rigorous treatment is warranted. Nevertheless the theory presented is extremely profound and beautiful, and reading it will be an honour for the mind, if nothing else.&lt;/p&gt;
&lt;h1 id=&#34;stochastic-processes&#34;&gt;Stochastic Processes&lt;/h1&gt;
&lt;p&gt;For this whole blog post, we shall place ourselves on a complete probability space $\probsp$ endowed with a filtration $\bF = \se{\F(t)} _ {0 \le t &amp;lt; \infty}$, i.e., a family of sub$-\sigma-$algebras of $\F$ which is increasing in the sense that
$$\F(s) \subseteq \F(t) \subseteq \F(\infty) := \sigma\left(\bigcup_{u \in \oi} \F(u)\right), \quad 0 \le s \le t &amp;lt; \infty.$$&lt;/p&gt;
&lt;p&gt;We sometimes denote this setup conveniently as $(\Omega, \F, \bF, \bP)$ and call it a &lt;em&gt;filtered probability space&lt;/em&gt;. We shall assume that this filtration satisfies the &lt;em&gt;usual conditions&lt;/em&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Right-continuity, i.e., $\F(t) = \F(t+) := \bigcap_{s &amp;gt; t} \F(s)$ for all $t \in [0, \infty)$, and&lt;/li&gt;
&lt;li&gt;$\F(0)$ contains all the $\bP-$negligible events in $\F$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For our purposes, a stochastic process is simply a collection of real-valued random variables $X = \se{X(t)}_{0 \le t &amp;lt; \infty}$ on $(\Omega, \F)$. The &lt;em&gt;sample paths&lt;/em&gt; of $X$ are the mapping $\oi \ni t \mapsto X(t, \omega) \in \R$ obtained when fixing $\omega \in \Omega$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 18:&lt;/span&gt; Consider two stochastic process $X$ and $Y$ defined on the same probability space $\probsp$. We say&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;$X$ and $Y$ have the &lt;em&gt;same finite-dimensional distributions&lt;/em&gt; if, for any integer $n \ge 1$, real numbers $0 \le t_1 &amp;lt; t_2 &amp;lt; \cdots &amp;lt; t_n &amp;lt; \infty,$ and $A \in \B(\R^n)$, we have
$$\bP[(X_{t_1}, \ldots, X_{t_n}) \in A] = \bP[(Y_{t_1}, \ldots, Y_{t_n}) \in A]$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$Y$ is a &lt;em&gt;modification&lt;/em&gt; or a &lt;em&gt;version&lt;/em&gt; of $X$ if, for every $t \in \oi$, we have $\bP(X_t = Y_t) = 1$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$X$ and $Y$ are &lt;em&gt;indistinguishable&lt;/em&gt; if almost all their sample paths agree, i.e., $\bP(X_t = Y_t, \; \forall \, t \in \oi) = 1$. Technically, this formulation isn&amp;rsquo;t rigorous since the set $\se{X_t = Y_t, \; \forall \, t \in \oi}$ need not be measurable, and so we can define indistinguishability as follows: $X$ and $Y$ are indistinguishable if there exists a negligible subset $\Psi$ of $\Omega$ such that
$$X_t(\omega) = Y_t(\omega), \quad \forall \; \omega \in \Omega \setminus \Psi, \; \forall \; t \in \oi$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 19:&lt;/span&gt; A stochastic process $X = \se{X(t)}_{0 \le t &amp;lt; \infty}$ is called&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;adapted&lt;/em&gt;, if $X(t)$ is $\F(t)-$measurable for every $t \in \oi$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;measurable&lt;/em&gt;, if the mapping
$$[0, \infty) \times \Omega \ni (t,\omega) \mapsto X(t,\omega) \in \R \tag{5} \label{eq_meas}$$
is $\B(\oi) \otimes \F-$measurable, when $\R$ is endowed with its Borel $\sigma-$algebra.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;progressively measurable&lt;/em&gt;, if for every $t \in \oi$ the mapping $$[0,t] \times \Omega \ni (t,\omega) \mapsto X(t,\omega) \in \R$$ is $\B([0,t]) \otimes \F(t)-$measurable, when $\R$ is endowed with its Borel $\sigma-$algebra.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Recall that every progressively measurable process is both measurable and adapted; and every adapted process with right-continuous or with left-continuous paths, is progressively measurable [1]. On the other hand, we have the following&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 11:&lt;/span&gt; Every measurable and adapted process $X = \se{X(t)} _ {0 \le t &amp;lt; \infty}$ has a progressively measurable modification $Y = \se{Y(t)} _ {0 \le t &amp;lt; \infty}$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Consider the space $\LO$ of equivalence classes of measurable functions $f : \Omega \to \R$, and endow it with the topology of convergence in probability, for example with the pseudo-metrics $\rho(f, g) = \bE (1 \wedge |f-g|)$ or $\rho(f,g) = \bE \left( \frac{|f-g|}{1 + |f-g|} \right)$. Recall that if a sequence $\se{f_n} _ \nn \subseteq \LO$ satisfies $\sum _ \nn \rho(f _ n, f _ {n+1}) &amp;lt; \infty,$ then it converges in probability &amp;ldquo;fast&amp;rdquo;, thus also almost surely.&lt;/p&gt;
&lt;p&gt;Every process $Y = \se{Y(t)} _ {0 \le t &amp;lt; \infty}$ can be thought of as a mapping from $\oi$ into $\LO$, which associates to each element in $\oi$ the equivalence class $\mathcal{Y} _ t$ of $Y(t)$. Consider now the collection $\sH$ of processes $Y$ such that the mapping $\oi \ni t \mapsto \Y_t \in \LO$ satisfies&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;it takes values in a separable subspace of $\LO$;&lt;/li&gt;
&lt;li&gt;under it, the inverse image of every open ball of $\LO$ is a Borel subset of $\oi$; and&lt;/li&gt;
&lt;li&gt;it is the uniform limit of a sequence of simple measurable functions with values in the space $\LO$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then note that property 3 implies the other two, and that conversely, properties 1 and 2 together imply property 3, because a real-valued function is measurable if and only if it is the increasing limit of a sequence of simple measurable functions.&lt;/p&gt;
&lt;p&gt;Next, note that $\sH$ is a real vector space on account of property 3, and is closed under sequential pointwise convergence on account of properties 1 and 2. It is easy to see that $\sH$ contains all processes of the form $Y(t,\omega) = K(t) \xi(\omega)$, where $K$ is the indicator of an interval in $\oi$ and $\xi : \Omega \to \R$ is a bounded measurable function. Thus, monotone class theorem implies $\sH$ contains all bounded measurable processes. Using a standard slicing argument it is easily seen that every measurable process $Y$ has properties 1, 2 and 3.&lt;/p&gt;
&lt;p&gt;Consider now a measurable and adapted process $X = \se{X(t)} _ {0 \le t &amp;lt; \infty}$. For each $\nn$, there exists a process $X^{(n)} = \se{X^{(n)}(t)} _ {0 \le t &amp;lt; \infty}$ which is simple and measurable (in the sense that there exists a partition $\se{A^{(n)} _ k} _ {k \in \N}$ of $\oi$ into Borel sets, and a sequence of random variable $\se{H^{(n)} _ k} _ {k \in \N}$, such that $X^{(n)}(t) = H^{(n)} _ k$ for $t \in A^{(n)} _ k$), and satisfies
$$\rho(X(t), X^{(n)}(t)) \le 2^{-(n+1)}, \quad \forall \; t \in \oi$$&lt;/p&gt;
&lt;p&gt;Next, we define a sequence of random variables $\se{G^{(n)} _ k} _ {k \in \N}$ for each $\nn$ using the sequence $\se{H^{(n)} _ k} _ {k \in \N}$ to get &amp;ldquo;enhanced&amp;rdquo; measurability properties. To this end, define $s^{(n)} _ k = \inf A^{(n)} _ k$, and define $G^{(n)} _ k = X(s^{(n)} _ k)$ if $s^{(n)} _ k \in A^{(n)} _ k$; if, on the other hand $s^{(n)} _ k \notin A^{(n)} _ k$, we pick any $\F(s^{(n)} _ k)-$measurable $G^{(n)} _ k$ that satisfies
$$\rho\left(G^{(n)} _ k, H^{(n)} _ k\right) \le 2^{-(n+1)}$$
Such a $G^{(n)} _ k$ always exists since we may take a decreasing sequence $\se{\theta_m} _ {m \in \N} \subseteq A^{(n)} _ k$ with $\theta_m \downarrow s^{(n)} _ k$, and set $G^{(n)} _ k = \liminf _ {m \to \infty} X(\theta_m)$; and the above inequality holds for all integers $k$.&lt;/p&gt;
&lt;p&gt;We define now the process $Y^{(n)} = \se{Y^{(n)}(t)} _ {0 \le t &amp;lt; \infty}$ by $Y^{(n)}(t) := G^{(n)} _ k, \, t \in A^{(n)} _ k$, and check that it is progressively measurable and satisfies $\rho\left(X(t), Y^{(n)}(t)\right) \le 2^{-(n+1)}$ for all $t \in \oi$ and $\nn$.&lt;/p&gt;
&lt;p&gt;Finally, we construct a progressively measurable process $Y$ by
$$Y(t, \omega) := \limn Y^{(n)}(t, \omega), \quad \forall \; (t,\omega) \in \oi \times \Omega \text{ such that this limit exists,}$$
and $Y(t, \omega) := 0$ otherwise. The properties of $\rho$ then implies $\bP(X(t) = Y(t)) = 1$ holds for all $t \in \oi$, so $Y$ is a modification of $X$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 20:&lt;/span&gt; The &lt;em&gt;$\sigma-$algebra of progressively measurable sets&lt;/em&gt;, denoted by $\sP _ \star$, is the smallest $\sigma-$algebra on the product space $\oi \times \Omega$, with respect to which the mapping as in \eqref{eq_meas} are measurable for all progressively measurable processes $X$.&lt;/p&gt;
&lt;p&gt;It is easy to see that a subset $A$ of $\oi \times \Omega$ belongs to $\sP _ \star$ if and only if, for every $t \in \oi$, $A \cap ([0,t] \times \Omega) \in \B([0,t]) \otimes \F(t).$ This important characterization will be used later.&lt;/p&gt;
&lt;h1 id=&#34;stopping-times&#34;&gt;Stopping Times&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 21:&lt;/span&gt; A random variable $T : \Omega \to [0, \infty]$ is called a &lt;em&gt;stopping time&lt;/em&gt; if $\se{T \le t} \in \F(t)$ for all $t \in \oi$.&lt;/p&gt;
&lt;p&gt;Because $\bF$ is right-continuous this is equivalent to $\se{T &amp;lt; t} \in \F(t)$ for all $t \in \oi$ as can be seen by writing $\se{T \le t} = \bigcap_{\substack{q \in \Q \ t &amp;lt; q &amp;lt; s}} \se{T &amp;lt; q} \in \F(s)$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 22:&lt;/span&gt; For a stopping time $T$, we define&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;$\sigma-$algebra of events prior to it&lt;/em&gt; by
$$\F(T) := \set{A \in \F}{ A \cap \se{T \le t} \in \F(t), \quad \forall \; t \in \oi}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;$\sigma-$algebra of events strictly prior to it&lt;/em&gt; by
$$\F(T-) := \sigma(\F(0) \cup \set{A \cap \se{T &amp;gt; t}}{t \in \oi, A \in \F(t)})$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The following properties are useful and their proofs can be found in any standard textbook [1,2].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 6:&lt;/span&gt; Let $T$ and $S$ be any two stopping times. Then&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\F(T-) \subseteq \F(T)$;&lt;/li&gt;
&lt;li&gt;$T$ is measurable with respect to both $\F(T)$ and $\F(T-)$;&lt;/li&gt;
&lt;li&gt;$\F(T) \cap \F(S) = \F(T \wedge S)$;&lt;/li&gt;
&lt;li&gt;$A \cap \se{T \le S} \in \F(S)$, for all $A \in \F(T)$;&lt;/li&gt;
&lt;li&gt;$A \cap \se{T &amp;lt; S} \in \F(S-)$, for all $A \in \F(T)$;&lt;/li&gt;
&lt;li&gt;If the stopping times satisfy $T \le S$, then $\F(T) \subseteq \F(S)$ and $\F(T-) \subseteq \F(S-)$;&lt;/li&gt;
&lt;li&gt;If the stopping times satisfy $T &amp;lt; S$ on the set $\se{0 &amp;lt; S &amp;lt; \infty}$, then $\F(T) \subseteq \F(S-)$.&lt;/li&gt;
&lt;li&gt;If $\se{T_n} _ \nn$ is an increasing sequence of stopping times and $T = \lim _ \nn \uparrow T_n$, then
$$\F(T-) = \sigma\left(\bigcup_\nn \F(T_n-)\right); \quad \text{as well as  } \F(T-) = \sigma\left(\bigcup_\nn \F(T_n)\right)$$
provided that $T_n &amp;lt; T$ holds on the set $\se{0 &amp;lt; T &amp;lt; \infty}$ for every $\nn$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;predictable-stopping-times&#34;&gt;Predictable Stopping Times&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 23:&lt;/span&gt; A stopping time $T$ is called &lt;em&gt;predictable&lt;/em&gt;, if there exists an increasing (&amp;ldquo;announcing&amp;rdquo;) sequence $\se{T_n} _ \nn$ of stopping times with $T = \lim _ \nn \uparrow T_n$, and $T_n &amp;lt; T$ holds on the set $\se{T &amp;gt; 0}$ for all $\nn$.&lt;/p&gt;
&lt;p&gt;Note $\se{T \le t} = \bigcap_\nn \se{T_n \le t} \in \F(t)$ and therefore if in the definition above we had not required $T$ to be a stopping time, it would still be a stopping time. A canonical example of a predictable stopping time is the first time Brownian motion $W$ hits or exceeds a certain level $b$. It is announced by sequence of first times $W$ hits or exceeds the levels $b - 1/n$, for $\nn$. In fact, we have (from [3])&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 12:&lt;/span&gt; Let $X$ be a continuous adapted process and $b$ be a real number. Then
$$T = \inf \set{t \in \oi}{X(t) \ge c}$$
is a predictable stopping time.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Let $T_n = \inf \set{t \in \oi}{X(t) \ge b - 1/n}$ which, by the Debut Theorem below is a stopping time. This gives an increasing sequence $\se{T_n} _ \nn$ of stopping times bounded above by $T$. Also, $X(T _ n) \ge c - 1/n$ whenever $T _ n &amp;lt; \infty$ and, by left-continuity, setting $\tau = \lim _ n T _ n$ gives $X(\tau) \ge c$ whenever $\tau &amp;lt; \infty$. So $T \ge \tau$, showing that $T = \lim _ \nn \uparrow T_n$. If $0 &amp;lt; T _ n \le T &amp;lt; \infty$ then, by continuity, $X(T_n) = c - 1/n &amp;lt; c = X(T)$. So, $T_n &amp;lt; T$ on the set $\se{0 &amp;lt; T &amp;lt; \infty}$ and the sequence $\se{T_n \wedge n} _ \nn$ announces $T$. $\square$&lt;/p&gt;
&lt;p&gt;Of course, in the above proof we assumed Debut Theorem for stopping times and proving it is our next agenda. In the same flavor as Lemma-6 we have&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 7:&lt;/span&gt; If $T$ is a predictable and $S$ an arbitrary stopping time, then&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A \cap \se{T \le S} \in \F(S-)$ for all $A \in \F(T-)$;&lt;/li&gt;
&lt;li&gt;$A \cap \se{S = \infty} \in \F(S-)$ for all $A \in \F(\infty)$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In particular, both events $\se{T \le S}$ and $\se{T = S}$ belong to $\F(S-)$.&lt;/p&gt;
&lt;h1 id=&#34;debut-of-a-progressive-set&#34;&gt;Debut of a Progressive Set&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 24:&lt;/span&gt; For any measurable mapping $Z : \Omega \to [0, \infty]$ and any $A \in \F$, we define the &lt;em&gt;restriction of $Z$ on $A$&lt;/em&gt; as
$$Z_A := Z \ind{A} + \infty \ind{A^\comp}$$&lt;/p&gt;
&lt;p&gt;Notice that $\se{Z_A \le Z} = A \sqcup \left(A^\comp \cap \se{Z = \infty}\right)$. If $T$ is a stopping time and $A \in \F(T)$, then since $\se{T_A \le t} = \se{T \le t} \cap A \in \F(t)$ for all $t \in \oi$, $T_A$ is also a stopping time.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 13 [Debut of a Progressive Set]:&lt;/span&gt; Under the usual conditions, if $A$ is a progressively measurable set, i.e., $A \in \sP_\star$, then the debut $D_A$ is a stopping time.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Since $A \in \sP_\star \subseteq \B(\oi) \otimes \F$, Measurable Debut theorem (Theorem 9) implies $D_A$ is a random variable.&lt;br&gt;
Just like in the proof of Theorem 9, for any real number $t &amp;gt; 0$, the set $\se{D_A &amp;lt; t}$ is the projection onto $\Omega$ of the set $A_t := A \cap ([0, t) \times \Omega)$. Recall that the fact that $A \in \sP_\star$ implies $A_t \in \B([0,t]) \otimes \F(t)$. Therefore, Measurable Projection theorem (Theorem 7) implies $\se{D_A &amp;lt; t} = \pi(A_t) \in \F(t)$. Right continuity of the filtration now implies $D_A$ is an $\bF-$stopping time. $\square$&lt;/p&gt;
&lt;p&gt;The following important theorem now becomes an easy corollary.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 14 [First Hitting Times]:&lt;/span&gt; In the context above, if $X$ is a progressively measurable process and $\Gamma \in \B(\R)$, the first hitting time
$$H_\Gamma := \inf \set{t \in \oi}{X(t) \in \Gamma}$$
is a stopping time.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Since $X$ is a progressively measurable process, the set $A = \set{(t,\omega) \in \oi \times \Omega}{X(t, \omega) \in \Gamma}$ is a progressively measurable set. Theorem 13 then implies the debut $D_A$ is a stopping time, but it is easy to see $D_A = H_\Gamma$. $\square$&lt;/p&gt;
&lt;h1 id=&#34;optional-and-predictable-processes&#34;&gt;Optional and Predictable Processes&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 25:&lt;/span&gt; The &lt;em&gt;predictable $\sigma-$algebra&lt;/em&gt;, denoted by $\sP$, is the smallest $\sigma-$algebra on $\oi \times \Omega$ with respect to which left-continuous $\bF-$adapted processes are measurable. A stochastic process is said to &lt;em&gt;predictable}&lt;/em&gt; if it is $\sP-$measurable.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 26:&lt;/span&gt; The &lt;em&gt;optional $\sigma-$algebra&lt;/em&gt;, denoted by $\sO$, is the smallest $\sigma-$algebra on $\oi \times \Omega$ with respect to which right-continuous $\bF-$adapted processes are measurable. A stochastic process is said to &lt;em&gt;optional&lt;/em&gt; if it is $\sO-$measurable.&lt;/p&gt;
&lt;p&gt;Since all right-continuous or left-continuous adapted processes are progressively measurable, all optional or predictable processes are adapted. We need the concept of stochastic intervals to ease some notation and see some examples of optional and predictable sets.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 26:&lt;/span&gt; If $U,V : \Omega \to [0, \infty]$ are two random variables, and $U \le V$, then define the &lt;em&gt;stochastic intervals&lt;/em&gt; as
$$\gr{U,V} := \set{(t,\omega) \in \oi \times \Omega}{U(\omega) \le t \le V(\omega)}$$
$$\grl{U,V} := \set{(t,\omega) \in \oi \times \Omega}{U(\omega) \le t &amp;lt; V(\omega)}$$
$$\grr{U,V} := \set{(t,\omega) \in \oi \times \Omega}{U(\omega) &amp;lt; t \le V(\omega)}$$
$$\grrl{U,V} := \set{(t,\omega) \in \oi \times \Omega}{U(\omega) &amp;lt; t &amp;lt; V(\omega)}$$
Also, for $A \in \F(0)$, define
$$0_A(\omega) = \begin{cases}
0 &amp;amp;\text{ if } \omega \in A \\&lt;br&gt;
\infty &amp;amp;\text{ if } \omega \in A^\comp
\end{cases}$$&lt;/p&gt;
&lt;p&gt;Notice that $\gr{U} = \gr{U,U} = \gr{0, U} \setminus \grl{0, U}$, and that $0_A$ is predictable with the announcing sequence $\se{0_A \wedge n} _ \nn$. Denote by $\fS$ the collection of all stopping times and by $\fSP$ the collection of all predictable stopping times. The next lemma collects some useful properties of stochastic intervals in relation to optional and predictable $\sigma-$algebras.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 8:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If $S,T \in \fS$ such that $S \le T$, then the stochastic intervals $\gr{S,T}, \grl{S,T}, \grr{S,T}, \grrl{S,T}$ and the graphs $\gr{S}, \gr{T}$ are optional.&lt;/li&gt;
&lt;li&gt;If $S,T \in \fS$ such that $S \le T$, then the stochastic interval $\grr{S,T}$ is predictable. If $S \in \fSP$, then the stochastic interval $\grl{S, \infty}$ is predictable.&lt;/li&gt;
&lt;li&gt;The $\sigma-$algebra $\sO$ of optional sets is generated by stochastic intervals in many ways
$$
\sO = \sigma\left(\set{\grl{S,\infty}}{S \in \fS}\right) = \sigma\left(\set{\grl{S,T}}{S, T \in \fS}\right) = \sigma\left(\set{\gr{S,T}}{S,T \in \fS}\right)
$$&lt;/li&gt;
&lt;li&gt;Similarly, $\sigma-$algebra $\sP$ of predictable sets is generated by stochastic intervals in many ways
$$
\sP = \sigma\left(\set{\grr{S,T}, \gr{0_A}}{S,T \in \fS \text{ and } A \in \F(0)}\right) = \sigma\left(\set{\grl{S,\infty}}{S \in \fSP}\right) = \sigma\left(\set{\gr{S,T}}{S,T \in \fSP}\right)
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; I will only be proving some results. For a complete picture see [3,4].&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is easy to see that $\ind{\grl{S,T}}$ is right-continuous and adapted, hence $\grl{S,T}$ is optional. If $T_n = S + 1/n$ for $\nn$, then $\gr{S} = \bigcap _ \nn \grl{S, T_n}$, and thus $\gr{S}$ is also optional. Similar, for $\gr{T}$. These facts along with simple manipulation of sets immediately imply $\gr{S,T}, \grr{S,T}, \grrl{S,T}$ are also optional.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is easy to see that $\ind{\grr{S,T}}$ is left-continuous and adapted, hence $\grr{S,T}$ is predictable. $\square$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is an easy exercise to show&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 9:&lt;/span&gt; Every predictable process is optional, and every optional process is progressively measurable. In other words
$$ \sP \subseteq \sO \subseteq \sP _ \star \subseteq \B(\oi) \otimes \F $$&lt;/p&gt;
&lt;h1 id=&#34;properties-of-debuts&#34;&gt;Properties of Debuts&lt;/h1&gt;
&lt;p&gt;Recall the setting of section &amp;ldquo;Measurable Section&amp;rdquo; from 
&lt;a href=&#34;https://makkar.github.io/post/grlthprcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 1&lt;/a&gt;. We showed there that if $A \in \B([0, \infty)) \otimes \F$, then we can define a &lt;em&gt;measurable&lt;/em&gt; mapping $T : \Omega \to [0, \infty]$ such that for all $\omega \in \pi(A)$ we have $(T(\omega), \omega) \in A$, or stated more succinctly, $\gr{T} \subseteq A$. Now, what if we want something stronger, say, $T$ should be a stopping time? As we will see, we will have to make stronger assumptions on the set $A$ and even then if we want $T$ to be a stopping time we will have to let go of the nice property that $\se{T &amp;lt; \infty}= \pi(A)$.&lt;/p&gt;
&lt;p&gt;But first, we need to prove a certain property of debuts which we will need in proving the aforementioned section theorem, and that&amp;rsquo;s the goal of this section.&lt;/p&gt;
&lt;p&gt;It isn&amp;rsquo;t difficult to see that $\sP$ coincides with the mosaic generated by the paving on $\oi \times \Omega$ that consists of finite unions of stochastic intervals of the form $\gr{S,T}$, where $S,T \in \fSP$. Generalizing this, let us consider a collection $\fA$ of stopping times that contains $0$ and $\infty$, and which is closed under a.s. equality and under finitely many lattice operations ($\wedge$ and $\vee$). We denote by $\cJ := \set{\grl{S,T}}{S,T \in \fA}$, and by $\cT$ the collection of finite unions of elements of $\cJ$.&lt;/p&gt;
&lt;p&gt;Simple observations like $\grl{S,T}^\comp = \grl{0, S} \cup \grl{T, \infty}$ or $\grl{S,T} \,\cap \, \grl{U,V} = \grl{S \vee U, (S \vee U) \vee (T \wedge V)}$, can be used to show that $\cJ$ is a paving on $\oi \times \Omega$ and $\cT$ is an algebra on $\oi \times \Omega$.&lt;/p&gt;
&lt;p&gt;For more structure, let us impose the following conditions on the collection $\fA$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For any pair $S,T \in \fA$, the stopping time $S _ {\se{S &amp;lt; T}}$ (recall the notation of Definition 24 and the observation right after it) belongs to $\fA$;&lt;/li&gt;
&lt;li&gt;For any increasing sequence $\se{S_n} _ \nn \subseteq \fA$, the limit $\lim _ \nn \uparrow S_n$ belongs to $\fA$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Condition 1 ensures that the debut of an element of $\cT$ belongs to $\fA$. This is because $S_{\se{S &amp;lt; T}} = D_{\grl{S,T}}$ and because the debut of union of a finite collection of elements from $\cJ$ is equal to to the minimum of the debuts of those elements.&lt;/p&gt;
&lt;p&gt;Condition 2 ensures that the debut of an element of $\cT _ \delta$ (which recall equals $\set{\bigcup _ \nn A_n}{A_n \in \cT \text{ for all } \nn}$) belongs to to $\fA$. This is the main result of this section, and is proved below as Theorem 15.&lt;/p&gt;
&lt;p&gt;Coming out of the abstraction for a bit, we note that the collection $\fS$ of all stopping times satisfies all the conditions imposed above on $\fA$. Similarly for the collection $\fSP$ of all predictable stopping times. The first claim is trivial to see. To show the second claim, we first prove the following lemma.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 10:&lt;/span&gt; For any stopping time $T$ and set $A \in \F(T)$, the random variable $T_A$ is a stopping time as we saw above. For $T_A$ to be a predictable stopping time, it is necessary that $A \in \F(T-)$; this condition is also sufficient if $T$ is predictable.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; If $T_A$ is a predictable stopping time, then since $A = \se{T_A \le T} \setminus \left(A^\comp \cap \se{T = \infty}\right)$ and by Lemma 7 both $\se{T_A \le T}, \left(A^\comp \cap \se{T = \infty}\right) \in \F(T-)$, we have $A \in \F(T-)$.&lt;br&gt;
Conversely, suppose $T$ is a predictable stopping time. Consider the collection $\mathscr{A} := \set{A \in \F(T-)}{T_A \text{ and } T _ {A^\comp} \text{ are both predictable}}$. It is a $\sigma-$algebra: $\Omega \in \mathscr{A}$ clearly and it is closed under complements by definition; if $A_1, A_2, \ldots \in \mathscr{A}$ with $A = \bigcup _ \nn A_n$, note $T_A = \inf _ \nn T _ {A _ n}$. Let $\se{T _ n ^ m} _ {m \in \N}$ be the announcing sequence for $T _ {A _ n}$ for each $\nn$, and define $\tau _ n := \min \set{T _ j ^ k}{j,k \le n}$. Then $\se{\tau _ n} _ \nn$ announces $T_A$ and hence $A \in \mathscr{A}$.&lt;br&gt;
Thus, it suffices to show that $T_A$ is predictable for any $A$ in a collection of sets that generates $\F(T-)$. To this end, suppose $\se{T^n} _ \nn$ announces $T$, and fix an arbitrary $n \in \N$ and an arbitrary $A \in \F(T^n)$. For every integer $m \ge n$, the restriction $T _ A ^ m$ is a stopping time and so is $R _ m := T ^ m _ A \wedge m$. The sequence $\se{R_m} _ {m \in \N}$ then announces $T_A$, showing $T_A$ is predictable; similarly for $T _ {A^\comp}$. We conclude $T_A, T _ {A^\comp}$ are predictable for every $A \in \bigcup _ \nn \F(T^n)$. But recall $\F(T-) = \sigma\left(\bigcup _ \nn \F(T^n)\right)$ since $T^n &amp;lt; T$ on $\se{T &amp;gt; 0}$, so we obtain the property for all $A \in \F(T-)$.  $\square$&lt;/p&gt;
&lt;p&gt;Coming back to the second claim above, if $S,T \in \fSP$, then by Lemma 7, $\se{S &amp;lt; T} \in \F(S-)$, and thus by Lemma 10, $S _ {\se{S &amp;lt; T}} \in \fSP$. The rest of the conditions are trivial to check.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 15:&lt;/span&gt; Suppose $\fA$ is a collection of stopping times with the properties postulated above, and let $B \in \cT_\delta$. Then the debut $D_B$ of this set is a stopping time in $\fA$, and its graph $\gr{D_B} \subseteq B$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; We first show that $\gr{D_B} \subseteq B$. If $(s, \omega) \in \gr{D_B}$, then $D_B(\omega) = s$ which is same as saying $s = \inf \set{t \in \oi}{(t,\omega) \in B}$. But we constructed our collection $\cT_\delta$ using stochastic intervals of the form $\grl{S,T}$, and thus $B(\omega) = \set{t \in \oi}{(t,\omega) \in B}$ is closed implying $(s, \omega) \in B$.&lt;br&gt;
To show that $D_B \in \fA$, consider the collection $\mathfrak{B} := \set{S \in \fA}{S \le D_B}$, and note that $\mathfrak{B}$ contains $0$, is closed under pairwise maximization, i.e., $S,T \in \mathfrak{B}$ implies $S \vee T \in \mathfrak{B}$, and is closed under countable increasing limits, i.e., if $\se{S _ n} _ \nn \subseteq \mathfrak{B}$ is an increasing sequence then $\lim _ \nn \uparrow S_n \in \mathfrak{B}$. Then $\mathfrak{B}$ contains a representative $T := \esssup \mathfrak{B}$ of its essential supremum (see [4,5]).&lt;br&gt;
Since $B \in \cT_\delta$ we can find a decreasing sequence $\se{B_n} _ \nn \subseteq \cT$ with $\bigcap _ \nn B_n = B$. Define $T_n := D _ {B_n \cap \grl{T, \infty}}$ for all $\nn$ and note $\gr{T_n} \subseteq B_n$. As $B_n \cap \grl{T, \infty} \, \in \cT$ and thus can be expressed as a finite union $\bigcup _ {k=1} ^ m \grl{U_k, V_k}$ for $U_k, V_k \in \cT$, we have
$$T_n = \min _ {1 \le k \le m} D _ {\grl{U_k, V_k}} = \min _ {1 \le k \le m} (U_k) _ {\se{U_k &amp;lt; V_k}} \in \cT$$
Since $B \subseteq B_n$, we have $T \le T_n \le D_B$, and therefore $T_n \in \mathfrak{B}$. Thus, by definition of essential supremum $T_n = T$ a.s., and as $\gr{T} \subseteq \bigcap _ \nn B_n = B$, we conclude that $T = D_B$. $\square$&lt;/p&gt;
&lt;h1 id=&#34;section-theorems&#34;&gt;Section Theorems&lt;/h1&gt;
&lt;p&gt;We are now ready to come back to proving section theorems. We start by proving a general section theorem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 16 [General Section Theorem]:&lt;/span&gt; In the context of Theorem 15, let $\sH$ be the $\sigma-$algebra generated by the algebra $\cT$. For every set $A \in \sH$ and $\eps &amp;gt; 0$, there exists a stopping time $T_\eps \in \fA$ with
$$\gr{T_\eps} \subseteq A \text{ and } \bP\left[\pi(A)\right] \le \bP(T_\eps &amp;lt; \infty) + \eps.$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Measurable Section Theorem (Theorem 10 in 
&lt;a href=&#34;https://makkar.github.io/post/grlthprcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 1&lt;/a&gt;) implies there exists a random variable $Z : \Omega \to [0, \infty]$ with $\gr{Z} \subseteq A$ and $\pi(A) = \se{Z &amp;lt; \infty}$. We denote by $\nu$ the measure on the measurable space $\left(\oi \times \Omega, \sH\right)$ defined by
$$
\int _ \oio f(t, \omega) \, \nu(\dd t, \dd \omega) = \int _ {\se{Z &amp;lt; \infty}} f(Z(\omega), \omega) \, \bP(\dd \omega)
$$
for every $\sH-$measurable $f : \oio \to \oi$. Notice that $(Z(\omega), \omega) \in A$ for $\omega \in \se{Z &amp;lt; \infty}$. By first taking $f$ to be the indicator $\ind{A}$ for $A$ and then indicator $\ind{\oio}$ for $\oio$ in the equation above, we see that $\nu(A) = \bP(Z &amp;lt; \infty) \equiv \bP\left[\pi(A)\right] = \nu(\oio)$. Thus, the measure $\nu$ is carried by the set $A$. Similarly, taking $f$ to be $\ind{B}$ for $B \in \sH$, we get $$\nu(B) = \int _ {\pi(A)} \ind{B}(Z(\omega), \omega) \, \bP(\dd \omega) \le \bP\left[\pi(A \cap B)\right].$$
Choquetâ€™s capacitability theorem (Theorem 6 in 
&lt;a href=&#34;https://makkar.github.io/post/grlthprcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 1&lt;/a&gt;) applied to the paving $\cT$ and to the capacity $$\nu^*(C) := \inf _ {B \in \sH, C \subseteq B} \nu(B), \quad C \subseteq \oio$$
we obtain the existence of a subset $B_\eps \in \cT_\delta$ of $A$, such that
$$
\bP\left[\pi(A)\right] = \nu(A) \le \nu(B_\eps) + \eps \le \bP\left[\pi(A \cap B_\eps)\right] + \eps \le \bP\left[\pi(B_\eps)\right] + \eps
$$
Now take $T_\eps$ to be the debut $D _ {B_\eps}$ which by Theorem 15 is a stopping time in $\fA$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 17 [Optional and Predictable Sections]:&lt;/span&gt; Let $A$ be an optional (respectively, predictable) subset of the product space $\oio$. For every $\eps &amp;gt; 0$, there exists a stopping time (respectively, a predictable stopping time) $T_\eps$ with
$$\gr{T_\eps} \subseteq A \text{ and } \bP\left[\pi(A)\right] \le \bP(T_\eps &amp;lt; \infty) + \eps \tag{6} \label{eq_apprx}$$
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Both statements follow from the General Section Theorem (Theorem 16) in conjunction with the observation before Lemma 10, by taking $\fA$ to be either $\fS$ or $\fSP$. $\square$&lt;/p&gt;
&lt;p&gt;As mentioned in the beginning of the section &amp;ldquo;Properties of Debuts&amp;rdquo;, we didn&amp;rsquo;t get the nice equality
$$\gr{T} \subseteq A \text{ and } \se{T &amp;lt; \infty} = \pi(A) \tag{7} \label{eq_exact}$$
as we got in Measurable Section theorem, but instead obtained an approximation \eqref{eq_apprx}. The condition $\gr{T_\eps} \subseteq A$ makes sure that $\se{T_\eps &amp;lt; \infty} \subseteq \pi(A)$, and the $\bP\left[\pi(A)\right] \le \bP(T_\eps &amp;lt; \infty) + \eps$ part ensures that the measure of the difference $\bP\left[ \pi(A) \setminus \se{T_\eps &amp;lt; \infty} \right]$ of these two events can be made as small as desired.&lt;/p&gt;
&lt;p&gt;To see that it is not always possible to choose a stopping time $T$ that satisfies \eqref{eq_exact} if $A \in \sO$, we will construct a filtration $\se{\F_t}_{t \ge 0}$ and choose a set $A \in \sO$ that forces $\pi(A) \setminus \se{T &amp;lt; \infty} \neq \emp$ for every stopping time $T$ (from [3]).&lt;/p&gt;
&lt;p&gt;To this end, let $\tau : \Omega \to (0, \infty)$ be a random variable such that $\Prob{\tau &amp;lt; t} &amp;gt; 0$ for all $t &amp;gt; 0$. For example, let $\tau$ be such that its distribution is uniform on $(0,1)$. Let $\se{\F_t}_{t \ge 0}$ be the completed filtration such that $\F_t$ is generated by $\set{\se{\tau \le s}}{s \le t}$, and thus $\tau$ becomes a stopping time. Let $A = \grrl{0, \tau} \in \sO$. Note that $\Prob{\pi(A)} = 1$ by our construction.&lt;/p&gt;
&lt;p&gt;It is easy to see that $\F_t$ is trivial when restricted to $\se{\tau &amp;gt; t}$. So, every $\F_t-$measurable random variable is a.s. constant on the event $\se{\tau &amp;gt; t}$. Therefore, any stopping time $T$ is deterministic on the event $\se{T &amp;lt; \tau}$. So, if $\gr{T} \subseteq A$, we have
$$T = \begin{cases} s &amp;amp;\text{on } \se{\tau &amp;gt; s} \\ \infty &amp;amp;\text{on } \se{\tau \le s} \end{cases}$$
a.s. for some fixed $s &amp;gt; 0$. But then $\Prob{T &amp;lt; \infty} = \Prob{\tau &amp;gt; s} &amp;lt; 1 = \Prob{\pi(A)}$.&lt;/p&gt;
&lt;p&gt;A similar argument can be used to show that it is not possible to choose a predictable stopping time $T$ that satisfies \eqref{eq_exact} if $A \in \sP$.&lt;/p&gt;
&lt;h1 id=&#34;applications-of-section-theorems&#34;&gt;Applications of Section Theorems&lt;/h1&gt;
&lt;p&gt;Recall the measurable graph theorem (Theorem 8 in 
&lt;a href=&#34;https://makkar.github.io/post/grlthprcs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;part 1&lt;/a&gt;) which implies a map $T : \Omega \to [0, \infty]$ is measurable if and only if its graph $\gr{T}$ is measurable. We also have the following neat result:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 18:&lt;/span&gt; A random variable $T : \Omega \to [0, \infty]$ is a stopping time (respectively, a predictable stopping time), if and only if its graph $\gr{T}$ is an optional (respectively, predictable) set.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; The necessity follows from the characterization of $\sO$ and $\sP$ in Lemma 8. In the optional case, sufficiency follows from Theorem 13 and the fact that $\sO \subseteq \sP_\star$. For the sufficiency in the predictable case, suppose $\gr{T}$ is predictable. Apply Predictable Section theorem (Theorem 17) for $A = \gr{T}$ to construct a sequence $\se{T_n} _ \nn$ of predictable stopping times such that
$$\gr{T_n} \subseteq \gr{T} \text{ and } \bP(T &amp;lt; \infty) \le \bP(T_n &amp;lt; \infty) + 2^{-n}, \quad \forall \; \nn$$
Replacing $T_n$ be $T_1 \vee \cdots \vee T_n$ if necessary, we may assume that this sequence is increasing. It follows then, that the limit $T = \lim _ \nn \uparrow T_n = \sup _ \nn T_n$ is predictable (if $\se{T_n^m} _ {m \in \N}$ announces $T_n$, then $\tau_n = \max \set{T_j^k}{j,k \le n}$ announces $T$). $\square$&lt;/p&gt;
&lt;p&gt;Suppose $X$ and $Y$ are measurable processes such that $\Probb{X(T) \ind{\se{T &amp;lt; \infty}} = Y(T) \ind{\se{T &amp;lt; \infty}}} = 1$ for each $\F-$measurable $T : \Omega \to [0, \infty]$. Then what can we say what $X$ and $Y$? We will show that $X$ and $Y$ are indistinguishable! To this end, let $F = \set{(t,\Omega) \in \oio}{X(t,\omega) \neq Y(t, \omega)}$. It is measurable since $X$ and $Y$ are measurable. We want to show that $\Probb{\pi(F)} = 0$, so suppose to the contrary that $\Probb{\pi(F)} &amp;gt; 0$. Any $\F-$measurable $T : \Omega \to [0, \infty]$ satisfying $\gr{T} \subseteq F$ must also satisfy $X(T) \ind{\se{T &amp;lt; \infty}} \neq Y(T) \ind{\se{T &amp;lt; \infty}}$. Now Measurable Section theorem (Theorem 10) says there exists a random variable $T : \Omega \to [0,\infty]$ such that $\gr{T} \subseteq F$ and $\se{T &amp;lt; \infty} = \pi(F)$, and therefore $\Probb{X(T) \ind{\se{T &amp;lt; \infty}} \neq Y(T) \ind{\se{T &amp;lt; \infty}}} \ge \Prob{T &amp;lt; \infty} = \Probb{\pi(F)} &amp;gt; 0$ contradicting our initial assumption.&lt;/p&gt;
&lt;p&gt;Remarkably, we have similar results for optional and predictable processes.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 19:&lt;/span&gt; Suppose two optional (respectively, predictable) processes $X$ and $Y$ agree at finite stopping times (respectively, predictable stopping times), i.e., we have
$$\Probb{X(T) \ind{\se{T &amp;lt; \infty}} = Y(T) \ind{\se{T &amp;lt; \infty}}} = 1, \quad \forall \; T \in \fS \; \left(\text{respectively, } T \in \fSP\right)$$
Then the two processes $X$ and $Y$ are indistinguishable.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Consider the optional case &amp;ndash; the predictable case is similar. Just like above we have to show that for optional set $F = \set{(t,\Omega) \in \oio}{X(t,\omega) \neq Y(t, \omega)}$, $\Probb{\pi(F)} = 0$. Suppose to the contrary that $\Probb{\pi(F)} = 2\eps &amp;gt; 0$ for some $0 &amp;lt; \eps \le 1/2$. Then Optional Section theorem (Theorem 17) implies there exists a stopping time $T_\eps$ with the properties $\gr{T_\eps} \subseteq F$ and $2\eps = \Probb{\pi(F)} \le \Prob{T_\eps &amp;lt; \infty} + \eps$. But then $\Probb{X(T) \ind{\se{T &amp;lt; \infty}} \neq Y(T) \ind{\se{T &amp;lt; \infty}}} \ge \Prob{T_\eps &amp;lt; \infty} \ge \eps &amp;gt; 0$ contradicting our initial assumption. $\square$&lt;/p&gt;
&lt;p&gt;As we can see section theorems are really powerful and allow simple proofs of otherwise very difficult results. Other than these two applications, section theorems can be used to prove Proposition 1.2.26 in [1] which is left unproved there. Let me skip this proof.&lt;/p&gt;
&lt;h1 id=&#34;projection-theorems&#34;&gt;Projection Theorems&lt;/h1&gt;
&lt;p&gt;To motivate optional and predictable projections, let us start with a fundamental problem in 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Filtering_problem_%28stochastic_processes%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;filtering theory&lt;/a&gt;: Assume an underlying complete probability space $\probsp$. There is an underlying signal $X = \se{X(t)} _ {t \ge 0}$ which is modelled as a stochastic process and which we are interested in studying. Our observation process has noise and therefore instead of observing $X$ we observe a process $Z = \se{Z(t)} _ {t \ge 0}$ such that $Z(t) = X(t) + \eps_t$ for some additive noise $\eps_t$. Therefore, it makes sense to consider $\bF = \se{\F(t)} _ {t \ge 0}$ the completed natural filtration of $Z$. The problem, then, is to compute an estimate for $X$ based on the observable data at time $t$, viz. the filtration $\bF$.&lt;/p&gt;
&lt;p&gt;Looking at the expected value of $X$ conditional on the observable data, we obtain the following estimate for $X$ at each time $t \in \oi$,
$$
Y(t) = \Exc{X(t)}{\F(t)} \tag{8} \label{motiv}
$$
Since, the conditional expectation is defined up to $\bP-$a.s., we have some flexibility in choosing a version of the process $Y = \se{Y(t)} _ {t \ge 0}$. We would be very lucky if it were possible for us to choose a version of $Y$ such that \eqref{motiv} holds not only for all $t \in \oi$ but also for all finite stopping times. And indeed this is possible! This is part of the statement of the optional projection theorem.&lt;/p&gt;
&lt;p&gt;On the other hand, if we want an estimate of $X$ based on observable data before time $t$, then our estimate would be $Y(t) = \Exc{X(t)}{\F(t-)}$ where recall $\F(t-) := \sigma\left(\bigcup _ {s &amp;lt; t} \F(s)\right)$. Again it is possible to choose a version of $Y$ such that this equality holds not only for all $t \in \oi$ but also for all finite predictable stopping times, and this is part of the statement of the predictable projection theorem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 20 [Optional and Predictable Projections]:&lt;/span&gt; Let $X$ be a bounded, measurable (though not necessarily adapted) process.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There is a unique, modulo indistinguishability, optional process $\oX$, called &lt;em&gt;optional projection of $X$&lt;/em&gt;, that satisfies for all stopping times $T \in \fS$ the identity
$$
\Exc{X(T) \ind{\se{T &amp;lt; \infty}}}{\F(T)} = \oX(T) \ind{\se{T &amp;lt; \infty}} \tag{9} \label{o_proj}
$$&lt;/li&gt;
&lt;li&gt;There is a unique, modulo indistinguishability, predictable process $\pX$, called &lt;em&gt;predictable projection of $X$&lt;/em&gt;, that satisfies for all predictable stopping times $T \in \fSP$ the identity
$$
\Exc{X(T) \ind{\se{T &amp;lt; \infty}}}{\F(T-)} = \pX(T) \ind{\se{T &amp;lt; \infty}} \tag{10} \label{p_proj}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Remarks:&lt;/span&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Taking expectations in \eqref{o_proj} we get
$$
\Ex{X(T) \ind{\se{T &amp;lt; \infty}}} = \Ex{\oX(T) \ind{\se{T &amp;lt; \infty}}} \tag{11} \label{o_proj2}
$$
for all stopping times $T \in \fS$. Now suppose \eqref{o_proj2} holds for all stopping times $T \in \fS$. Fix an arbitrary stopping time $S \in \fS$ and a set $A \in \F(S)$, and write \eqref{o_proj2} for $T = S_A$ to get
$$\Ex{X(S) \ind{\se{S &amp;lt; \infty}} \ind{A}} = \Ex{\oX(S) \ind{\se{S &amp;lt; \infty}} \ind{A}}$$
But this immediately implies \eqref{o_proj} for stopping time $S$. Therefore, requiring \eqref{o_proj} to hold for all stopping times is equivalent to requiring \eqref{o_proj2} to hold for for all stopping times.&lt;/li&gt;
&lt;li&gt;Similarly, requiring \eqref{p_proj} to hold for all predictable stopping times is equivalent to requiring
$$\Ex{X(T) \ind{\se{T &amp;lt; \infty}}} = \Ex{\pX(T) \ind{\se{T &amp;lt; \infty}}} \tag{12} \label{p_proj2}$$
to hold for for all predictable stopping times.&lt;/li&gt;
&lt;li&gt;Operators $\sideset{^o}{}\,$ and $\sideset{^p}{}\,$ are linear operators, i.e., for bounded, measurable processes $X,Y$ and $a,b \in \R$
$$\sideset{^o}{}(aX + bY) = a \oX + b \sideset{^o}{}Y$$
$$\sideset{^p}{}(aX + bY) = a \pX + b \sideset{^p}{}Y$$
as can be easily seen by properties of conditional expectation. The formulation of \eqref{o_proj2} and \eqref{p_proj2} makes it obvious that $\sideset{^o}{}(\oX) = \oX$ and $\sideset{^p}{}(\pX) = \pX$. This explains the terminology of &amp;ldquo;projection&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Uniqueness is immediate from Theorem 19, so let&amp;rsquo;s focus on existence.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We will employ the monotone class theorem for functions. We need a simple class of processes for which finding an optional projection is easy. To this end, consider the processes of the form
$$
X(t, \omega) = \ind{B}(\omega) \ind{[u,v)}(t), \quad 0 \le u &amp;lt; v &amp;lt; \infty \text{ and } B \in \F
$$
We claim that the candidate for the optional projection is $\oX(t, \omega) := M(t,\omega) \ind{[u,v)}(t)$, where $M$ is the right-continuous version of the bounded, thus also uniformly integrable, martingale $\Exc{\ind{B}}{\F(t)}$ (see [1,2] for the proof of existence of such a martingale; it is here that the usual conditions on the filtration $\bF$ become crucial).&lt;/p&gt;
&lt;p&gt;With these choices and an arbitrary stopping time $T$, the left-hand side of \eqref{o_proj2} becomes $\bP(B \cap \se{u \le T &amp;lt; v})$, whereas optional stopping theorem [1,2] shows that its right-hand side is $\Ex{\Exc{\ind{B}}{\F(T)} \ind{[u,v)}(T)}$. Recalling now that $T$ is $\F(T)-$measurable shows that the two sides are equal.&lt;/p&gt;
&lt;p&gt;Finally, use linearity and monotone class arguments to establish existence for arbitrary bounded, measurable $X$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Similar ideas work in the predictable case. We first consider processes of the form
$$
X(t, \omega) = \ind{B}(\omega) \ind{(u,v]}(t), \quad 0 \le u &amp;lt; v &amp;lt; \infty \text{ and } B \in \F
$$
We claim that the candidate for the predictable projection is $\pX(t, \omega) = M_{-}(t,\omega) \ind{[u,v)}(t)$, where $M_{-}(t,\omega) := M(t,\omega)$ is the left-continuous version of the martingale $M$ from above. Again using $\F(T-)-$measurability of $T$ we see the two sides of \eqref{p_proj2} are equal. Monotone class arguments now allow us to finish the proof. $\square$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Ioannis Karatzas and Steven Shreve. &lt;em&gt;Brownian Motion and Stochastic Calculus&lt;/em&gt;, Graduate Texts in Mathematics Volume 113, Springer-Verlag New York, 1998.&lt;/li&gt;
&lt;li&gt;Jean-FranÃ§ois Le Gall. &lt;em&gt;Brownian Motion, Martingales, and Stochastic Calculus&lt;/em&gt;, Graduate Texts in Mathematics Volume 274, Springer International Publishing, 2016.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://almostsuremath.com/stochastic-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Almost Sure&lt;/a&gt; blog by George Lowther.&lt;/li&gt;
&lt;li&gt;Sheng-wu He, Jia-gang Wang and Jia-an Yan. &lt;em&gt;Semimartingale Theory and Stochastic Calculus&lt;/em&gt;, CRC Press, 1992.&lt;/li&gt;
&lt;li&gt;Ioannis Karatzas and Steven Shreve. &lt;em&gt;Methods of Mathematical Finance&lt;/em&gt;, Probability Theory and Stochastic Modelling Volume 39, Springer-Verlag New York, 1998.&lt;/li&gt;
&lt;li&gt;Claude Dellacherie. &lt;em&gt;CapacitÃ©s et processus stochastiques&lt;/em&gt;, Springer-Verlag, 1972.&lt;/li&gt;
&lt;li&gt;Claude Dellacherie and Paul-AndrÃ© Meyer. &lt;em&gt;Probabilities and Potential&lt;/em&gt;, North-Holland Publishing Company, 1978.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>General Theory of Processes - Part 1</title>
      <link>https://makkar.github.io/post/grlthprcs/</link>
      <pubDate>Tue, 13 Apr 2021 00:31:57 -0400</pubDate>
      <guid>https://makkar.github.io/post/grlthprcs/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\Prob}[1]{\bP\left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathcal{E}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\K}{\mathscr{K}}
\newcommand{\PO}{\mathfrak{P}}
\newcommand{\f}{\mathfrak{f}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\eps}{\varepsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 \; : \; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 \; | \; #2 \right\}}
\newcommand{\Ex}[1]{\bE\left[#1\right]}
\newcommand{\Exc}[2]{\bE\left(#1 \mid #2\right)}
\newcommand{\Pc}[2]{\PP\left( \left. #1 \, \right\vert \, #2\right)}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\PM}{\widehat{\otimes}_p}
\newcommand{\nn}{{n \in \N}}
\newcommand{\oi}{[0, \infty)}
\newcommand{\gr}[1]{[\![ #1 ]\!]}
\newcommand{\grr}[1]{]\!] #1 ]\!]}
\newcommand{\grl}[1]{[\![ #1 [\![}
\newcommand{\grrl}[1]{]\!] #1 [\![}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator{\dQ}{d\QQ}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\bE}{\mathbb{E}}
\DeclareMathOperator{\bP}{\mathbb{P}}
\DeclareMathOperator{\V}{\mathrm{Var}}
\DeclareMathOperator{\Log}{\mathrm{Log}}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt; 
&lt;p&gt;I have been attending a reading course on stochastic analysis led by 
&lt;a href=&#34;http://www.math.columbia.edu/~ik/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor Ioannis Karatzas&lt;/a&gt;, where the students take turn in presenting a topic of their choice. I recently presented on Choquet&amp;rsquo;s theory of capacities and its applications to measure theory and in the general theory of processes. This blog post is based on this presentation. I have freely copied&lt;sup&gt;
&lt;a href=&#34;#plagiarism&#34;&gt;*&lt;/a&gt;&lt;/sup&gt; from many sources, but my primary reference are the unfortunately unpublished lecture notes by Prof. Karatzas.&lt;/p&gt;
&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Introduction&lt;/li&gt;
&lt;li&gt;Pavings and Mosaics
&lt;ul&gt;
&lt;li&gt;Properties of pavings and mosaics&lt;/li&gt;
&lt;li&gt;Compact pavings&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Envelopes
&lt;ul&gt;
&lt;li&gt;Properties of envelopes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Capacitance&lt;/li&gt;
&lt;li&gt;Scrapers
&lt;ul&gt;
&lt;li&gt;Mixing of Scrapers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Choquet Capacities&lt;/li&gt;
&lt;li&gt;Measurable Projection&lt;/li&gt;
&lt;li&gt;Measurable Graph&lt;/li&gt;
&lt;li&gt;Debut&lt;/li&gt;
&lt;li&gt;Measurable Section&lt;/li&gt;
&lt;li&gt;Epilogue&lt;/li&gt;
&lt;li&gt;References&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Kolmogorov laid the modern axiomatic foundations of probability theory with the German monograph &lt;em&gt;Grundbegriffe der Wahrscheinlichkeitsrechnung&lt;/em&gt; which appeared in Ergebnisse Der Mathematik in 1933. This was a period of intense discussions on the foundations of probability, and a majority of probabilists at the time considered measure theory not only a waste of time, but also an offense to &amp;ldquo;probabilistic intuition&amp;rdquo; [1]. But by 1950, with the work of Doob in particular, these discussions of foundations had been settled.&lt;/p&gt;
&lt;p&gt;Continuous-time processes, on the other hand, were difficult to tame even with measure theory: if a particle is subject to random evolution, to show that its trajectory is continuous, or bounded, requires that all time values be considered, whereas classical measure theory can only handle a &lt;em&gt;countable&lt;/em&gt; infinity of time values. Thus, not only does probability depend on measure theory, but it also requires more of measure theory than the rest of analysis [1].&lt;/p&gt;
&lt;p&gt;The missing pieces of the puzzle, which will be the highlight of this and the next blog post, are the debut, section and projection theorems. These theorems are also indispensable in many applications, for instance in dynamic programming and stochastic control [3].&lt;/p&gt;
&lt;p&gt;To get a taste of these theorems, let&amp;rsquo;s recall a famous error made by Lebesgue in the paper &lt;em&gt;Sur les fonctions reprÃ©sentables analytiquement&lt;/em&gt; published in 1905. Consider the measurable space $(\R^2, \B(\R^2))$ and the projection map $\pi$ given by $\R^2 \ni (x,y) \mapsto \pi(x,y) = y \in \R$. It is easy to see that for any open set $O$ in $\R^2$, the set $\pi(O)$ is also open in $\R$ : Recall that the standard topology on $\R^2$ is same as the product topology on $\R^2$. By definition of the product topology on $\R^2$ an open set $O$ in $\R^2$ is  of the form $O = \bigcup_{i \in I} \bigcap_{j \in J_i} U_{ij} \times V_{ij}$ for open $U_{ij}, V_{ij}$ in $\R$, $I$ arbitrary and $J_i$ finite. A simple argument gives $\pi_2(O) = \bigcup_{i \in I} \bigcap_{j \in J_i} U_{ij}$ which is open in $\R$. In fact, more generally 
&lt;a href=&#34;https://proofwiki.org/wiki/Projection_from_Product_Topology_is_Open&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;projection from product topology is open&lt;/a&gt;. Now it seems reasonable to expect that for any Borel set $B \in \B(\R^2)$ its projection is also a Borel set in $\B(\R)$, and Lebesgue assumed this in his paper. But, in fact, this is FALSE! The error was spotted in around 1917 by Mikhail Suslin, who realised that the projection need not be Borel, and this lead to his investigation of analytic sets and to begin the study of what is now known as descriptive set theory [2].&lt;/p&gt;
&lt;p&gt;The problem is projection doesn&amp;rsquo;t commute with countable decreasing intersection. For example, consider the decreasing sequence of sets $S_n = (0, 1/n) \times \R$. Then $\pi(S_n) = \R$ for all $n$, giving $\bigcap_\nn \pi(S_n) = \R$, but $\bigcap_\nn S_n = \emp$, giving $\pi \left( \bigcap_\nn S_n \right) = \emp$. The measurable projection theorem stated next will be one of the highlights of this post.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Measurable Projection Theorem: &lt;/span&gt; Let $\probsp$ be a complete probability space, let $(K, \B(K))$ be a locally compact separable metric space endowed with the collection of its Borel sets, and denote by $\pi$ the projection of $K \times \Omega$ onto $\Omega$.
Then, for every $B \in \B(K) \otimes \F$, the projection $\pi(B) \in \F$.&lt;/p&gt;
&lt;p&gt;Why is proving such results difficult? As mentioned above it&amp;rsquo;s because projection doesn&amp;rsquo;t behave nicely with intersections. Nevertheless, let us try to see how one might try to prove the above theorem. A standard approach in measure theory is to construct a collection like&lt;/p&gt;
&lt;p&gt;$$
\mathscr{E} = \set{S \subseteq K \times \Omega}{\pi(S) \in \F}
$$&lt;/p&gt;
&lt;p&gt;which contains the sets satisfying the desired property, and show that it is a $\sigma-$algebra containing a simple collection, say $\mathscr{A},$ such that it easy to show that elements of $\mathscr{A}$ satisfy the desired property and $\mathscr{A}$ generates $\B(K) \otimes \F$, because then we will have $\B(K) \otimes \F \subseteq \mathscr{E}$. To this end, let&lt;/p&gt;
&lt;p&gt;$$
\mathscr{A} = \set{S \subseteq K \times \Omega}{S = E \times F \text{ for } E \in \B(K), F \in \F}
$$&lt;/p&gt;
&lt;p&gt;Then it is easy to see that $\mathscr{A} \subseteq \mathscr{E}$, $\mathscr{A}$ generates $\B(K) \otimes \F$ and that $\mathscr{A}$ is an algebra. If we could show that $\mathscr{E}$ is a monotone class, then we would be done on account of monotone class theorem. Increasing sequences are easily handled, in fact if $\se{S_n} _ \nn \subseteq K \times \Omega$ is any sequence, then&lt;/p&gt;
&lt;p&gt;$$
\pi\left(\bigcup_{n=1}^\infty S_n\right) = \bigcup_{n=1}^\infty \pi(S_n)
$$&lt;/p&gt;
&lt;p&gt;But if $S_1 \supseteq S_2 \supseteq \cdots$, then in general we CANNOT say&lt;/p&gt;
&lt;p&gt;$$
\pi\left(\bigcap_{n=1}^\infty S_n\right) = \bigcap_{n=1}^\infty \pi(S_n)
$$&lt;/p&gt;
&lt;p&gt;Enter Choquet&amp;rsquo;s theory of capacities. It provides the language to prove results like these. We know that every finite measure $\mu$ on $\R^d$ has the interior regularity property&lt;/p&gt;
&lt;p&gt;$$
\mu(B) = \sup_{\substack{K \in \K(\R^d) \\ K \subseteq B}} \mu(K), \quad \forall \; B \in \B(\R^d)
$$&lt;/p&gt;
&lt;p&gt;where $\K(\R^d)$ is the collection of compact sets in $\R^d$. The Choquet&amp;rsquo;s theory of capacities generalizes this approximation-from-below property, and distills those properties of measure that allow for such approximation to hold in very general settings. As we will see, monotonicity and continuity from above and below properties are at play here, and notions corresponding to complements or differences will be absent.&lt;/p&gt;
&lt;p&gt;The next few sections will be very abstract and it is easy to lose sight of our goal. Some people enjoy this mental gymnastics, but even if you find this dry, the reward at the end will be worth the initial struggle. We start with Choquet&amp;rsquo;s theory of capacities. The highlight of this part will be Choquet&amp;rsquo;s capacitability theorem. To prove this major result we will need to define a lot of new terminology and prove some major results like SierpiÅ„ski&amp;rsquo;s theorem and Sion&amp;rsquo;s theorem. Armed with Choquet&amp;rsquo;s capacitability theorem we will prove Measurable Section theorem which in turn will form the backbone of various other results in measure theory. These results in measure theory will then help us prove results in general theory of processes, but we will discuss this part in the next blog post.&lt;/p&gt;
&lt;h1 id=&#34;pavings-and-mosaics&#34;&gt;Pavings and Mosaics&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 1: &lt;/span&gt; Let $E$ be a nonempty set. A collection $\E$ of subsets of $E$ is called a &lt;em&gt;paving&lt;/em&gt; if it closed under finite unions and finite intersections. The pair $(E, \E)$ is then called a &lt;em&gt;paved space&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The concept of paving generalizes the concept of algebra.
It is easy to check that an arbitrary intersection of pavings is also a paving, and that the collection $\PO(E)$ of all subsets of $E$ is a paving. Thus for any collection $\A$ of subsets of $E$, we can define the notion of &lt;em&gt;paving generated by $\A$&lt;/em&gt; as the smallest paving of subsets of $E$ that contains $\A$ by simply defining it to be the intersection of all pavings of $E$ containing $\A$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 2: &lt;/span&gt; For two paved spaces $(E,\E)$ and $(F,\F)$, the &lt;em&gt;product paving&lt;/em&gt; of $\E$ and $\F$, denoted by $\E \otimes_p \F$, is a paving on $E \times F$ generated by all rectangles $\cR = \set{A \times B}{A \in \E, B \in \F}$.&lt;/p&gt;
&lt;p&gt;Using the fact that $(A_1 \times B_1) \cap (A_2 \times B_2) = (A_1 \cap A_2) \times (B_1 \cap B_2)$ we see that $\cR$ is stable under finite intersections. Therefore, any element of $\E \otimes_p \F$ is of the form $\bigcup_{i=1}^n A_i \times B_i$, where $A_i \in \E$ and $B_i \in \F$ for all $i=1,\ldots,n$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 3: &lt;/span&gt; Let $E$ be a nonempty set. A collection of subsets of $E$ which is closed under countable unions and countable intersections is called a &lt;em&gt;mosaic&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The concept of mosaic generalizes the concept of $\sigma-$algebra. Just like paving, it is easy to define the notion of &lt;em&gt;mosaic generated by a collection&lt;/em&gt;. They will always occur in the context of a paving $\E$ on $E$. We denote by $\widehat{\E}$ the mosaic generated by $\E$. $\E \PM \F$ will denote the mosaic generated by the product paving $\E \otimes_p \F$.&lt;/p&gt;
&lt;p&gt;Henceforth the notation $\E$ will be used for a paving on $E$.&lt;/p&gt;
&lt;h2 id=&#34;properties-of-pavings-and-mosaics&#34;&gt;Properties of pavings and mosaics&lt;/h2&gt;
&lt;p&gt;Just like the results connecting algebra and $\sigma-$algebra, we have results connecting pavings and mosaics.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 1: &lt;/span&gt; If $A \in \E$ implies $A^\comp = E \setminus A \in \widehat{\E}$, then $\widehat{\E} = \sigma(\E)$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Follows immediately from the monotone class theorem. $\square$&lt;/p&gt;
&lt;p&gt;As an example, if $E$ is a separable, locally compact metric space, and $\E = \K(E)$ is the collection of compact subsets of $E$, then this property holds. Too see this, note that in metric spaces compact sets are closed and thus their complement is open. It is a standard result in topology that in this case every open set is a countable union of compact sets. In fact, every second-countable locally compact Hausdorff space is $\sigma-$compact.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 2: &lt;/span&gt; The mosaic $\widehat{\E}$ is the smallest collection of subsets of $E$ that contains $\E$ and is closed under countable increasing unions and under countable decreasing intersections. In other words, if $\M(\E)$ denotes the monotone class generated by $\E$, then $\widehat{\E} = \M(\E)$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Since a mosaic is a monotone class and since $\E \subseteq \widehat{\E}$, we have $\M(\E) \subseteq \widehat{\E}$. For the other side, we will be done if we show that $\M(\E)$ is a mosaic, since $\E \subseteq \M(\E)$.&lt;br&gt;
The first property to note is that a monotone paving is a mosaic. To see this, let $\cR$ be a monotone paving and $A_1, A_2, \ldots \in \cR$. Then since $\cR$ is a paving, $B_n = \bigcup_{i=1}^n A_i \in \cR$ for all $\nn$. But $\se{B_n}_\nn$ is an increasing sequence and $\cR$ is a monotone class, and therefore their union $\bigcup_\nn A_n = \bigcup_\nn B_n \in \cR$, and hence $\cR$ is closed under countable unions. Similarly for countable intersections.&lt;br&gt;
Therefore, we will be done if we show that $\M(\E)$ is a paving.
To this end, for any $B \subseteq E$, let
$$\cK(B) := \set{A \subseteq E}{ A \cup B, A \cap B \in \M(\E)}$$
Notice that by symmetry, $A \in \cK(B) \iff B \in \cK(A)$. If $A_1 \subseteq A_2 \subseteq \cdots \in \cK(B)$ is an increasing sequence, then
$$
\bigcup_\nn A_n \cup B = \bigcup_\nn (A_n \cup B) \in \M(\E)
$$
$$
\bigcup_\nn A_n \cap B = \bigcup_\nn (A_n \cap B) \in \M(\E)
$$
and similarly for decreasing sequences. Therefore, if $\cK(B) \neq \emp$, then it is a monotone class. If $A, B \in \E$, then by the definition of paving, $A \in \cK(B)$. Since this is true for every $A \in \E$, we have $\E \subseteq \cK(B)$, and $\cK(B)$ is a monotone class containing $\E$. Since $\M(\E)$ is the smallest monotone class containing $\E$, we have
$$\M(\E) \subseteq \cK(B)$$
Hence if $A \in \M(\E)$ and $B \in \E$, then $A \in \cK(B)$, and therefore $B \in \cK(A)$. Since this is true for every $B \in \E$, it follows that
$$\M(\E) \subseteq \cK(A)$$
The validity of this relation for every $A \in \M(\E)$ is equivalent to the assertion that $\M(\E)$ is a paving. $\square$&lt;/p&gt;
&lt;h2 id=&#34;compact-pavings&#34;&gt;Compact pavings&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 4: &lt;/span&gt; A paving $\E$ is a &lt;em&gt;compact paving&lt;/em&gt;, if every decreasing sequence of nonempty elements of $\E$ has a nonempty intersection.&lt;/p&gt;
&lt;p&gt;For example, if $E$ is a separable metric space, the collection, $\K(E)$, of all compact subsets of $E$ is a compact paving. This is easy to see in light of the fact that every compact subset of $E$ is closed and this allows use of Cantor&amp;rsquo;s intersection theorem.
We define $$\E_\delta := \set{\bigcap_{\nn} A_n}{A_n \in \E \text{ for all } n \in \N}$$ and note that if $\E$ is a compact paving then so is $\E_\delta$.
The next lemma tells us when it acceptable to commute projection with countable intersections.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 3: &lt;/span&gt; Let $K$ and $E$ be two nonempty sets, and denote by $\pi$ the projection of $K \times E$ onto $E$. Suppose that $\cH$ is a paving of subsets of $K \times E$ with the property that, for every $x \in E$, the collection $\cH(x) := \set{H(x)}{H \in \cH}$ is a compact paving on $K$.&lt;br&gt;
Then, for every decreasing sequence $\se{H_n} _ {n \in \N}$ of sets in the paving $\cH_\delta$, we have
$$\pi\left(\bigcap _ {n \in \N} H_n\right) = \bigcap _ {n \in \N} \pi(H_n)$$
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; $\pi\left(\bigcap_{n \in \N} H_n\right) \subseteq \bigcap_{n \in \N} \pi(H_n)$ is easy to see because if $x \in \pi\left(\bigcap_{n \in \N} H_n\right)$ then there exists $(y,x) \in K \times E$ such that $(y,x) \in \bigcap_{n \in \N} H_n$ which implies $x \in \pi(H_n)$ for all $n \in \N$.&lt;br&gt;
For the other side, let $x \in \bigcap_{n \in \N} \pi(H_n)$. Then the sequence $\se{H_n(x)}_{n \in \N}$ is decreasing whose elements are nonempty and they are in $\cH(x)_\delta$. But since $\cH(x)_\delta$ is a compact paving by assumption, $\bigcap_n H_n(x)$ must be nonempty, implying $x \in \pi\left(\bigcap_{n \in \N} H_n\right)$. $\square$&lt;/p&gt;
&lt;h1 id=&#34;envelopes&#34;&gt;Envelopes&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 5: &lt;/span&gt; Let $(E, \E)$ be a paved space, and fix a subset $A \subseteq E$ as well as a decreasing sequence $\se{A_k} _ {k \in \N} \subseteq \mathfrak{P}(E)$. We say that $A$ is an &lt;em&gt;$\E-$envelope&lt;/em&gt; of $\se{A_k} _ {k \in \N}$, if there exists a decreasing sequence $\se{C_k} _ {k \in \N} \subseteq \E \cup \se{E}$ such that
$$A_k \subseteq C_k, \quad \forall \; k \in \N \quad \text{ and } \quad \bigcap _ {k \in \N} C_k \subseteq A \tag{1} \label{envelope}$$&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Let $E$ be a separable metric space, and $\E$ the paving consisting of all closed subsets of $E$. Then a subset $A$ of $E$ is an $\E-$envelope of a given decreasing sequence $\se{A_k} _ {k \in \N} \subseteq \PO(E)$ if, and only if, $A$ contains $\bigcap _ {k \in \N} \overline{A}_k$, the intersection of the closures of the sets $A_k, k \in \N$ in the sequence.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An abstract version of the one above: Let $(E, \E)$ be a paved space; for every subset $A$ of $E$, introduce the collection of sets $\A := \set{B \in \E \cup \se{E}}{A \subseteq B}$ and assume that the intersection $\overline{A} := \bigcap_{B \in \A} B$, called the &lt;i&gt;adherent of $A$ in the paving $\E$&lt;/i&gt;, belongs to $\E_\delta \cup \se{E}$, i.e., $\overline{A}$ is a countable intersection of sets in $\E \cup \se{E}$. We claim that $A$ is an $\E-$envelope of a given decreasing sequence $\se{A_k}_{k \in \N} \subseteq \PO(E)$ if, and only if, $A$ contains $\bigcap_{k \in \N} \overline{A}_k$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 4:&lt;/span&gt; In the setting of the last example, a subset $A$ of $E$ is an $\E-$envelope of a given decreasing sequence $\se{A_k} _ {k \in \N} \subseteq \PO(E)$ if, and only if, $A$ contains $\bigcap_{k \in \N} \overline{A}_k$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; The necessity is clear: if there exists a decreasing sequence $\se{C_k}_{k \in \N} \subseteq \E \cup \se{E}$ such that \eqref{envelope} is satisfied then $\overline{A}_k \subseteq C_k$ and therefore $\bigcap_{k \in \N} \overline{A}_k \subseteq \bigcap_{k \in \N} C_k \subseteq A$.&lt;br&gt;
To see the sufficiency, for every $k \in \N$, let $\se{B_n^k}_{n \in \N} \subseteq \E \cup \se{E}$ be a decreasing sequence such that $\overline{A}_k = \bigcap_{n \in \N} B_n^k$ (such a sequence exists because of the assumption in Example 2). Then $$C_k := B_k^1 \cap B_k^2 \cap \cdots \cap B_k^k, \quad k \in \N$$ defines a decreasing sequence of elements in $\E \cup \se{E}$ with $A_k \subseteq \overline{A}_k \subseteq C_k$ and $\bigcap_{k \in \N} \overline{A}_k =\bigcap_{k \in \N} C_k$. It follows that the set $A$ envelops the sequence $\se{A_k}_{k \in \N}$, if $A$ contains the countable intersection $\bigcap_{k \in \N} \overline{A}_k$; for then the decreasing sequence $\se{C_k}_{k \in \N} \subseteq \E \cup \se{E}$ satisfies the requirements of \eqref{envelope}. $\square$&lt;/p&gt;
&lt;h2 id=&#34;properties-of-envelopes&#34;&gt;Properties of envelopes&lt;/h2&gt;
&lt;p&gt;The next lemma lists some properties of envelopes which we will be using frequently.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Lemma 5:&lt;/span&gt; (i) If $A$ is an envelope of a given decreasing sequence $\se{A_n} _ {n \in \N} \subseteq \PO(E)$, then every subset of $E$ that contains $A$ is also an envelope of $\se{A_n} _ {n \in \N}$.&lt;br&gt;
(ii) Two decreasing sequences of subsets of $E$ that possess a common subsequence, admit the exact same envelopes.&lt;br&gt;
(iii) The collection of envelopes of a given decreasing sequence of subsets of $E$, is closed under countable intersections.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Parts (i) and (ii) are trivial. For part (iii), let $\se{A^k} _ {k \in \N}$ be a sequence of envelopes of a given decreasing sequence $\se{A_n} _ {n \in \N} \subseteq \PO(E)$. For each $k \in \N$, let $\se{B_n^k} _ \nn \subseteq \E \cup \se{E}$ be a decreasing sequence, such that $A_n \subseteq B_n^k$ for all $n \in \N$ and $\bigcap_{n \in \N} B_n^k \subseteq A^k$. Then $$C_n := B_n^1 \cap B_n^2 \cap \cdots \cap B_n^n, \quad n \in \N$$ defines a decreasing sequence of elements in $\E \cup \se{E}$ that satisfies $A_n \subseteq C_n$ and $\bigcap_{n \in \N} C_n = \bigcap_{(k,n) \in \N^2} B_n^k \subseteq \bigcap_{k \in \N} A^k$. It follows that $\bigcap_{k \in \N} A^k$ is an $\E-$envelope of $\se{A_n}_{n \in \N}$. $\square$&lt;/p&gt;
&lt;h1 id=&#34;capacitance&#34;&gt;Capacitance&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 6: &lt;/span&gt; Let $E$ be a nonempty set. A collection $\C$ of subsets of $E$ is called a &lt;em&gt;capacitance&lt;/em&gt;, if&lt;br&gt;
(i) whenever $A \in \C$ and $A \subseteq B$, then $B \in \C$, and&lt;br&gt;
(ii) whenever $\se{A_n} _ {n \in \N}$ is an increasing sequence of subsets of $E$ such that $\bigcup _ {n \in \N} A_n \in \C$, there is an integer $m$ such that $A_m \in \C$.&lt;/p&gt;
&lt;p&gt;Intuitively, a capacitance is a collection of &amp;ldquo;big&amp;rdquo; sets: the power set $\PO(E)$ is a capacitance, and so are the collections of nonempty and of uncountable subsets of $E$. The notion of pre-capacity, defined next, gives a more useful example.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 7: &lt;/span&gt; A function $I \colon \PO(E) \to \RR$ is called a &lt;em&gt;pre-capacity&lt;/em&gt;, if it is&lt;br&gt;
(i) monotone increasing, i.e., $I(A) \le I(B)$ holds for every $A \subseteq B$, and&lt;br&gt;
(ii) ascending, i.e., for every increasing sequence $\se{A_n} _ {n \in \N}$ we have $$I\left(\bigcup_{n \in \N} A_n\right) = \sup_{n \in \N} I(A_n)$$&lt;/p&gt;
&lt;p&gt;If $I \colon \PO(E) \to \RR$ is a pre-capacity, then for every given real number $t$ the collection $\C = \set{A \in \PO(E)}{I(A) &amp;gt; t}$ is a capacitance. Conversely, given a capacitance $\C$, one can associate to it a pre-capacity by defining $I(A) := 1$ whenever $A \in \C$ , and $I(A) := 0$ whenever $A \notin \C$; this then leads to the identification $\C = \set{A \in \PO(E)}{I(A) &amp;gt; 0}$.&lt;/p&gt;
&lt;p&gt;Henceforth assume that there is an underlying paved space $(E, \E)$ and a capacitance $\C$ of subsets of $E$.&lt;/p&gt;
&lt;h1 id=&#34;scrapers&#34;&gt;Scrapers&lt;/h1&gt;
&lt;img src=&#34;planing.jpg&#34; alt=&#34;scraping&#34; width=&#34;400&#34;/&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 8:&lt;/span&gt; A sequence $\f = \se{f_n} _ {n \in \N}$ of mappings $f_n \colon \left(\PO(E)\right)^n \to \PO(E)$ is called a &lt;em&gt;$\C-$scraper&lt;/em&gt; if&lt;br&gt;
(i) $f_n(B_1, B_2, \ldots, B_n) \subseteq B_n$ for all $n \in \N$ and for all sets $B_1, \ldots, B_n \in \PO(E)$, and&lt;br&gt;
(ii) whenever $B_n \in \C$, then $f_n(B_1, B_2, \ldots, B_n) \in \C$.&lt;/p&gt;
&lt;p&gt;Property (i) expresses the intuitive notion that $f_n(B_1, B_2, \ldots, B_n)$ &amp;ldquo;scrapes&amp;rdquo; $B_n$ and property (ii) ensures that &amp;ldquo;the scraping does not remove too big a chunk&amp;rdquo; from $B_n$. In French, scraper is called &lt;em&gt;rabotage&lt;/em&gt; which can be translated as &lt;em&gt;planing&lt;/em&gt;. A simple example of a scraper is the &lt;i&gt;identity scraper&lt;/i&gt; : $f_n(B_1, \ldots B_n) = B_n$ for all $n \in \N$ and for all sets $B_1, \ldots, B_n \in \PO(E)$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 9:&lt;/span&gt; Given a $\C-$scraper $\f = \se{f_n} _ {n \in \N}$, a (necessarily decreasing) sequence $\se{B_n} _ {n \in \N}$ of subsets of $E$ will be called &lt;em&gt;$\f-$scraped&lt;/em&gt;, if for all $n \in \N$ we have $$B_{n+1} \subseteq f_n(B_1, B_2, \ldots, B_n) \quad \text{and} \quad B_n \in \C$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 10:&lt;/span&gt; For any $B \in \PO(E)$ and $\C-$scraper $\f = \se{f_n} _ {n \in \N}$, the sequence $\se{P_n} _ {n \in \N} \subseteq \C$ defined by $P_1 := B$, $P_{n+1} := f_n(P_1, \ldots, P_n)$ for all $n \in \N$ is $\f-$scraped. It is called the &lt;i&gt;$\f-$scraped orbit of $B$&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 11:&lt;/span&gt; A $\C-$scraper $\f = \se{f_n}_{n \in \N}$  is called &lt;em&gt;compatible&lt;/em&gt; with a given set $A \in \PO(E)$, if $A$ envelopes every $\f-$scraped sequence $\se{B_n} _ {n \in \N}$ with $B_1 \subseteq A$.&lt;br&gt;
A set $A \in \PO(E)$ is &lt;em&gt;smooth&lt;/em&gt; for the capacitance $\C$, if there exists a $\C-$scraper compatible with it.&lt;/p&gt;
&lt;p&gt;The next result is central in this theory.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 1 [SierpiÅ„ski]:&lt;/span&gt; Let $(E, \E)$ be a paved space, and $\C$ a capacitance. The collection of subsets of $E$ which are smooth for the capacitance, is closed under countable increasing unions and under countable intersections.&lt;/p&gt;
&lt;p&gt;We will come back to its proof later. Let&amp;rsquo;s prove some of its consequences first.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 2:&lt;/span&gt; Let $(E ,\E)$ be a paved space, and $\C$ a capacitance. The elements of the mosaic $\widehat{\E}$ generated by $\E$ are smooth.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; An easy consequence of Theorem 1, Lemma 2 and the fact that every element of $\E$ is smooth because they are compatible with the identity scraper. $\square$&lt;/p&gt;
&lt;p&gt;This theorem is in turn very useful in proving some important results. We will discuss two of them. The first one is the metric space version of Choquet&amp;rsquo;s capacitability theorem. The proof of it will be very similar to the general Choquet&amp;rsquo;s capacitability theorem, but we will need Sion&amp;rsquo;s theorem for the general version, which will be the second result.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 12:&lt;/span&gt; Let $E$ be a compact metric space, endowed with the paving $\E = \K(E)$ of its compact sets. Let $I$ be a &lt;i&gt;metric capacity&lt;/i&gt; on $(E, \E)$, i.e., a pre-capacity that &amp;ldquo;descends on compacts&amp;rdquo; in the sense that for every decreasing sequence $\se{K_n} _ \nn \subseteq \K(E)$ it satisfies $I\left(\bigcap _ {n \in \N}K_n\right) = \inf _ {n \in \N} I(K_n)$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 3 [Metric space version of Choquet&amp;rsquo;s capacitability theorem]:&lt;/span&gt; For every Borel subset $B$ of a compact metric space $E$, and any metric capacity $I \colon \PO(E) \to \RR$, we have
$$
I(B) = \sup_{\substack{K \in \K(E) \\ K \subseteq B}} I(K)
$$
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Fix an arbitrary $B \in \B(E)$. If $I(B) = -\infty$ then $I(B) = I(\emp)$, and we have our desired equality trivially. Otherwise, we need to show that whenever $I(B) &amp;gt; t$ holds for some given real number $t$, there exists a compact set $K \subseteq B$ such that $I(K) \ge t$. Recall that $$\C = \set{A \in \PO(E)}{I(A) &amp;gt; t}$$ is a capacitance. Also recall that a subset $A$ of $E$ is an $\E-$envelope of a decreasing sequence $\se{A_n}_{n \in \N} \subseteq \PO(E)$ if and only if $A$ contains $\bigcap_{n \in \N}\overline{A}_n$.&lt;br&gt;
Lemma 1 gives that the mosaic $\widehat{\E}$ generated by $\E = \K(E)$ coincides with the Borel $\sigma-$algebra $\B(E)$. Hence by Theorem 2 every Borel set is smooth. Thus, there exists a $\C-$scraper $\f = \se{f_n}_{n \in \N}$ compatible with the set $B$.&lt;br&gt;
Consider the $\f-$scraped orbit of $B$, $\se{P_n} _ {n \in \N} \subseteq \C$. By construction $B$ is an envelope of $\se{P_n} _ {n \in \N}$, and hence it contains $K := \bigcap _ {n \in \N} \overline{P}_n$. $K$ is closed and hence also compact on account of being a subset of a compact set $E$; similarly for $\overline{P}_n$ for all $n \in \N$. But since $\se{P_n} _ {n \in \N} \subseteq \C$ we have $I(\overline{P}_n) &amp;gt; t$ for all $n \in \N$. Now use the descending on compacts property of $I$ to get $I(K) = I\left(\bigcap _ {n \in \N} \overline{P}_n\right) = \inf _ {n \in \N} I(\overline{P}_n) \ge t$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 4 [Sion&amp;rsquo;s theorem]:&lt;/span&gt; Let $(E,\E)$ be a paved space, and $\C$ a capacitance. For every element $B$ of $\C \cap \widehat{\E}$, there exists a decreasing sequence $\se{K_n} _ {n \in \N} \subseteq \C \cap \E$ such that $\bigcap _ {n \in \N} K_n \subseteq B$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Theorem 2 implies that the set $B$ is smooth, and thus there exists a $\C-$scraper $\f = \se{f_n} _ {n \in \N}$ compatible with it. Let $\se{P_n} _ {n \in \N} \subseteq \C$ be the $\f-$scraped orbit of $B$. Then $B$ is an envelope of $\se{P_n} _ {n \in \N}$, so there exists a decreasing sequence $\se{B_n} _ {n \in \N}$ of subsets of $\E \cup \se{E}$ such that $\bigcap _ \nn B_n \subseteq B$ and $P_n \subseteq B_n$ for all $\nn$. Notice $B_n \in \C$.&lt;br&gt;
If the sets $B_n$ belong to the paving $\E$ from a certain index $m$ onward, we take $K_n := B_{m+n}, n \in \N$ as our sequence. Otherwise if $B_n = E$ holds for all integers $n$, the set $B=E$ is the union of an increasing sequence of sets in $\E$ because $B \in \widehat{\E}$ and Lemma 2. Therefore, the fact that $B \in \C$ implies $B$ contains a set $K \in \C \cap \E$; it suffices then to take $K_n = K$ for all integers $n$. $\square$&lt;/p&gt;
&lt;p&gt;We now come back to the proof of Theorem 1. But first we will need the following clever operation on scrapers, and a couple of results.&lt;/p&gt;
&lt;h2 id=&#34;mixing-of-scrapers&#34;&gt;Mixing of Scrapers&lt;/h2&gt;
&lt;p&gt;Consider a sequence $\se{\f^k, k \in \N} = \se{\se{f^k_n} _ \nn, k \in \N}$ of scrapers, and a bijection $\N^2 \ni (p,q) \mapsto \beta(p,q) = p \star q \in \N$ which is strictly increasing in each of its arguments. For every integer $\nn$ and sets $P_1, P_2, \ldots, P_n$, if $n = p \star q$, let
$$
f_n(P_1, P_2, \ldots, P_n) := f^p_q(P_{p \star 1}, P_{p \star 2}, \ldots, P_{p \star q})
$$
It is easy to see that this defines a new scraper $\f = \se{f_n}_\nn$, called the &lt;i&gt;mixing of the scrapers $\se{\f^k, k \in \N}$ via the bijection $\beta$&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 5 :&lt;/span&gt; Let $\se{\f^k, k \in \N}$ be a sequence of scrapers, and denote by $\f$ its mixing by a bijection $\beta$. In order for a subset $A$ of $E$ to be compatible with $\f$, it suffices that it be compatible with one of the scrapers $\f^k, k \in \N$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Let $A \in \PO(E)$ be compatible with $\f^k$ for some arbitrary but fixed $k$. Consider also a sequence of sets $\se{P_n} _ \nn$, which is $\f-$scraped and whose first term $P_1$ is contained in $A$. We need to show that the set $A$ envelops $\se{P_n} _ \nn$.&lt;br&gt;
To do this, we exploit Lemma 5 (ii) and construct a decreasing sequence $\se{Q_n} _ \nn \subseteq \PO(E)$ which is a subsequence of $\se{P_n} _ \nn$ and show that $A$ envelops $\se{Q_n} _ \nn$. This will then imply $A$ envelops $\se{P_n} _ \nn$. To this end, let $$Q_n := P_{k \star n} \quad \forall \; \nn$$ Because $Q_1 = P_{k \star 1} \subseteq P_{1 \star 1} = P_1 \subseteq A$ and $A$ is compatible with $\f^k$, to show that $A$ envelops $\se{Q_n}_\nn$ it suffices to show $\se{Q_n} _ \nn$ is $\f^k-$scraped.&lt;br&gt;
Now, $Q_n \in \C$ for all $\nn$, so all that remains to be shown is that $Q_{n+1} \subseteq f^k_n(Q_1, Q_2, \ldots, Q_n)$ holds for all $\nn$. Because $\se{P_n} _ \nn$ is $\f-$scraped we have $$Q_{n+1} = P_{k \star (n+1)} \subseteq P_{1 + k\star n} \subseteq f_{k \star n}(P_1, P_2, \ldots, P_{k \star n}) = f^k_n(Q_1, Q_2, \ldots, Q_n)$$ giving the desired result. $\square$&lt;/p&gt;
&lt;p&gt;An immediate corollary of this theorem is : If $\se{A_n}_\nn$ is a sequence of smooth subsets of $E$, there exists a scraper $\f$ which is compatible with all the sets $A_n, \nn$.&lt;/p&gt;
&lt;h2 id=&#34;proof-of-theorem-1&#34;&gt;Proof of Theorem 1&lt;/h2&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Closure under countable intersections:&lt;/span&gt;&lt;br&gt;
Suppose $\se{A^k} _ {k \in \N}$ is a sequence of smooth sets, $A = \bigcap _ {k \in \N} A^k$, and $\f$ is a $\C-$scraper compatible with all of the sets $A^k, k \in \N$. If $\se{P_n} _ \nn$ is an $\f-$scraped sequence of sets such that $P_1 \subseteq A$, then $P_1 \subseteq A^k$ for all $k \in \N$. Our construction then implies $A^k$ is an $\E-$envelope of $\se{P_n} _ \nn$ for all $k \in \N$. Lemma 5 (iii) now implies $A$ is also an $\E-$envelope $\se{P_n} _ \nn$, showing that $A$ is compatible with $\f$, and hence smooth.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Closure under countable increasing unions:&lt;/span&gt;&lt;br&gt;
Suppose $\se{A^k} _ {k \in \N}$ is an increasing sequence of smooth sets, $A = \bigcup _ {k \in \N} A^k$, and $\f$ is a $\C-$scraper compatible with all of the sets $A^k, k \in \N$. The scraper $\f$ doesn&amp;rsquo;t work for this case and so we create a new one. For any $n \in \N$ and sets $P_1, P_2, \ldots, P_n$ we define
$$
\varphi_n(P_1, P_2, \ldots, P_n) = \begin{cases} P_n &amp;amp;\text{if } A \cap P_1 \notin \C \\ f_n(A^p \cap P_1, P_2, \ldots, P_n) &amp;amp;\text{if } A \cap P_1 \in \C \end{cases}
$$
where $p$ is the smallest integer such that $A^p \cap P_1 \in \C$. Such an integer does exist from part (ii) of the definition of capacitance. It is easy to see that $\Phi = \se{\varphi_n} _ \nn$ is a $\C-$scraper. It is sufficient to show that $\Phi$ is compatible with $A$ to finish our proof.&lt;br&gt;
Let $\se{P_n} _ \nn$ be a $\Phi-$scraped sequence of sets such that $P_1 \subseteq A$. By definition $P_1 \in \C$ and $A \cap P_1 = P_1$, and so from our construction $\varphi_n(P_1, P_2, \ldots, P_n) = f_n(A^p \cap P_1, P_2, \ldots, P_n)$. All elements of the sequence $A^p \cap P_1, P_2, \ldots, P_n, \ldots$ are in $\C$ and for all $n \in \N$
$$
P_{n+1} \subseteq \varphi_n(P_1, P_2, \ldots, P_n) = f_n(A^p \cap P_1, P_2, \ldots, P_n)
$$
and thus it follows that this sequence is $\f-$scraped. Now since $A^p$ is compatible with $\f$, $A^p$ is an envelope of this sequence, and also of $\se{P_n} _ \nn$ by Lemma 5 (ii). It follows that $A$ is an envelope of $\se{P_n} _ \nn$ by Lemma 5 (i) because $A^p \subseteq A$. $\square$&lt;/p&gt;
&lt;h1 id=&#34;choquet-capacities&#34;&gt;Choquet Capacities&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 13:&lt;/span&gt; A mapping $I \colon \PO(E) \to \RR$ is called a Choquet &lt;em&gt;$\E-$capacity&lt;/em&gt; if it is&lt;br&gt;
(i) monotone increasing, i.e., $I(A) \le I(B)$ holds for every $A \subseteq B$,&lt;br&gt;
(ii) ascending, i.e., for every increasing sequence $\se{A_n} _ {n \in \N} \subseteq \PO(E)$ we have $$I\left(\bigcup _ {n \in \N} A_n\right) = \sup _ {n \in \N} I(A_n)$$
(iii) descending on pavings, i.e., for every decreasing sequence $\se{E_n} _ {n \in \N} \subseteq \E$ we have $$I\left(\bigcap _ {n \in \N} E_n\right) = \inf _ {n \in \N} I(E_n)$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 14:&lt;/span&gt; A set $A \in \PO(E)$ is called &lt;em&gt;$I-$capacitable&lt;/em&gt; if
$$I(A) = \sup_{\substack{K \in \E_\delta \\ K \subseteq A}} I(K)$$&lt;/p&gt;
&lt;h3 id=&#34;examples-1&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Consider a paved space $(E, \E)$ with compact paving $\E$, and define $I(A) = 0$ if $A = \emp$, $I(A) = 1$ if $A \neq \emp$. Then $I$ is a Choquet $\E-$capacity. The property (iii) in the definition of capacity reflects now the assumption that the paving $\E$ is compact.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consider a probability space $\probsp$, then the outer measure
$$\PP^*(A) := \inf_{\substack{B \in \F \\ A \subseteq B}} \PP(B)$$
is a Choquet $\F-$capacity. Proof of this is a standard exercise in measure theory, albeit with a different terminology of continuity from below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Consider a locally compact, separable metric space $K$, and the paving $\K$ of its compact subsets. If $\pi$ denotes the projection of $K \times \Omega$ onto $\Omega$ and $$I(A) := \PP^*(\pi(A)) \text{ for all } A \in \PO(K \times \Omega),$$ then $I$ is a Choquet $(\K \otimes_p \F)-$capacity. Proof of this follows from $\pi\left( \bigcup_n A_n\right) = \bigcup_n \pi(A_n)$ and Lemma 3 combined with the properties of outer measure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 6 [Choquet&amp;rsquo;s capacitability theorem]:&lt;/span&gt; Consider a paved space $(E, \E)$, and let $I \colon \PO(E) \to \RR$ be a Choquet $\E-$capacity. Then every set $A \in \widehat{\E}$ is $I-$capacitable.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; Fix an arbitrary set $A \in \widehat{\E}$. If $I(A) = -\infty$ then $I(A) = I(\emp)$, and we have our desired equality trivially. Otherwise, we need to show that whenever $I(A) &amp;gt; t$ holds for some given real number $t$, there exists a set $K \in \E_\delta$ with $K \subseteq A$ and $I(K) \ge t$.&lt;br&gt;
Recall that $$\C = \set{B \in \PO(E)}{I(B) &amp;gt; t}$$ is a capacitance. Then $A \in \C$, and from Sion&amp;rsquo;s Theorem (Theorem 4) there exists a decreasing sequence $\se{K_n} _ \nn$ of elements in $\E \cap \C$ such that $\bigcap _ \nn K_n \subseteq A$. But then $I\left(\bigcap _ \nn K_n\right) = \inf _ \nn I(K_n) \ge t$, and thus we can take $K = \bigcap_\nn K_n$. $\square$&lt;/p&gt;
&lt;p&gt;We are now ready to prove some major results in measure theory.&lt;/p&gt;
&lt;h1 id=&#34;measurable-projection&#34;&gt;Measurable Projection&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 7 [Measurable Projection]:&lt;/span&gt; Let $\probsp$ be a complete probability space, let $(K, \B(K))$ be a locally compact separable metric space endowed with the collection of its Borel sets, and denote by $\pi$ the projection of $K \times \Omega$ onto $\Omega$. Then, for every $B \in \B(K) \otimes \F$, the projection $\pi(B) \in \F$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; We start by noticing $\B(K) \PM \F = \B(K) \otimes \F$. This follows from the fact that if $A \in \B(K) \otimes_p \F$, then $A = \bigcup _ {i=1}^n U_i \times V_i$ for some $U_i \in \B(K)$ and $V_i \in \F$, and thus it can be shown $A^\comp \in \B(K) \PM \F$, and thus Lemma 1 gives $\B(K) \PM \F = \sigma(\B(K) \otimes_p \F) = \B(K) \otimes \F$.&lt;/p&gt;
&lt;p&gt;Consider the paving $\K$ on $K$ consisting of all compact subsets of $K$, and introduce the $(\K \otimes_p \F)-$capacity $I(A) = \PP^*(\pi(A))$, $A \in \B(K \times \Omega)$ we saw before. Recall that $\B(K) = \widehat{\K}$, and so $\B(K) \PM \F = \K \PM \F$, the mosaic generated by the paving $\K \otimes_p \F$ ($\supseteq$ is trivial. For $\subseteq$ let $A \in \B(K) \otimes_p \F$ and show $A \in \K \PM \F$).&lt;/p&gt;
&lt;p&gt;Choquet&amp;rsquo;s capacitability theorem (Theorem 6) thus guarantees that every set in $\B(K) \otimes \F$ is $I-$capacitable. In particular, for every integer $n \in \N$, there exists a set $C_n \in (\K \otimes_p \F)_\delta$ contained in $B$ and such that&lt;/p&gt;
&lt;p&gt;$$I(C_n) \le I(B) \le I(C_n) + (1/n) \tag{2} \label{ms}$$&lt;/p&gt;
&lt;p&gt;Because $C_n \in (\K \otimes_p \F)_\delta$, $C_n$ is a countable intersection $C_n = \bigcap _ {m \in \N} G_n^m$, where each $G_n^m$ is a finite union of sets of the form $U \times V (U \in \K, V \in \F)$. Letting $H_n^m = \bigcap _ {i=1}^m G_n^i$, we see that $H_n^1 \supseteq H_n^2 \supseteq \cdots$ and $C_n = \bigcap _ {m \in \N} H_n^m$, where now $H_n^m$ is also a finite union of sets of the form $U \times V (U \in \K, V \in \F)$.&lt;/p&gt;
&lt;p&gt;The form of $H_n^m$ immediately implies $\pi(H_n^m) \in \F$ for all $(m,n) \in \N^2$. Lemma 3  implies
$$\pi(C_n) = \pi\left(\bigcap _ {m \in \N} H_n^m\right) = \bigcap _ {m \in \N} \pi(H_n^m) \in \F, \quad \forall \; n \in \N$$
which further implies $\pi\left(\bigcup _ {n \in \N} C_n\right) = \bigcup _ {n \in \N} \pi(C_n) \in \F$.&lt;/p&gt;
&lt;p&gt;On the other hand, $C_n \subseteq B$ for all $\nn$ and thus $\pi\left(\bigcup _ {n \in \N} C_n\right) \subseteq B$. But \eqref{ms} implies that the difference of these two sets is a $\PP-$null set, and the completeness of the probability space gives the desired conclusion $\pi(B) \in \F$. $\square$&lt;/p&gt;
&lt;p&gt;It is not easy to construct an example of a Borel set in the product space whose projection is not Borel. It requires study of analytic sets. Check out Corollary 8.2.17 in [4] for more details.&lt;/p&gt;
&lt;h1 id=&#34;measurable-graph&#34;&gt;Measurable Graph&lt;/h1&gt;
&lt;p&gt;The next theorem is a very visual theorem, especially in the case where $K$ and $\Omega$ are $\R$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 15:&lt;/span&gt; We call a set $G \in \B(K) \otimes \F$ &lt;i&gt;measurable graph&lt;/i&gt;, if for every $\omega \in \Omega$ the section $G(\omega) = \set{y \in K}{(y,\omega) \in G}$ contains at most one point.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 8 [Measurable Graph]:&lt;/span&gt; A subset $G$ of $K \times \Omega$ is a measurable graph, if and only if, there exists a set $\Xi \in \F$ and a measurable mapping $g \colon \Xi \to K$, such that $G = \set{(y,\omega) \in K \times \Xi}{y = g(\omega)}$.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: Sufficiency.&lt;/span&gt; If $\Xi$ and $g$ are as stated, the set $G = \set{(y,\omega) \in K \times \Xi}{y = g(\omega)}$ equals the pre-image $\varphi^{-1}(\Delta)$ of the diagonal $\Delta = \set{(y,y) \in K \times K}{y \in K}$ under the mapping
$$K \times \Xi \ni (y,\omega) \mapsto \varphi(y,\omega) := (y, g(\omega)) \in K \times K$$
This mapping $\varphi$ is $(\B(K) \otimes \F)-$measurable because of the facts that $\B(K \times K) = \B(K) \otimes \B(K)$, on account of $K$ being separable, and that $g$ is measurable. Since $\Delta$ is a closed set, $\Delta \in \B(K \times K)$ and thus $G$ is a measurable graph.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Necessity.&lt;/span&gt; Suppose that $G$ is a measurable graph, and let $\Xi := \pi(G)$. Then $\Xi \in \F$ by Measurable Projection theorem. For every $\omega \in \Xi$, define $g(\omega)$ to be the unique element of the set $G(\omega)$. We want to show $g \colon \Xi \to K$ is measurable. Indeed for any $H \in \B(K)$, it is easy to see that $g^{-1}(H) = \pi\left(G \cap (H \times \Omega)\right) \in \F$ where the inclusion follows from Measurable Projection theorem. $\square$&lt;/p&gt;
&lt;h1 id=&#34;debut&#34;&gt;Debut&lt;/h1&gt;
&lt;p&gt;Now let $K = [0, \infty)$, the case important in stochastic processes.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 16:&lt;/span&gt; Let $A \subseteq [0, \infty) \times \Omega$. The &lt;em&gt;debut&lt;/em&gt; of $A$ is the nonnegative function $D_A \colon \Omega \to \RR$ defined as
$$D_A(\omega) = \inf\set{t \in [0, \infty)}{(t, \omega) \in A}$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 9 [Measurable Debut]:&lt;/span&gt; Let $\probsp$ be a complete probability space, and consider a measurable set $A \in \B([0, \infty)) \otimes \F.$ Then the debut $D_A$ of this set is a random variable.&lt;br&gt;
&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; For any given real number $t &amp;gt; 0$, the set $D_A^{-1}([0,t)) = \set{\omega \in \Omega}{D_A(\omega) &amp;lt; t}$ is the projection onto $\Omega$ of the measurable subset $A \cap ([0, t) \times \Omega) \in \B([0, \infty)) \otimes \F$. To see this, note $\omega \in D_A^{-1}([0,t))$ $\iff$ $0 \le D_A(\omega) &amp;lt; t$ $\iff$ $\exists , s \in [0,t)$ such that $(s, \omega) \in A$ $\iff$ $\omega \in \pi(A \cap ([0, t) \times \Omega))$.  Measurable Projection theorem shows that this set is in $\F$. $\square$&lt;/p&gt;
&lt;h1 id=&#34;measurable-section&#34;&gt;Measurable Section&lt;/h1&gt;
&lt;p&gt;Let $\probsp$ be a complete probability space, and consider a set $A \subseteq \oi \times \Omega$. Then for every $\omega \in \pi(A)$, there exists a $t \in \oi$ such that $(t, \omega) \in A$. In other words, we can define a mapping $Z \colon \pi(A) \to \oi$. It is convenient to extend $Z$ to the whole of $\Omega$ by setting $Z = \infty$ on $\Omega \setminus \pi(A)$. When is it possible to choose $Z$ such that it is measurable? The measurable section theorem (also known as measurable selection theorem) says that it is possible to define $Z$ to be measurable if $A \in \B(\oi) \otimes \F$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 17:&lt;/span&gt; For a mapping $Y \colon \Omega \to [0, \infty]$ we shall define its &lt;em&gt;graph&lt;/em&gt; as the product set
$$\gr{Y} := \set{(t, \omega) \in [0, \infty) \times \Omega}{Y(\omega) = t}$$&lt;/p&gt;
&lt;p&gt;The condition $(Z(\omega), \omega) \in A$ whenever $Z &amp;lt; \infty$ can then be expressed by saying $\gr{Z} \subseteq A$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 10 [Measurable Section]:&lt;/span&gt; Let $\probsp$ be a complete probability space, and consider a measurable set $A \in \B([0, \infty)) \otimes \F$. Then there exists a random variable $Z \colon \Omega \to [0, \infty]$ with $\gr{Z} \subseteq A$ and $\se{Z &amp;lt; \infty} = \pi(A)$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof:&lt;/span&gt; &lt;span style=&#34;color:darkgray; font-size: 14pt&#34;&gt;We shall first show that for every $\eps &amp;gt; 0$ there exists a random variable $Z_\eps \colon \Omega \to [0, \infty]$ with $\gr{Z_\eps} \subseteq A$ and $\Prob{\pi(A)} \le \Prob{Z_\eps &amp;lt; \infty} + \eps$.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To this end, recall that if $K = [0, \infty)$ and $\K$ is the paving of all compact subsets of $K$, then we have the $\K \otimes_p \F-$capacity $I(A) = \PP^*(\pi(A)), A \in \PO(K \times \Omega)$. Therefore, every $A \in \B(K) \otimes \F$ is $I-$capacitable, and fixing $A$, there exists for every $\eps &amp;gt; 0$ a set $C_\eps \in (\K \otimes_p \F)_\delta$ such that
$$C_\eps \subseteq A, \quad I(C_\eps) \le I(A) \le I(C_\eps) + \eps \tag{3} \label{eq_sec}$$
Let $Z_\eps := D_{C_\eps}$ be the debut of this $C_\eps$, then Measurable Debut theorem (Theorem 9) implies $Z_\eps$ is a random variable. For every $\omega \in \Omega$, the section $C_\eps(\omega)$ is a compact subset of $K$ (use the facts that compact$\iff$closed and bounded here, and $C_\eps \in (\K \otimes_p \F)_\delta$). Notice that if $(t, \omega) \in \gr{D_{C_\eps}}$ then $D_{C_\eps}(\omega) = t$ which is same as saying $t = \inf \set{s \in [0, \infty)}{(s,\omega) \in C_\eps}$, but since $C_\eps(\omega)$ is closed this implies $(t, \omega) \in C_\eps$. Therefore, $\gr{Z_\eps} \subseteq C_\eps$, showing the first requirement.&lt;/p&gt;
&lt;p&gt;The second requirement $\Prob{\pi(A)} \le \Prob{Z_\eps &amp;lt; \infty} + \eps$ is true because $\pi(A) \in \F$ by Measurable Projection theorem (Theorem 7) and $\se{Z_\eps &amp;lt; \infty} = \pi(C_\eps) \in \F$, and now use \eqref{eq_sec}.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 14pt&#34;&gt;Let us now construct a random variable $Z$ to satisfy the properties claimed in the theorem.&lt;/span&gt; Set $A_1 = A$; from above there exists a random variable $Z_1 \colon \Omega \to [0, \infty]$ with $\gr{Z_1} \subseteq A_1$ and $\Prob{\pi(A_1)} \le 2 \Prob{Z_1 &amp;lt; \infty}$, by taking $\eps = \Prob{Z_1 &amp;lt; \infty} &amp;gt; 0$; if it happens that $\Prob{Z_1 &amp;lt; \infty} = 0$, then $\Prob{\pi(A_1)} = 0$ and the inequality is still true.&lt;br&gt;
Set $A_2 = A_1 \setminus ([0, \infty) \times \se{Z_1 &amp;lt; \infty})$ and, reasoning as before, construct $Z_2 \colon \Omega \to [0, \infty]$ with $\gr{Z_2} \subseteq A_2$ and $\Prob{\pi(A_2)} \le 2 \Prob{Z_2 &amp;lt; \infty}$.&lt;/p&gt;
&lt;p&gt;Continuing this way, we obtain a sequence $\se{Z_n} _ {\nn}$ such that $\gr{Z_n} \subseteq A$, the projections $\pi(\gr{Z_n})$ are disjoint, and we have
$$\sum_{k=1}^n \Prob{Z_k &amp;lt; \infty} \ge (1 - 2^{-n}) \Prob{\pi(A)}, \quad \forall \; \nn \tag{4} \label{eq_notsure}$$
The random variable $Z$ defined as $Z := Z_k$ on $\se{Z_k &amp;lt; \infty}$ for each $k \in \N$, and $Z := \infty$ otherwise, satisfies therefore $\gr{Z} \subseteq A$, thus also $\se{Z &amp;lt; \infty} \subseteq \pi(A).$&lt;/p&gt;
&lt;p&gt;On the other hand, letting $n \to \infty$ in \eqref{eq_notsure}, we see that $\se{Z &amp;lt; \infty}$ and $\pi(A)$ have the same probability. Therefore, the completeness of the probability space implies these two sets can be made equal.  $\square$&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;With this we are done laying the foundations. In the 
&lt;a href=&#34;https://makkar.github.io/post/grlthprcs2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next blog post&lt;/a&gt;, we will discuss applications of these results in the general theory of processes.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Paul-AndrÃ© Meyer. 
&lt;a href=&#34;http://www.jehps.net/juin2009/Meyer.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Stochastic Processes from 1950 to the Present&lt;/em&gt;&lt;/a&gt; (Translated from the French by Jeanine Sedjro), 2009.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://almostsuremath.com/stochastic-calculus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Almost Sure&lt;/a&gt; blog by George Lowther.&lt;/li&gt;
&lt;li&gt;Nicole El Karoui and Xiaolu Tan. 
&lt;a href=&#34;https://arxiv.org/abs/1310.3363&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Capacities, Measurable Selection and Dynamic Programming Part I: Abstract Framework&lt;/em&gt;&lt;/a&gt;, 2013.&lt;/li&gt;
&lt;li&gt;Donald L. Cohn. &lt;em&gt;Measure Theory&lt;/em&gt;, Second edition, BirkhÃ¤user, 2013.&lt;/li&gt;
&lt;li&gt;Claude Dellacherie. &lt;em&gt;Capacities and analytic sets&lt;/em&gt;, Part of the Lecture Notes in Mathematics book series (LNM, volume 839), Springer-Verlag, 1981.&lt;/li&gt;
&lt;li&gt;Claude Dellacherie. &lt;em&gt;CapacitÃ©s et processus stochastiques&lt;/em&gt;, Springer-Verlag, 1972.&lt;/li&gt;
&lt;li&gt;Claude Dellacherie and Paul-AndrÃ© Meyer. &lt;em&gt;Probabilities and Potential&lt;/em&gt;, North-Holland Publishing Company, 1978.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;p&gt;&lt;a name=&#34;plagiarism&#34;&gt;*&lt;/a&gt;: If you steal from one author, itâ€™s plagiarism; if you steal from many, itâ€™s research.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A note on conditional probability</title>
      <link>https://makkar.github.io/post/regularcondprob/</link>
      <pubDate>Fri, 05 Feb 2021 17:24:39 -0500</pubDate>
      <guid>https://makkar.github.io/post/regularcondprob/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Prob}[1]{\PP \left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 \; : \; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 ; | ; #2 \right\}}
\newcommand{\Ex}[1]{\E\left(#1\right)}
\newcommand{\Exc}[2]{\E\left(#1 \mid #2\right)}
\newcommand{\Pc}[2]{\PP\left( \left. #1 \, \right\vert \, #2\right)}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ph}[1]{\varphi^{-1}\left(#1\right)}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathrm{Var}}
\DeclareMathOperator{\Log}{\mathrm{Log}}
\DeclareMathOperator{\O}{\mathcal{O}}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The concept of conditional probability is central to probability theory and excellent treatment of it can be found in many books. My aim with this blog post is to consolidate in one place some ideas around it which helped me form a better intuition. These ideas will be useful if you have already been exposed to this concept from a textbook and just want one more person&amp;rsquo;s ramblings about it.&lt;/p&gt;
&lt;p&gt;I will start by defining conditional expectation and stating some of its properties. It will be a grave injustice to claim my discussion of it is complete since I don&amp;rsquo;t even prove its existence; this section exists solely for establishing notation. I will then spend some time discussing conditional probability, relating it to the traditional notion of&lt;/p&gt;
&lt;p&gt;$$
\PP(A \mid B) = \frac{\PP(A \cap B)}{\PP(B)}
$$&lt;/p&gt;
&lt;p&gt;These discussions will naturally lead to the notions of regular conditional probability and regular conditional distribution which I discuss next.&lt;/p&gt;
&lt;h1 id=&#34;conditional-expectation&#34;&gt;Conditional expectation&lt;/h1&gt;
&lt;p&gt;Recall the concept of conditional expectation.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 1: &lt;/span&gt; Let $\probsp$ be a probability space, and $X$ a random variable with $\Ex{|X|} &amp;lt; \infty$. Let $\G$ be a sub-$\sigma$-algebra of $\F$. Then there exists a random variable $Y$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$Y$ is $\G$ measurable,&lt;/li&gt;
&lt;li&gt;$\Ex{|Y|} &amp;lt; \infty$,&lt;/li&gt;
&lt;li&gt;$\int_G Y \dP = \int_G X \dP$ for every $G \in \G$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Remarks: &lt;/span&gt; (1.) It is easy to see from the $\pi-\lambda$ theorem that the last condition can be relaxed such that $\int_G Y \dP = \int_G X \dP$ for every $G$ in some $\pi$-system which contains $\Omega$ and generates $\G$. (2.) If $Y&#39;$ is another random variable with the three properties above then $Y&amp;rsquo; = Y$ a.s.. Therefore, $Y$ in the theorem above is called a &lt;em&gt;version&lt;/em&gt; of the conditional expectation. The notation $\Exc{X}{\G}$ is used to denote this unique (up to a.e. equivalence) random variable.&lt;/p&gt;
&lt;p&gt;The proof of this standard theorem can be found in any probability text. See [1,2] for example.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 1: &lt;/span&gt; In the setting of Theorem 1, if $Z$ is a random variable, we write $\Exc{X}{Z}$ for $\Exc{X}{\sigma(Z)}$.&lt;/p&gt;
&lt;p&gt;The fact that conditional expectation is defined as a random variable might come as surprising, but the correspondence with the traditional usage of conditional expectation as a number becomes clear once you realize that here we are conditioning on a $\sigma$-algebra instead of a single event. For example, consider the life expectancy of a new born baby conditioned on sex. This is a random variable that takes one value for males and another value for females.&lt;/p&gt;
&lt;h2 id=&#34;properties-of-conditional-expectation&#34;&gt;Properties of conditional expectation&lt;/h2&gt;
&lt;p&gt;For completeness I state some useful properties of conditional expectation. You can find the proofs in [1,2] for example. Most of them are parallels to well-known properties of (unconditional) expectation. Assume that all the $X$&#39;s satisfy $\Ex{|X|} &amp;lt; \infty$ and let $\G, \cH$ be sub-$\sigma$-algebras of $\F$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[Linearity] $\Exc{a_1 X_1 + a_2 X_2}{\G} = a_1 \Exc{X_1}{\G} + a_2 \Exc{X_2}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Positivity] If $X \ge 0$, then $\Exc{X}{\G} \ge 0$ a.s.&lt;/li&gt;
&lt;li&gt;[Monotone convergence theorem for conditional expectation] If $\Ex{|Y|} &amp;lt; \infty$ and $Y \le X_n \uparrow X$ a.s., then $\Exc{X_n}{\G} \uparrow \Exc{X}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Fatou&amp;rsquo;s lemma for conditional expectation] If $\Ex{|Y|} &amp;lt; \infty$ and $Y \le X_n$ for all $n \ge 1$ a.s., then $\Exc{\limi X_n}{\G} \le \limi \Exc{X_n}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Dominated convergence theorem for conditional expectation] If $|X_n| \le |Y|$ for all $n \ge 1$, $\Ex{Y} &amp;lt; \infty$, and $X_n \to X$ a.s., then $\Exc{X_n}{\G} \to \Exc{X}{\G}$ a.s.&lt;/li&gt;
&lt;li&gt;[Tower property] If $\cH \subseteq \G$, then $\Exc{\Exc{X}{\G}}{\cH} = \Exc{\Exc{X}{\cH}}{\G} = \Exc{X}{\cH}$ a.s.&lt;/li&gt;
&lt;li&gt;[Taking out what&amp;rsquo;s known] If $Y$ is $\G$-measurable and bounded, then
$$\Exc{Y X}{\G} = Y \Exc{X}{\G} \quad \text{a.s.}\tag{1} \label{eq1}$$
If $p &amp;gt; 1$, $1/p + 1/q = 1$, $X \in L^p(\Omega, \F, \PP)$ and $Y \in L^q(\Omega, \G, \PP)$, then \eqref{eq1} again holds. If $X$ is a nonnegative $\F$-measurable random variable, $Y$ is a nonnegative $\G$-measurable random variable, $\Ex{X} &amp;lt; \infty$ and $\Ex{XY} &amp;lt; \infty$, then \eqref{eq1} again holds.&lt;/li&gt;
&lt;li&gt;[Role of independence] If $\cH$ is independent of $\sigma(\sigma(X), \G)$, then $\Exc{X}{\sigma(\G, \cH)} = \Exc{X}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;conditional-probability&#34;&gt;Conditional probability&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 2: &lt;/span&gt; In the setting of Theorem 1, if $A \in \F$, we let $\Pc{A}{\G}$ to mean $\Exc{\ind{A}}{\G}$ and call it the &lt;em&gt;conditional probability of&lt;/em&gt; $A$ &lt;em&gt;given&lt;/em&gt; $\G.$ Here $\ind{A}$ is the indicator random variable. If $B \in \F$, we let $\Pc{A}{B}$ to mean $\Exc{\ind{A}}{\ind{B}}$.&lt;/p&gt;
&lt;p&gt;Just like conditional expectation, conditional probability, as defined above, is a random variable! Unlike conditional expectation this isn&amp;rsquo;t very palpable and deserves more rumination [3]. We have our probability space $\probsp$ and let $A,B \in \F$ be such that $\PP(B) \neq 0$ and $\PP(B^\comp) \neq 0$. Then our traditional notion of conditional probability tells us that the conditional probability of $A$ given $B$ is defined by&lt;/p&gt;
&lt;p&gt;$$
\PP_B(A) = \frac{\PP(A \cap B)}{\PP(B)}
$$&lt;/p&gt;
&lt;p&gt;Let us investigate how $\PP_B(A)$ depends on $B$. To this end, introduce the discrete measurable space $(\Lambda, 2^\Lambda)$ with $\Lambda = \se{\lambda_1, \lambda_2}$, and a measurable mapping $T \colon \Omega \to \Lambda$ such that&lt;/p&gt;
&lt;p&gt;$$
T(\omega) = \begin{cases}
\lambda_1 &amp;amp; \text{ if } \omega \in B   \\&lt;br&gt;
\lambda_2 &amp;amp; \text{ if } \omega \in B^\comp
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;Define the two measures $\nu_A$ and $\nu$ on $(\Lambda, 2^\Lambda)$ as follows for any $E \subseteq \Lambda$&lt;/p&gt;
&lt;p&gt;$$
\nu_A(E) = \Prob{A \cap T^{-1}(E)}
$$&lt;/p&gt;
&lt;p&gt;$$
\nu(E) = \Prob{T^{-1}(E)}
$$&lt;/p&gt;
&lt;p&gt;Then it is easy to see that&lt;/p&gt;
&lt;p&gt;$$
\PP_B(A) = \frac{\nu_A(\se{\lambda_1})}{\nu(\se{\lambda_1})}
$$&lt;/p&gt;
&lt;p&gt;$$
\PP_{B^\comp}(A) = \frac{\nu_A(\se{\lambda_2})}{\nu(\se{\lambda_2})}
$$&lt;/p&gt;
&lt;p&gt;In other words conditional probability may be viewed as a measurable function on $\Lambda$. This can easily be generalized to any finite setting as follows. Let $\se{A_1, \ldots, A_n} \subseteq \F$ be a partition of $\Omega$, i.e., $A_i \cap A_j = \emp$ for $i \neq j$ and $\bigcup_i A_i = \Omega$. Introduce the discrete measurable space $(\Lambda, 2^\Lambda)$ with $\Lambda = \se{\lambda_1, \ldots, \lambda_n}$. Define a measurable mapping $T \colon \Omega \to \Lambda$ such that $T(\omega) = \lambda_i$ whenever $\omega \in A_i$. Define the measures $\nu_{A_1}, \ldots, \nu_{A_n}, \nu$ on $(\Lambda, 2^\Lambda)$ as follows for any $E \subseteq \Lambda$&lt;/p&gt;
&lt;p&gt;$$
\nu_{A_i}(E) = \Prob{A_i \cap T^{-1}(E)} \quad \text{for all } i = 1, \ldots, n
$$&lt;/p&gt;
&lt;p&gt;$$
\nu(E) = \Prob{T^{-1}(E)}
$$&lt;/p&gt;
&lt;p&gt;Then once again we have for any $A \in \F$&lt;/p&gt;
&lt;p&gt;$$
\PP_{A_i}(A) = \frac{\PP(A \cap A_i)}{\PP(A_i)} = \frac{\nu_{A_i}(\se{\lambda_i})}{\nu(\se{\lambda_i})} \quad \text{for all } i = 1, \ldots, n
$$&lt;/p&gt;
&lt;p&gt;These considerations are what motivated the definition of conditional probability in general cases as you see in Definition 2. If $T$ is any measurable mapping from $\probsp$ into a measurable space $(\Lambda, \LL)$, and if we write $\nu_A(E) = \Prob{A \cap T^{-1}(E)}$ where $A \in \F$ and $E \in \LL$, then it is clear that $\nu_E$ and $\PP \circ T^{-1}$ are measures on $\LL$ such that $\nu_A \ll \PP \circ T^{-1}$. Radon-Nikodym theorem now implies that there exists an $\PP \circ T^{-1}$-integrable function $p_A$, unique upto $\PP \circ T^{-1}$-a.e., such that&lt;/p&gt;
&lt;p&gt;$$
\Prob{A \cap T^{-1}(E)} = \int_E p_A(\lambda) \; \PP \circ T^{-1}(\dd \lambda) \quad \text{for all } E \in \LL
$$&lt;/p&gt;
&lt;p&gt;We anoint $p_A(\lambda)$ as the conditional probability of $A$ given $\lambda$ or the conditional probability of $A$ given that $T(\omega) = \lambda$. Note that here we are conditioning on a measurable mapping $T$ instead of a sub-$\sigma$-algebra, but this notion is related to conditioning on $\sigma(T)$ as will become clear ahead. Keep this &amp;ldquo;rumination&amp;rdquo; in mind when we discuss regular conditional distribution later.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at our definition of conditional probability from the other direction and show that $\Pc{A}{B}$ as defined in Definition 2 conforms to our traditional usage. To start, note that $\sigma(\ind{B}) = \se{\emp, B, B^\comp, \Omega}$, and since $\Pc{A}{B}$ is $\sigma(\ind{B})$ measurable, it must be constant on each of the sets $B, B^\comp$, thereby necessitating&lt;/p&gt;
&lt;p&gt;$$
\Pc{A}{B}(\omega) = \begin{cases}
\frac{\PP(A \cap B)}{\PP(B)} &amp;amp; \text{ if } \omega \in B   \\&lt;br&gt;
\frac{\PP(A \cap B^\comp)}{\PP(B^\comp)} &amp;amp; \text{ if } \omega \in B^\comp
\end{cases}
$$
because of property 3. of Theorem 1 by taking $G$ to be $B$ and $B^\comp$. Of course, if any of the sets $B$ or $B^\comp$ is of measure $0$, then you can take the corresponding value for $\Pc{A}{B}(\omega)$ to be anything in $[0,1]$ and it won&amp;rsquo;t matter since the concept of conditional probability is defined up to sets of measure $0$.&lt;/p&gt;
&lt;h2 id=&#34;properties-of-conditional-probability&#34;&gt;Properties of conditional probability&lt;/h2&gt;
&lt;p&gt;Positivity property (property 2. above) and monotone convergence property (property 3. above) imply $0 \le \Pc{A}{\G} \le 1$ a.s. for any $A \in \F$, $\Pc{A}{\G} = 0$ a.s. if and only if $\PP(A) = 0$, and $\Pc{A}{\G} = 1$ a.s. if and only if $\PP(A) = 1.$&lt;/p&gt;
&lt;p&gt;Let $A_1, A_2, \ldots \in \F$ be a sequence of disjoint sets. By linearity (property 1. above) and monotone convergence theorem for conditional expectation, we see that&lt;/p&gt;
&lt;p&gt;$$
\Pc{\bigcup_n A_n}{\G} = \sum_n \Pc{A_n}{\G} \quad \text{a.s.} \tag{2} \label{eq2}
$$&lt;/p&gt;
&lt;p&gt;If $A_n \in \F$, $n \ge 1$ and $\limn A_n = A$, then we also have $\limn \Pc{A_n}{\G} = \Pc{A}{G}$ a.s..&lt;/p&gt;
&lt;p&gt;It seems very tempting from the foregoing discussion to claim that $\Pc{\cdot}{\G}$ is a probability measure on $\F$ for almost all $\omega \in \Omega$, but except for some nice spaces, which we will discuss below, this isn&amp;rsquo;t true. Let us first try to see this intuitively [2]. Equation \eqref{eq2} holds for all $\omega \in \Omega$ EXCEPT for some null set which may well depend on the particular sequence $\se{A_n}$.  It does NOT stipulate that there exists a fixed null set $N \in \F$ such that&lt;/p&gt;
&lt;p&gt;$$
\Pc{\bigcup_n A_n}{\G}(\omega) = \sum_n \Pc{A_n}{\G}(\omega), \quad \omega \in N^\comp
$$&lt;/p&gt;
&lt;p&gt;for every disjoint sequence $\se{A_n} \subseteq \F$. Except in trivial cases, there are uncountably many disjoint sequences, and therefore we will need uncountable union of such null sets to be of measure $0$, which of course may not even be defined let alone be of measure $0$. To further drive this point home you can take a look at an explicit example of how this can fail in an exercise in [3] page 210.&lt;/p&gt;
&lt;h1 id=&#34;regular-conditional-probability&#34;&gt;Regular conditional probability&lt;/h1&gt;
&lt;p&gt;Motivated by our discussion above, we define regular conditional probability as follows.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 3: &lt;/span&gt; Let $\probsp$ be a probability space and $\G, \cH$ be sub-$\sigma$-algebras of $\F$. A &lt;em&gt;regular conditional probability&lt;/em&gt; on $\cH$ given $\G$ is a function $\PP(\cdot, \cdot) \colon \cH \times \Omega \to [0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;for a.e. $\omega \in \Omega$, $\PP(\cdot, \omega)$ is a probability measure on $\cH$,&lt;/li&gt;
&lt;li&gt;for each $A \in \cH$, $\PP(A, \cdot)$ is a $\G$-measurable function on $\Omega$ coinciding with the conditional probability of $A$ given $\G$, i.e., $\PP(A, \cdot) = \Pc{A}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;rsquo;s show that this definition is not outrageous by showing that it agrees with our traditional notion of conditional pdf and conditional expectation. So suppose that $X$ and $Y$ are random variables which have a joint probability density function $f_{X,Y}(x,y).$ This means that we are considering the probability space $(\R^2, \B(\R^2), \PP)$ with $X$ and $Y$ being the coordinate random variables, i.e. $(x,y) \mapsto x$ and $(x,y) \mapsto y$ respectively, and having an absolutely continuous distribution function $F_{X,Y}(x,y)$ such that&lt;/p&gt;
&lt;p&gt;$$
F_{X,Y}(x,y) = \int_{-\infty}^y \int_{-\infty}^x f_{X,Y}(s,t) \; \dd s \, \dd t
$$&lt;/p&gt;
&lt;p&gt;We recall that $f_X(x) = \int_\R f_{X,Y}(x,y) \; \dd y$ and $f_Y(y) = \int_\R f_{X,Y}(x,y) \; \dd x$ act as probability density functions for $X$ and $Y$ respectively, and&lt;/p&gt;
&lt;p&gt;$$
f_{X \mid Y}(x \mid y) = \begin{cases}
\frac{f_{X,Y}(x,y)}{f_Y(y)} &amp;amp; \text{ if } f_Y(y) \neq 0   \\&lt;br&gt;
0 &amp;amp; \text{ otherwise}
\end{cases}
$$
defines the elementary conditional pdf $f_{X \mid Y}$ of $X$ given $Y$. By Fubini&amp;rsquo;s theorem $f_X$ and $f_Y$ are Borel functions on $\R$ and so $f_{X \mid Y}$ is a Borel function on $\R^2$. Let $\cH = \B(\R^2) = \sigma(X, Y)$ and $\G = \sigma(Y) = \R \times \B(\R)$. For $A \in \cH$ and $\omega = (x,y) \in \R^2$ we define&lt;/p&gt;
&lt;p&gt;$$
\PP(A, \omega) = \int_{\set{s}{(s,y) \in A}} f_{X \mid Y}(s \mid y) \; \dd s
$$&lt;/p&gt;
&lt;p&gt;Then for each $\omega \in \R^2$, $\PP(\cdot, \omega)$ is a probability measure on $\cH$, and for each $A \in \cH$, $\PP(A, \cdot)$ is a Borel function in $y$ and hence $\G$-measurable. To verify that $\PP(A, \cdot) = \Pc{A}{\G}$ for any $A \in \cH$ we just need to verify property 3. of Theorem 1. To this end, fix $A \in \cH$  and $G \in \G$, and note that $G$ must be of the form $G = \R \times B$ for $B \in \B(\R)$. Thus&lt;/p&gt;
&lt;p&gt;\begin{align*}
\int_G \PP(A, \omega) \, \dP(\omega) &amp;amp;= \int_B \int_R \PP(A, (s,t)) f_{X \mid Y}(s,t) \; \dd s \, \dd t \quad \text{(by absolute continuity and Fubini&amp;rsquo;s theorem)} \\&lt;br&gt;
&amp;amp;= \int_B \int_R \left[ \int_{\set{u}{(u,t) \in A}} f_{X \mid Y}(u \mid t) \; \dd u \right] f_{X \mid Y}(s,t) \; \dd s \, \dd t \\&lt;br&gt;
&amp;amp;= \int_B \left[ \int_{\set{u}{(u,t) \in A}} f_{X \mid Y}(u \mid t) \; \dd u \right] f_{Y}(t) \; \dd t \\&lt;br&gt;
&amp;amp;= \int_B \int_{\set{u}{(u,t) \in A}} f_{X, Y}(u, t) \; \dd u \, \dd t \\&lt;br&gt;
&amp;amp;= \int_B \int_{\R} \ind{A}(u,t) f_{X, Y}(u, t) \; \dd u \, \dd t \\&lt;br&gt;
&amp;amp;= \int_{G} \ind{A}(\omega) \; \dP(\omega)
\end{align*}&lt;/p&gt;
&lt;p&gt;and so $\PP(A, \cdot) = \Exc{\ind{A}}{\G} = \Pc{A}{\G}$. Hence, $\PP(A, \omega)$ is a regular probability measure on $\cH$ given $\G$.&lt;/p&gt;
&lt;p&gt;For the corresponding analysis for conditional expectation, let $h$ be a Borel function on $\R^2$ such that $\Ex{|h(X,Y)|} = \int_\R \int_\R |h(x,y)| f_{X,Y}(x,y) \; \dd x \, \dd y &amp;lt; \infty$. Set&lt;/p&gt;
&lt;p&gt;$$
g(y) = \int_\R h(s, y) f_{X \mid Y}(s \mid y) \; \dd s
$$&lt;/p&gt;
&lt;p&gt;$g(y)$ is the traditional conditional density of $h(X, Y)$ given $Y = y$.
Then the claim is that $g(Y) = \Exc{h(X,Y)}{\sigma(Y)}$ a.s.. The typical element of $\sigma(Y)$ has the form $\set{\omega \in \R^2}{Y(\omega) \in B}$, where $B \in \B(\R).$ Hence, we must show that&lt;/p&gt;
&lt;p&gt;$$
L = \Ex{h(X,Y) \ind{B}(Y)} = \Ex{g(Y) \ind{B}(Y)} = R
$$&lt;/p&gt;
&lt;p&gt;But we can write $L$ and $R$ as&lt;/p&gt;
&lt;p&gt;\begin{align*}
L &amp;amp;= \int \int h(x,y) \ind{B}(y) f_{X,Y}(x,y) \; \dd x \, \dd y \\&lt;br&gt;
R &amp;amp;= \int g(y) \ind{B}(y) f_Y(y) \; \dd y
\end{align*}&lt;/p&gt;
&lt;p&gt;and they are equal by Fubini&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;In general, we have the following useful theorem (taken from [2]) which allows us to view conditional expectations as ordinary expectations relative to the measure induced by regular conditional probability.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 2: &lt;/span&gt; Consider the setting of Definition 3 and denote $\PP_\omega(\cdot) = \PP(\cdot, \omega)$. Let $X$ be an $\cH$-measurable function with $\Ex{X} &amp;lt; \infty$. Then&lt;/p&gt;
&lt;p&gt;$$
\Exc{X}{\G}(\omega) = \int_\Omega X \; \dP_\omega \quad \text{a.s.} \tag{3} \label{eq3}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Recall the monotone class theorem for functions:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; Let $\mathscr{H}$ be a family of nonnegative functions on $\Omega$ which contains all indicators of sets of some class $\cH$ of subsets of $\Omega$. If either (i) $\cH$ is a $\pi$-class and $\mathscr{H}$ is a $\lambda$-system, or (ii) $\cH$ is a $\sigma$-algebra and $\mathscr{H}$ is a monotone system, then $\mathscr{H}$ contains all nonnegative $\sigma(\cH)$-measurable functions.&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;By separate considerations of $X^+$ and $X^-$, it may be supposed that $X \ge 0$. Let&lt;/p&gt;
&lt;p&gt;$$
\mathscr{H} = \set{X}{X \ge 0, X \text{ is } \cH \text{-measurable, and \eqref{eq3} holds for } X}
$$&lt;/p&gt;
&lt;p&gt;By the definition of regular conditional probability, $\ind{A} \in \mathscr{H}$ for $A \in \cH$. $\cH$ is a $\sigma$-algebra. Let&amp;rsquo;s show that $\mathscr{H}$ is a monotone system. If $X_1, X_2 \in \mathscr{H}$ and $c_1, c_2 \ge 0$, then $c_1 X_1 + c_2 X_2 \ge 0$, $c_1 X_1 + c_2 X_2$ is $\cH$-measurable and Equation \eqref{eq3} holds because of linearity of expectation and conditional expectation, and thus $c_1 X_1 + c_2 X_2 \in \mathscr{H}$. If $\se{X_n} \subseteq \mathscr{H}$ such that $X_n \uparrow X$, then $X \ge 0$, $X$ is $\cH$-measurable, and Equation \eqref{eq3} holds for $X$ because of monotone convergence theorem for expectation and conditional expectation, and thus $X \in \mathscr{H}$. Therefore, by the monotone class theorem $\mathscr{H}$ contains all nonnegative $\cH$-measurable functions. $\square$&lt;/p&gt;
&lt;h1 id=&#34;regular-conditional-distribution&#34;&gt;Regular conditional distribution&lt;/h1&gt;
&lt;p&gt;In some cases even the concept of regular conditional probability in inadequate, and that motivates the concept of regular conditional distributions.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 4: &lt;/span&gt; Let $\probsp$ be a probability space, $\G \subseteq \F$ a $\sigma$-algebra, $(\Lambda, \LL)$ a measurable space, and $T \colon \Omega \to \Lambda$ a measurable mapping. A &lt;em&gt;regular conditional distribution&lt;/em&gt; for $T$ given $\G$ is a function $\PP_T \colon \LL \times \Omega \to [0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;for a.e. $\omega \in \Omega$, $\PP_T(\cdot, \omega)$ is a probability measure on $\LL$,&lt;/li&gt;
&lt;li&gt;for each $A \in \LL$, $\PP_T(A, \cdot)$ is a $\G$-measurable function on $\Omega$ coinciding with the conditional probability of $T^{-1}(A)$ given $\G$, i.e., $\PP_T(A, \cdot) = \Pc{T^{-1}(A)}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is clear that when $\Lambda = \Omega$, $\LL = \cH \subseteq \F$ and $T$ is the identity map, $\PP_T$ is exactly the regular conditional probability as defined in Definition 3.&lt;/p&gt;
&lt;p&gt;Now would be a good time to reread the first &amp;ldquo;rumination&amp;rdquo; in the section Conditional Probability and realize that the definition of regular conditional distribution is in fact well motivated.&lt;/p&gt;
&lt;p&gt;A corresponding version of Theorem 2 exists, proof of which I&amp;rsquo;ll leave as an easy exercise:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 3: &lt;/span&gt; In the setting of Definition 4, if $\PP_T^\omega(A) = \PP_T(A, \omega)$ and $h \colon \Lambda \to \R$ is a Borel function with $\Ex{|h(T)|} &amp;lt; \infty$, then&lt;/p&gt;
&lt;p&gt;$$
\Exc{h(T)}{\G}(\omega) = \int_\Lambda h(\lambda) \; \PP_T^\omega(\dd \lambda)
$$&lt;/p&gt;
&lt;p&gt;To see the power of thinking about conditional probabilities like this, let&amp;rsquo;s give an unbelievably short proof of conditional HÃ¶lder&amp;rsquo;s inequality [2]. Contrast it with 
&lt;a href=&#34;https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality#Conditional_H%C3%B6lder_inequality&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;other proofs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 4: &lt;/span&gt; If $X,Y$ are random variables on $\probsp$, $\G \subseteq \F$ is a $\sigma-$algebra and $ 1 &amp;lt; p &amp;lt; \infty$, $1/p + 1/q = 1$, then&lt;/p&gt;
&lt;p&gt;$$
\Exc{|XY|}{\G} \le \left( \Exc{|X|^p}{\G} \right)^{1/p} \left( \Exc{|Y|^q}{\G} \right)^{1/q} \quad \text{a.s.}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; For $B \in \B(\R^2)$ and $\omega \in \Omega$, let $\PP_{X,Y}^\omega(B) = \PP_{X,Y}(B, \omega)$ be the regular conditional distribution for $(X,Y)$ given $\G$. Theorem 3 allows us to write&lt;/p&gt;
&lt;p&gt;\begin{align*}
\Exc{|XY|}{\G}(\omega) &amp;amp;= \int_{\R^2} |x y| \; \PP_{X,Y}^\omega(\dd (x,y)) \\&lt;br&gt;
\left( \Exc{|X|^p}{\G} \right)^{1/p} &amp;amp;= \left( \int_{\R^2} |x|^p \; \PP_{X,Y}^\omega(\dd (x,y)) \right)^{1/p} \\&lt;br&gt;
\left( \Exc{|Y|^q}{\G} \right)^{1/q} &amp;amp;= \left( \int_{\R^2} |y|^q \; \PP_{X,Y}^\omega(\dd (x,y)) \right)^{1/q}
\end{align*}&lt;/p&gt;
&lt;p&gt;And now our desired inequality follows immediately from the ordinary HÃ¶lder&amp;rsquo;s inequality. $\square$&lt;/p&gt;
&lt;h1 id=&#34;existence-of-regular-conditional-distribution&#34;&gt;Existence of regular conditional distribution&lt;/h1&gt;
&lt;p&gt;Before we discuss their existence, let us define the concept of standard Borel space [6].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 5: &lt;/span&gt; Let $(X, \X)$ and $(Y, \Y)$ be measurable spaces. They are called &lt;em&gt;isomorphic&lt;/em&gt; is there exists a bijection $f \colon X \to Y$ such that $f$ and $f^{-1}$ are both measurable. The function $f$ is called an &lt;em&gt;isomorphism&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 6: &lt;/span&gt; A measurable space $(X, \X)$ is called a &lt;em&gt;standard Borel space&lt;/em&gt; if it satisfies any of the following equivalent conditions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$(X, \X)$ is isomorphic to some compact metric space with the Borel $\sigma$-algebra;&lt;/li&gt;
&lt;li&gt;$(X, \X)$ is isomorphic to some Polish space (i.e., a separable complete metric space) with the Borel $\sigma$-algebra;&lt;/li&gt;
&lt;li&gt;$(X, \X)$ is isomorphic to some Borel subset of some Polish space with the Borel $\sigma$-algebra.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As you can guess most spaces we deal with are standard Borel spaces. Durrett [5] calls these space &lt;em&gt;nice&lt;/em&gt; since we already have too many things named after Borel. I am not sure I agree with his reasoning but I like Durrett&amp;rsquo;s terminology.&lt;/p&gt;
&lt;p&gt;The next two theorems show the existence of regular conditional distribution and are taken from [5, Section 4.1.3]. See also [4, Section V.8].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 5: &lt;/span&gt; Regular conditional distribution exists if $(\Lambda, \LL)$ is nice.&lt;/p&gt;
&lt;p&gt;A generalization of the last theorem:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 6: &lt;/span&gt; Suppose $(\Lambda, \LL)$ is a nice space, $T$ and $S$ are measurable mappings from $\Omega$ to $\Lambda$, and $\G = \sigma(S)$. Then there exists a function $\mu \colon \Lambda \times \LL \to [0,1]$ such that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;for a.e. $\omega \in \Omega$, $\mu(S(\omega), \cdot)$ is a probability measure on $\LL$,&lt;/li&gt;
&lt;li&gt;for each $A \in \LL$, $\mu(S(\cdot), A) = \Pc{T^{-1}(A)}{\G}$ a.s.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is instructive to prove Theorem 5 in the special case when $(\Lambda, \LL) = (\R^n, \B(\R^n))$. The theorem and the proof is taken from [2].&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 7: &lt;/span&gt; In the setting of Definition 4, let $(\Lambda, \LL) = (\R^n, \B(\R^n))$ and $T = (T_1, \ldots, T_n) \colon \Omega \to \R^n$. Then there exists a regular conditional distribution for $T$ given $\G$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Let&amp;rsquo;s recall the definition of an $n$-dimensional distribution function on $\R^n$ (the Russian convention of left-continuous distribution function).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; An $n$-dimensional distribution function on $\R^n$ is a function $F$ satisfying:
$$\lim_{x_j \to - \infty} F(x_1, \ldots, x_n) = 0, \quad 1 \le j \le n \tag{i}$$
$$\lim_{\substack{x_j \to \infty \\ 1 \le j \le n}} F(x_1, \ldots, x_n) = 1 \tag{ii}$$
$$\lim_{y_j \uparrow x_j} F(x_1, \ldots, x_{j-1}, y_j, x_{j+1}, \ldots, x_n) = F(x_1, \ldots, x_j, \ldots, x_n), \quad 1 \le j \le n \tag{iii}$$
\begin{align*}\Delta_n^{a,b} &amp;amp;:= F(b_1, \ldots, b_n) - \sum_{j=1}^n F(b_1, \ldots, b_{j-1}, a_j, b_{j+1}, \ldots, b_n) \\ &amp;amp;+ \sum_{1 \le j &amp;lt; k \le n} F(b_1, \ldots, b_{j-1}, a_j, b_{j+1}, \ldots, b_{k-1}, a_k, b_{k+1}, \ldots, b_n) - \cdots (-1)^n F(a_1, \ldots, a_n) \\ &amp;amp;\ge 0 \tag{iv}\end{align*}
&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We will try to construct a distribution function on $\R^n$. To this end, for any rational number $r_1, \ldots, r_n$ and $\omega \in \Omega$, define&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(r_1, \ldots, r_n) = \Pc{\bigcap_{i=1}^n \se{T_i &amp;lt; r_i}}{\G}(\omega) \tag{4} \label{eq4}
$$&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s evident that the properties of conditional probability discussed above imply there is a null set $N \in \G$ such that for $\omega \in N^\comp$ and all rational numbers $r_i, r_i&amp;rsquo;, q_{i,m}$ the following holds&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(r_1, \ldots, r_n) \ge F_n^\omega(r_1&amp;rsquo;, \ldots, r_n&amp;rsquo;) \text{ if } r_i &amp;gt; r_i&amp;rsquo;, \, 1 \le i \le n
$$&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(r_1, \ldots, r_n) = \lim_{\substack{q_{i,m} \uparrow r_i \\ 1 \le i \le n}} F_n^\omega(q_{1, m}, \ldots, q_{n, m})
$$&lt;/p&gt;
&lt;p&gt;$$
\lim_{r_i \to -\infty} F_n^\omega(r_1, \ldots, r_n) = 0, \quad 1 \le i \le n
$$&lt;/p&gt;
&lt;p&gt;$$
\lim_{\substack{r_i \to \infty \\ 1 \le i \le n}} F_n^\omega(r_1, \ldots, r_n) = 1
$$&lt;/p&gt;
&lt;p&gt;$$
\Delta_n^{r, r&amp;rsquo;} F_n^\omega \ge 0 \text{ if } r \le r&amp;rsquo;
$$&lt;/p&gt;
&lt;p&gt;where $r \le r&#39;$ means $r_i \le r_i&#39;$ for all $1 \le i \le n$. Having defined $F_n^\omega$ for rational values, define for any real numbers $x_1, \ldots, x_n$ as follows&lt;/p&gt;
&lt;p&gt;$$
F_n^\omega(x_1, \ldots, x_n) = \begin{cases}
\lim_{\substack{r_i \uparrow x_i \\ r_i \in \Q \\ 1 \le i \le n}} F_n^\omega(r_1, \ldots, r_n) &amp;amp; \text{ if } \omega \in N^\comp   \\&lt;br&gt;
\Prob{\bigcap_{i=1}^n \se{T_i &amp;lt; r_i}} &amp;amp; \text{ if } \omega \in N
\end{cases} \tag{5} \label{eq5}
$$&lt;/p&gt;
&lt;p&gt;Then for each $\omega \in \Omega$, $F_n^\omega(x_1, \ldots, x_n)$ is an $n$-dimensional distribution function and hence determines a Lebesgue-Stieltjes measure $\mu_\omega$ on $\B(\R^n)$ with $\mu_\omega(\R^n) = 1$. For $B \in \B(\R^n)$ and $\omega \in \Omega$ define&lt;/p&gt;
&lt;p&gt;$$
\PP_T(B, \omega) = \mu_\omega(B)
$$&lt;/p&gt;
&lt;p&gt;If&lt;/p&gt;
&lt;p&gt;\begin{align*}
\cH &amp;amp;= \set{B \in \B(\R^n)}{\PP_T(B, \cdot) = \Pc{T^{-1}(B)}{\G} \text{ a.s.} } \\&lt;br&gt;
\D &amp;amp;= \set{B \in \B(\R^n)}{B = [-\infty, r_1) \times \cdots \times [-\infty, r_n), \, r_i \in \Q}
\end{align*}&lt;/p&gt;
&lt;p&gt;then a moment&amp;rsquo;s reflection will convince you that that $\cH$ is a $\lambda-$class, $\D$ is a $\pi-$class, and $\cH \supseteq \D$. Hence, by the $\pi-\lambda$ theorem $\cH \supseteq \sigma(\D) = \B(\R^n)$, or in other words, $\PP_T(B, \omega)$ is a regular conditional distribution for $T$ given $\G$. $\square$&lt;/p&gt;
&lt;p&gt;In fact, this theorem is easily extended to $(\R^\infty, \B(\R^\infty))$ as follows: For all $n \ge 1$, define $F_n^\omega$ as in Equation \eqref{eq4}. Select the null set $N \in \G$ such that in addition to the conditions it satisfies above we also have the consistency condition&lt;/p&gt;
&lt;p&gt;$$
\lim_{r_{n+1} \to \infty} F_n^\omega(r_1, \ldots, r_n, r_{n+1}) = F_n^\omega(r_1, \ldots, r_n), \quad n \ge 1
$$&lt;/p&gt;
&lt;p&gt;For reals $x_1, \ldots, x_n$ define just like Equation \eqref{eq5}. Then for each $\omega \in \Omega$, $\se{F_n^\omega, \, n \ge 1}$ is a consistent family of distribution functions, and hence by the Kolmogorov extension theorem there exists a unique measure $\mu_\omega$ on $(\R^\infty, \B(\R^\infty))$ whose finite dimensional distributions are $\se{F_n^\omega, \, n \ge 1}$. Define $\PP_T(B, \omega) = \mu_\omega(B)$ for $B \in \B(\R^\infty)$. If&lt;/p&gt;
&lt;p&gt;\begin{align*}
\cH &amp;amp;= \set{B \in \B(\R^\infty)}{\PP_T(B, \cdot) = \Pc{T^{-1}(B)}{\G} \text{ a.s.} } \\&lt;br&gt;
\D &amp;amp;= \bigcup_{n=1}^\infty \set{B \in \B(\R^\infty)}{B = [-\infty, r_1) \times \cdots \times [-\infty, r_n) \times \R \times \R \times \cdots, \, r_i \in \Q}
\end{align*}&lt;/p&gt;
&lt;p&gt;then $\cH$ is a $\lambda-$class, $\D$ is a $\pi-$class, and $\cH \supseteq \D$. Hence, by the $\pi-\lambda$ theorem $\cH \supseteq \sigma(\D) = \B(\R^\infty)$.&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Williams, David. &lt;em&gt;Probability with Martingales&lt;/em&gt;. Cambridge mathematical textbooks, Cambridge University Press, 1991.&lt;/li&gt;
&lt;li&gt;Chow, Yuan Shih and Teicher, Henry. &lt;em&gt;Probability Theory: Independence, Interchangeability, Martingales&lt;/em&gt;, 3rd edn. Springer-Verlag New York, 1997.&lt;/li&gt;
&lt;li&gt;Halmos, P. R.. &lt;em&gt;Measure Theory&lt;/em&gt;. Van Nostrand, Princeton, N. J., 1950; Springer-Verlag, Berlin and New York, 1974.&lt;/li&gt;
&lt;li&gt;Parthasarathy, K. R.. &lt;em&gt;Probability Measures on Metric Spaces&lt;/em&gt;. AMS Chelsea Publishing, 1967.&lt;/li&gt;
&lt;li&gt;Durrett, R.: &lt;em&gt;Probability: Theory and Examples&lt;/em&gt;, 5th edn. Cambridge University Press, 2019.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://encyclopediaofmath.org/wiki/Standard_Borel_space&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://encyclopediaofmath.org/wiki/Standard_Borel_space&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Longest Increasing Subsequence</title>
      <link>https://makkar.github.io/post/lisprob/</link>
      <pubDate>Fri, 25 Dec 2020 17:17:08 -0500</pubDate>
      <guid>https://makkar.github.io/post/lisprob/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\ZZ}{\Z_{\geq 0}^N}
\newcommand{\N}{\mathbb{N}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\Prob}[1]{\PP \left( #1 \right)}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\ex}[1]{\exp\left{#1\right}}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\ceil}[1]{\left \lceil{#1}\right \rceil}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\newcommand{\set}[2]{\left\{ #1 ; : ; #2 \right\}}
\newcommand{\sett}[2]{\left\{ #1 ; | ; #2 \right\}}
\newcommand{\Ex}[1]{\E\left[#1\right]}
\newcommand{\pard}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\ph}[1]{\varphi^{-1}\left(#1\right)}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\du}{du}
\DeclareMathOperator{\dz}{d\matr{z}}
\DeclareMathOperator{\dt}{dt}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dP}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathrm{Var}}
\DeclareMathOperator{\Log}{\mathrm{Log}}
\DeclareMathOperator{\O}{\mathcal{O}}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Finding a longest increasing subsequence is a well-known problem in computer science (note that I use the article &amp;ldquo;a&amp;rdquo; instead of &amp;ldquo;the&amp;rdquo; because there could be multiple longest subsequences): Given a sequence $\se{a_1, \ldots, a_n}$ of real numbers, we want to find a subsequence $\se{a_{i_1}, \ldots, a_{i_k}}$ such that $0 \le i_1 &amp;lt; \cdots &amp;lt; i_k \le n$, $a_{i_1} \le \cdots \le a_{i_k}$, and the subsequence is as long as possible. It has a very easy 
&lt;a href=&#34;https://www.geeksforgeeks.org/longest-increasing-subsequence-dp-3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dynamic programming&lt;/a&gt; solution with a time complexity of $\O(n^2)$ and a slightly 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Longest_increasing_subsequence#Efficient_algorithms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;more involved solution&lt;/a&gt; with a time complexity of $\O(n \log n)$. But we are not interested in these algorithms in this blog post.&lt;/p&gt;
&lt;p&gt;We are interested in studying the asymptotics of the &lt;em&gt;length&lt;/em&gt; of the longest increasing subsequences of a sequence whose elements are coming from a random permutation. This simple to state problem will take us on a beautiful journey touching topics from combinatorics and probability theory. In particular, we will get to see the very elegant technique of Poissonization.&lt;/p&gt;
&lt;h1 id=&#34;problem&#34;&gt;Problem&lt;/h1&gt;
&lt;p&gt;Let us start by stating precisely what we are trying to prove. To do that we first define some notation. For any integer $n \ge 1$, let $S_n$ be the group of permutations of order $n$, i.e., it contains all permutations of $\se{1, 2, \ldots, n}$ and hence $S_n$ contains $n!$ elements.  If $\pi \in S_n$ then a subsequence of $\pi$ is a sequence $\se{\pi(i_1), \ldots, \pi(i_k)}$ such that $1 \leq i_1 &amp;lt; \cdots &amp;lt; i_k \leq n$. It is an increasing subsequence if $\pi(i_1) &amp;lt; \cdots &amp;lt; \pi(i_k)$ and similarly for the decreasing subsequence. Consider the uniform measure $\mu_n$ on the discrete measurable space $(S_n, 2^{S_n})$, i.e., $\mu_n(\pi) = 1 / n!$ for any $\pi \in S_n$. For $\pi \in S_n$ define $l_n(\pi)$ to be the maximal length of an increasing subsequence of $\pi$, i.e., $l_n(\pi)$ is the largest $k$ such that there are integers $1 \leq i_1 &amp;lt; \cdots &amp;lt; i_k \leq n$ so that $\pi(i_1) &amp;lt; \cdots &amp;lt; \pi(i_k)$. Similarly define $d_n(\pi)$ to be the maximal length of a decreasing subsequence of $\pi$.&lt;/p&gt;
&lt;p&gt;Since $l_n$ is a random variable we can consider its expectation $L_n = \E[l_n]$ on the probability space $(S_n, 2^{S_n}, \mu_n)$. It can be explicitly written as
$$
L_n = \frac{1}{n!} \sum_{\pi \in S_n} l_n(\pi) \tag{1} \label{eq1}
$$
We want to study the limiting properties of the sequence $\se{L_n}_{n \in \N}$. Specifically we will show that
$$
\frac{L_n}{\sqrt{n}} \to \gamma \text{ almost surely} \tag{2} \label{eq2}
$$
for some constant $\gamma$. It is known that $\gamma = 2$. We will not be showing this, but we will show that $1 \le \gamma \le e$.&lt;/p&gt;
&lt;h1 id=&#34;combinatorial-results&#34;&gt;Combinatorial results&lt;/h1&gt;
&lt;p&gt;We now prove some useful combinatorial results. The first result is called the ErdÅ‘sâ€“Szekeres theorem.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 1 [ErdÅ‘sâ€“Szekeres theorem]: &lt;/span&gt; In any sequence $\se{a_1, a_2, \ldots, a_{mn+1}}$ of $mn+1$ distinct real numbers, there exists either an increasing subsequence $a_{i_1} &amp;lt; \cdots &amp;lt; a_{i_{m+1}}$ $(i_1 &amp;lt; \cdots &amp;lt; i_{m+1})$ of length $m+1$, or a decreasing subsequence $a_{j_1} &amp;gt; \cdots &amp;gt; a_{j_{n+1}}$ $(j_1 &amp;lt; \cdots &amp;lt; j_{n+1})$ of length $n+1$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; For $1 \leq i \leq mn+1$ define $t_i$ to be the length of a longest increasing subsequence starting at $a_i$, i.e., the first element of a longest increasing subsequence must be $a_i$ and the rest of the $a$&#39;s must have indices greater than $i$. If $t_i \geq m+1$ for some $i$ then we are done since we then have an increasing subsequence of length $m+1$, so assume $t_i \leq m$ for all $i$. Since $t_i \geq 1$, pigeonhole principle now implies that there is some integer $1 \leq k \leq m$ such that $t_i = k$ for at least $n+1$ $i$&#39;s. Let $t_i = k$ for all $i \in (j_1 &amp;lt; \cdots &amp;lt; j_{n+1})$. Now note that if $a_{j_l} &amp;lt; a_{j_{l+1}}$ for some $1 \leq l \leq n$, then we would obtain an increasing subsequence of length $k+1$ starting at $a_{j_l}$ because there is an increasing subsequence of length $k$ starting at $a_{j_{l+1}}$. But this contradicts the fact that $t_{j_l} = k,$ and thus $a_{j_l} &amp;gt; a_{j_{l+1}}$ for all $1 \leq l \leq n$. But now this gives us a decreasing subsequence $a_{j_1} &amp;gt; \cdots &amp;gt; a_{j_{n+1}}$ of length $n+1$. $\square$&lt;/p&gt;
&lt;p&gt;The next two theorems prove lower and upper bounds for $L_n / \sqrt{n}$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 2: &lt;/span&gt; $L_n / \sqrt{n}$ is lower bounded as follows
$$
L_n \geq \sqrt{n} \text{ for all } n \geq 1 \tag{3} \label{eq3}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt;
Theorem 1 implies $l_n(\pi) d_n(\pi) \geq n$ for all $\pi \in S_n$. Note that for every permutation $\pi \in S_n$ there exists an inverse permutation $\pi&#39;$ such that $l_n(\pi) = d_n(\pi&amp;rsquo;)$ and thus we can write $L_n$ also as
$$
L_n = \frac{1}{n!} \sum_{\pi \in S_n} d_n(\pi) \tag{4} \label{eq4}
$$
Therefore, averaging the two ways of computing the expectation $L_n$ (Equations \eqref{eq1} and \eqref{eq4}) and using the AM-GM inequality we have
$$
L_n = \frac{1}{n!} \sum_{\pi \in S_n} \frac{l(\pi) + d(\pi)}{2} \geq \frac{1}{n!} \sum_{\pi \in S_n} \sqrt{l(\pi) d(\pi)} \geq \frac{1}{n!} \sum_{\pi \in S_n} \sqrt{n} = \sqrt{n} \qquad \square
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 3: &lt;/span&gt; Upper bound:
$$
\lims \frac{L_n}{\sqrt{n}} \leq e \tag{5} \label{eq5}
$$&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt;
If $\pi \in S_n$ and $1 \leq k \leq n$ let $X_{n,k}(\pi)$ be the number of increasing subsequences of $\pi$ which are of length $k$. They are exactly those subsets $S \subseteq \se{1, \ldots, n}$ such that $|S| = k$ and if $S = \se{i_1 &amp;lt; \cdots &amp;lt; i_k}$ then $\pi(i_1) &amp;lt; \cdots &amp;lt; \pi(i_k)$. On the probability space $(S_n, 2^{S_n}, \mu_n)$, by the linearity of expectation, the expected value of $X_{n,k}$ is given by the number of ways to select $k$ subsets of $\se{1, \ldots, n}$ times the probability that a selection has all the elements in the increasing order. The number of ways is simply $\binom{n}{k}$ and the probability is $1/k!$ and thus
$$
\E[X_{n,k}] = \frac{1}{k!} \binom{n}{k}
$$
The Taylor expansion of $e^x$ implies $e^x \ge x^k / k!$. Substituting $x = k$ we get $k! \geq (k/e)^k$. Therefore, we get
$$
\E[X_{n,k}] = \frac{1}{k!} \binom{n}{k} = \frac{n(n-1) \cdots (n-k-1)}{(k!)^2} \leq \frac{n^k}{(k/e)^{2k}}
$$&lt;/p&gt;
&lt;p&gt;Now for a discrete random variable $X_{n,k}$ we can write $\E[X_{n,k}] = \sum_{i=0}^n \mu_n(X_{n,k} \geq i)$ and thus $\mu_n(X_{n,k} \geq 1) \leq \Ex{X_{n,k}}$. Also note that $l_n(\pi) \geq k$ if and only if $X_{n,k}(\pi) \geq 1$. We thus get
$$
\mu_n(l_n \geq k) = \mu_n(X_{n,k} \geq 1) \leq \E[X_{n,k}] \leq \frac{n^k}{(k/e)^{2k}}
$$
Fix an arbitrary $\delta &amp;gt; 0$ and let $k = \ceil{(1+\delta)e\sqrt{n}}$ in the inequality above to get
$$
\mu_n(l_n \geq k) \leq \frac{n^k}{(k/e)^{2k}} \leq \left( \frac{1}{1+\delta} \right)^{2k} \leq \left( \frac{1}{1+\delta} \right)^{2(1+\delta)e\sqrt{n}}
$$
Since $l_n \leq n$, we have
$$
L_n = \Ex{l_n} \leq \mu_n(l_n &amp;lt; k) (1+\delta)e\sqrt{n} + \mu_n(l_n \geq k) n \leq (1+\delta)e\sqrt{n} + O(e^{-c\sqrt{n}})
$$
where $c$ is some positive constant that depends on $\delta$.
Since $\delta$ was arbitrary, we can let $\delta \to 0$ and then take $\limsup$ to get Equation \eqref{eq5}. $\square$&lt;/p&gt;
&lt;h1 id=&#34;poissonization&#34;&gt;Poissonization&lt;/h1&gt;
&lt;p&gt;To be able to show \eqref{eq2} we will draw a correspondence between this problem of longest increasing subsequences and a seemingly unrelated problem called the Poissonized version. This Poissonized version will allow us to use the powerful Subadditive Ergodic Theorem (Theorem 5 below) to show \eqref{eq2}.&lt;/p&gt;
&lt;p&gt;To this end, assume an underlying probability space $\probsp$ and let $N$ be a Poisson random measure on $\mathbb{R} _ + ^2$ with mean measure given by the Lebesgue measure on $\mathbb{R} _ + ^2$. In other words $N:\Omega \times \B(\R _ + ^2) \to \bar{\R} _ +$ is a transition kernel from $(\Omega, \F)$ into $(\R_+^2, \B(\R_+^2))$ with $\int_\Omega \PP(\dd \omega) N(\omega, A) = \text{Leb}(A)$ for any $A \in \B(\R_+^2)$. We can view the random process as a sequence $\se{(X_i, Y_i)} _ {i \geq 1}$ of independent random variables taking values in $\R _ + ^2$ and having a uniform probability measure (more correctly, Lebesgue measure on $(\R_+^2, \B(\R_+^2))$). If we let $R_{s,t}$ denote the rectangle with vertices $(s,s), (s,t), (t,t)$ and $(t,s)$, then for each outcome $\omega \in \Omega$, we can think of having $\text{Poisson}(\text{Leb}(R_{s,t}(\omega)))$ distributed number of such points inside the rectangle $R_{s,t}(\omega).$&lt;/p&gt;
&lt;p&gt;For $s &amp;lt; t \in [0, \infty)$ let $Z_{s,t}$ be the random variable denoting the length of the longest increasing path lying in the rectangle $R_{s,t}$, i.e., $Z_{s,t}$ is the largest integer $k$ for which there are points $(X_1,Y_1), \dots, (X_k, Y_k)$ in the Poisson process with $s &amp;lt; X_1 &amp;lt; \cdots &amp;lt; X_k &amp;lt; t$ and $s&amp;lt; Y_1 &amp;lt; \cdots &amp;lt; Y_k &amp;lt; t$.&lt;/p&gt;
&lt;p&gt;Let $\tau(n)$ be the smallest value of $t \in [0, \infty)$ for which there are $n$ points in $R_{0,t}$. Let the $n$ points in $R_{0, \tau(n)}$ be written as $\se{(X_i, Y_i)}_{1 \leq i \leq n}$ such that $0 &amp;lt; X_1 &amp;lt; \cdots &amp;lt; X_n \leq \tau(n)$ (the inequalities are strict almost surely since they have continuous distributions). Let $\pi_n \in S_n$ be the unique permutation such that $Y_{\pi_n(1)} &amp;lt; \cdots &amp;lt; Y_{\pi_{n}(n)}$ (It is not difficult to see the existence and the uniqueness). Then
$$
\pi_n \text{ is a uniformly random sample of } S_n \text{ , and } Z_{0, \tau(n)} = l_n(\pi_n) \tag{6} \label{eq6}
$$
The second claim is obvious from the definition of $\pi_n$. The first claim is equivalent to showing that if $U_1, \ldots, U_n$ are independent random variables sampled from a uniform distribution on $[0,1]$ and if $U_{(1)}, \ldots, U_{(n)}$ are the order statistics, i.e., $U_{(k)}$ is the $k$&lt;sup&gt;th&lt;/sup&gt; smallest among $U_1, \ldots, U_n$, then the probability that $U_{(1)}, \ldots, U_{(n)}$ is same as $U_{\pi(1)}, \ldots, U_{\pi(n)}$ for any $\pi \in S_n$ is $1/n!$. But this is obvious from the independence of $U_i$&#39;s. The next theorem is from Durrett:&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 4: &lt;/span&gt; $\tau(n) / \sqrt{n} \to 1$ almost surely.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt;
Let $S_n$ be the number of points in $R_{0,\sqrt{n}}$. Since $(R_{0, \sqrt{n}} \setminus R_{0, \sqrt{n-1}}) \cap (R_{0, \sqrt{m}} \setminus R_{0, \sqrt{m-1}}) = \emp$ for $n \neq m$, the definition of a Poisson random measure implies $\se{S_n - S_{n-1}}_{n \geq 1}$ are independent Poisson random variables with mean $1$. The strong law of large numbers now implies $S_n / n \to 1$ almost surely. For any $\e &amp;gt; 0$ we can find an $n$ large enough such that $S_{n(1-\e)} &amp;lt; n &amp;lt; S_{n(1+\e)}$ but then this means $\sqrt{n(1-\e)} \leq \tau(n) \leq \sqrt{n(1+\e)}$ which is same as the statement of the theorem since $\e$ was arbitrary. $\square$&lt;/p&gt;
&lt;p&gt;The last theorem along with \eqref{eq6} implies $Z_{0, \sqrt{n}} \to L_n$ almost surely and thus
$$
\frac{Z_{0,n}}{n} \to \frac{L_{n^2}}{n} \text{ almost surely} \tag{7} \label{eq7}
$$&lt;/p&gt;
&lt;h1 id=&#34;back-to-longest-increasing-subsequences&#34;&gt;Back to longest increasing subsequences&lt;/h1&gt;
&lt;p&gt;Having established the connection between the two ways of looking at the problem of longest increasing subsequences, we now freely jump between the two characterizations and use them to prove our results.&lt;/p&gt;
&lt;p&gt;Define $W_{s,t} = - Z_{s,t}$. We now check that $W_{m,n}, 0 \le m &amp;lt; n$ satisfies the conditions required for the Subadditive Ergodic Theorem. I state the theorem below for completeness. Check out Theorem 6.4.1 in 
&lt;a href=&#34;https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Durrett&lt;/a&gt; for a proof.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 5 [Subadditive Ergodic Theorem]: &lt;/span&gt; Suppose $W_{m,n}, 0 \le m &amp;lt; n$ satisfy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$W_{0,m} + W_{m,n} \ge W_{0,n}$,&lt;/li&gt;
&lt;li&gt;$\se{W_{nk, (n+1)k}, n \ge 1}$ is a stationary sequence for each $k$,&lt;/li&gt;
&lt;li&gt;The distribution of $\se{W_{m,m+k}, k \ge 1}$ does not depend on $m$,&lt;/li&gt;
&lt;li&gt;$\Ex{W_{0,1}^+} &amp;lt; \infty$ and $\inf_{n \ge 1} \frac{1}{n} \Ex{W_{0,1}} = \beta &amp;gt; - \infty$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\limn \frac{1}{n} \Ex{W_{0,1}} = \beta$,&lt;/li&gt;
&lt;li&gt;$W = \limn \frac{1}{n} W_{0,1}$ exists almost surely and in $L^1$, and $\Ex{X} = \beta$,&lt;/li&gt;
&lt;li&gt;If all stationary sequences in 2. are ergodic, then $W = \beta$ almost surely.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Coming back to the problem, let $0 &amp;lt; m &amp;lt; n$ then we claim $Z_{0,m} + Z_{m,n} \leq Z_{0,n}$. To see this fix $\omega \in \Omega$ and let $Z_{0,m}(\omega) = a$ and $Z_{m,n}(\omega) = b$. Then there exist $(X_1(\omega), Y_1(\omega)), \ldots (X_a(\omega), Y_a(\omega)), (X_{a+1}(\omega), Y_{a+1}(\omega)), \ldots, (X_{a+b}(\omega), Y_{a+b}(\omega))$ such that $0 &amp;lt; X_1(\omega) &amp;lt; \cdots &amp;lt; X_a(\omega) &amp;lt; m$, $0 &amp;lt; Y_1(\omega) &amp;lt; \cdots &amp;lt; Y_a(\omega) &amp;lt; m$ and $m &amp;lt; X_{a+1}(\omega) &amp;lt; \cdots &amp;lt; X_{a+b}(\omega) &amp;lt; n$, $m &amp;lt; Y_{a+1}(\omega) &amp;lt; \cdots &amp;lt; Y_{a+b}(\omega) &amp;lt; n$. But then it&amp;rsquo;s clear that $0 &amp;lt; X_1(\omega) &amp;lt; \cdots &amp;lt; X_{a+b}(\omega) &amp;lt; n$, $0 &amp;lt; Y_1(\omega) &amp;lt; \cdots &amp;lt; Y_{a+b}(\omega) &amp;lt; n$ and we have  $Z_{0,m}(\omega) + Z_{m,n}(\omega) \leq Z_{0,n}(\omega)$. Since $\omega$ was arbitrary equation our claim is true. Therefore, $W_{0,m} + W_{m,n} \ge W_{0,n}$ and condition 1. is true.&lt;/p&gt;
&lt;p&gt;For condition 2. we want to show that $\se{W_{nk, (n+1)k, n \geq 1}}$ is a stationary and ergodic sequence for all $k \geq 1$. This is clear from the observation that $Z_{ik, (i+1)k} \stackrel{d}{=} Y_{0,k}$ since $\text{Leb}(R_{ik, (i+1)k}) = \text{Leb}(R_{0,k})$ and thus by the definition of the Poisson random measure the number of points in each rectangle is an i.i.d. Poisson random variable. Checking the condition 3. is similar to condition 2..&lt;/p&gt;
&lt;p&gt;For condition 4. note that $W_{0,1}^+ = Z_{0,1}$ and $Z_{0,1} \leq \text{Poisson}(1)$ since there are $\text{Poisson}(1)$ number of points inside $[0,1]^2$ and at most all of them can be arranged in the increasing order. Thus, $\Ex{W_{0,1}^+} &amp;lt; \Ex{\text{Poisson}(1)} = 1 &amp;lt; \infty$. Now note that since $W_{0,n} = - Z_{0,n}$
$$
\inf_{n \geq 1} \frac{1}{n} \Ex{W_{0,n}} = - \sup_{n \geq 1} \frac{1}{n} \Ex{Z_{0,n}}
$$
and thus to show the second part of condition 4. we need to show that $\sup_{n \geq 1} \frac{1}{n} \Ex{Z_{0,n}} &amp;lt; \infty$. But this is immediate from Equation \eqref{eq5} in Theorem 3 and Equation \eqref{eq7}. Therefore, the subadditive ergodic theorem now implies
$$
\frac{Z_{0,n}}{n} \to \gamma \text{ a.s.}
$$
where $\gamma$ from Equations \eqref{eq3} and \eqref{eq5} lies in $[1,e]$ and we are done since this implies \eqref{eq2}.&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;I got introduced to this problem from an exam question in a math course I took recently. I recommend the book &amp;ldquo;The Surprising Mathematics of Longest Increasing Subsequences&amp;rdquo; by Dan Romik, which is 
&lt;a href=&#34;https://www.math.ucdavis.edu/~romik/book/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;freely available online&lt;/a&gt;, for a lot more content.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>A coin tossing game</title>
      <link>https://makkar.github.io/post/infcoingame/</link>
      <pubDate>Sun, 11 Oct 2020 16:59:05 -0400</pubDate>
      <guid>https://makkar.github.io/post/infcoingame/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\eq}[1]{\begin{align*}#1\end{align*}}
\newcommand{\eql}[1]{\begin{align}#1\end{align}}
\newcommand{\ind}[1]{\mathbf{1}_{#1}}
\newcommand{\indo}[1]{\mathbf{1}_{#1}(\omega)}
\newcommand{\F}{\mathcal{F}}
\newcommand{\probsp}{(\Omega, \F, \PP)}
\newcommand{\integ}[1]{\int_{\Omega} #1 \dmu}
\newcommand{\B}{\mathcal{B}}
\newcommand{\Bo}{\B(\R)}
\newcommand{\Bon}[1]{\B(\R^{#1})}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sumn}{\sum_{n=1}^{\infty}}
\newcommand{\trans}{\intercal}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\e}{\epsilon}
\newcommand{\comp}{\mathsf{c}}
\newcommand{\emp}{\varnothing}
\newcommand{\floor}[1]{\left \lfloor{#1}\right \rfloor}
\newcommand{\se}[1]{\left{ #1 \right}}
\newcommand{\set}[2]{\left{ #1 ; : ; #2 \right}}
\newcommand{\sett}[2]{\left{ #1 ; | ; #2 \right}}
\newcommand{\Ex}[1]{\E\left[#1\right]}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dy}{dy}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator{\dpr}{d\PP}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Ord}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt; 
&lt;p&gt;I recently came across this simple to state puzzle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span style=&#34;color:silver&#34;&gt; Suppose you have a coin that has probabilities $p$ for heads and $1-p$ for tails. You play the following game with a friend. The first player picks one of the outcomes $HH$, $TH$, $HT$ and $TT$. The second player observes the choice of the first player and picks one of the remaining 3 outcomes. You then proceed to toss the coin infinitely many times independently constructing an infinite sequence of $H$â€™s and $T$â€™s. A player wins if their choice appears first in the sequence. For example, if player one chose $HH$ and player two chose $TT$ then $HTHTHTTHTHHH&amp;hellip;$ results in a victory for player two since $TT$ occurred before $HH$ in the sequence. When is it better to go first in this game?&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let&amp;rsquo;s make the conditions under which a player wins more concrete.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;The strategy: &lt;/span&gt; Itâ€™s better to go as the first player in the game if for some choice $a \in \{HH,TT,HT,TH\}$, the probability of winning the game is greater than $1/2$ no matter the choice of the second player. Itâ€™s better to go as the second player in the game if for every choice of the first player, we can find some element of the set $\{HH,TT,HT,TH\} \setminus \{a\}$, where $a$ is the choice of the first player, such that the probability of winning the game is greater than $1/2$.&lt;/p&gt;
&lt;p&gt;As an example, suppose $p = 3/4$, then if the first player chooses $HH$, then no matter the choice of the second player, the probability of the first player winning the game is greater than $1/2$. To see this note that the probability of getting two consecutive heads in the first two coin tosses itself is $9/16 &amp;gt; 1/2$.&lt;/p&gt;
&lt;p&gt;On first impression it seems as if it must be better to go as the first player, no matter the value of $p$, since the first player has more choices, but as we will see, the fact that the second player has the advantage of choosing the outcome &lt;em&gt;after&lt;/em&gt; observing the choice of the first player gives him an advantage for certain values of $p$.&lt;/p&gt;
&lt;p&gt;But before we do that, we should prove an implicit assumption: the game ends with a winner in a finite number of moves with probability $1$. But to do that we first need to define a probability space on which the game is played. In particular, does it even exist? Indeed it does as we see below.&lt;/p&gt;
&lt;h2 id=&#34;constructing-the-probability-space&#34;&gt;Constructing the probability space&lt;/h2&gt;
&lt;p&gt;To formalize our analysis we need to define a sequence of i.i.d. random variables, one for each coin toss.
However, the existence of a probability space on which we can define this sequence is not obvious at all.
For example, the following claim shows that we cannot always construct desired number of random variables on a measurable space.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Claim: &lt;/span&gt; There do not exist uncountably many independent, non-constant random variables on $([0,1], \B([0,1]), \lambda)$, where $\lambda$ is the Lebesgue measure on the Borel $\sigma$-algebra $\B([0,1])$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Assume that $\{X_i \}_{i \in I}$ is a collection of independent non-constant random variables. Define the collection $\{Y_i \} _{i \in I}$ by letting $Y_i = X_i \ind{|X_i| &amp;lt; C_i}$ where $C_i$ is large enough that $Y_i$ isn&amp;rsquo;t a constant. The collection formed by the random variables $Z_i = Y_i - \Ex{Y_i}$ is a collection of independent random variables. Note that the random variables $Z_i$ are in the separable Hilbert space $L^2([0,1], \lambda)$ and are orthogonal to each other. But a separable Hilbert can have only countably such elements. Thus, $I$ must be countable. $\square$&lt;/p&gt;
&lt;p&gt;Fortunately the situation isn&amp;rsquo;t so bleak for our case and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kolmogorov extension theorem&lt;/a&gt; allows us to claim the existence of a probability space on which there exists our required sequence of i.i.d. random variables if we can show the existence of probability spaces $(\Omega_n, \F_n, \PP_n)$ for $n$ coin tosses that satisfy the consistency conditions. This is very easy: let $\Omega_n = \{H, T\}^n$, $\F_n = 2^{\Omega_n}$, $\PP({\omega}) = p^{m} (1-p)^{n-m}$ where $m = $ number of $H$ in $\omega \in \Omega_n$, and finally $\PP(A) = \sum_{\omega \in A} \PP({\omega})$ for any $A \in \F_n$. It is easy to see that this construction satisfies the consistency conditions and thus there exists a unique probability measure $\PP$ defined on the measurable space $(\Omega, \F)$ where $\Omega = \{H, T\}^{\N}$ and  $\F$ is the $\sigma$-algebra generated by the cylinder sets.&lt;/p&gt;
&lt;h2 id=&#34;the-game-has-a-winner-with-probability-1&#34;&gt;The game has a winner with probability $1$&lt;/h2&gt;
&lt;p&gt;We can now safely say the following statement: Let $\{X_n\}$ be a sequence of i.i.d. Bernoulli random variables such that $X_n = H$ or $T$ depending on the result of the $n$&lt;sup&gt;th&lt;/sup&gt; coin toss. To show that the game has a winner with probability $1$ we need to prove that in the sequence $X_1, X_2, \ldots$ all of the four choices $HH$, $TH$, $HT$ and $TT$ appear in a finite number of coin tosses with probability $1$.&lt;/p&gt;
&lt;p&gt;To that end, fix any of the four choices $HH$, $TH$, $HT$ and $TT$, and call it $XY$. Let $E_n$ be the event that $X_{2n-1} = X$ and $X_{2n} = Y$. Then $\PP(E_n) = \e &amp;gt; 0$ independent of $n$, where $\e$ is some positive real number dependent on $XY$ (for example, if $XY = HT$ then $\e = p(1-p)$). Now the events $\{E_n\}$ are independent and $\sum_{n=1}^\infty \PP(E_n) = \infty$, and thus we can apply the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Borel%E2%80%93Cantelli_lemma#Converse_result&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;second Borel-Cantelli lemma&lt;/a&gt;
to get $\PP(E_n \text{ i.o.}) = 1$. But this immediately implies that each of the four outcomes appear in that sequence infinitely many times with probability $1$.&lt;/p&gt;
&lt;h2 id=&#34;back-to-the-game&#34;&gt;Back to the game&lt;/h2&gt;
&lt;p&gt;Recall the strategy outlined above. Without loss of generality we may assume $p \geq 1/2$ because otherwise we can just flip the tags $H$ and $T$. From a first player perspective we just want to find one of the choices $HH$, $TH$, $HT$ or $TT$. Let&amp;rsquo;s calculate the minimum $p$ we get for each choice.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $HH$ and player $2$ chooses $TH$, then it&amp;rsquo;s better to be player $1$ if $p &amp;gt; 1/\sqrt{2}$. Checking for other choices of player $2$ we see that $TH$ is the optimal choice.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $TT$ and player $2$ chooses $HT$, then for no $p \geq 1/2$ it is better to be player $1$.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $HT$ and player $2$ chooses $HH$, then for no $p \geq 1/2$ it is better to be player $1$.&lt;/p&gt;
&lt;p&gt;If player $1$ chooses $TH$ and player $2$ chooses $HT$, then for no $p \geq 1/2$ it is better to be player $1$.&lt;/p&gt;
&lt;p&gt;Overall it is better to be player $1$ if $p &amp;gt; 1/\sqrt{2}$ or if $p &amp;lt; 1 - 1/\sqrt{2}$. And on the other hand it is better to be player $2$ if $1 - 1/\sqrt{2} &amp;lt; p &amp;lt; 1/\sqrt{2}$.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kernels - Part 2</title>
      <link>https://makkar.github.io/post/kernels2/</link>
      <pubDate>Mon, 07 Sep 2020 16:05:35 -0400</pubDate>
      <guid>https://makkar.github.io/post/kernels2/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\P}{\mathcal{P}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\L}{\mathcal{L}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\trans}{\intercal}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator{\dom}{d\omega}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\O}{\mathcal{O}}
\DeclareMathOperator{\E}{\mathbb{E}}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt; 
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;We discussed 
&lt;a href=&#34;https://makkar.github.io/post/kernels/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;some functional analysis&lt;/a&gt; and the 
&lt;a href=&#34;https://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;basics of Reproducing Kernel Hilbert Space theory&lt;/a&gt; in the previous two articles. The aim of this article is to discuss the need for approximating the kernel matrix and one method in particular to do that, Random Fourier Features.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s start by discussing Kernel Ridge Regression, a simple application of kernel methods.&lt;/p&gt;
&lt;h1 id=&#34;kernel-ridge-regression&#34;&gt;Kernel Ridge Regression&lt;/h1&gt;
&lt;h2 id=&#34;ridge-regression&#34;&gt;Ridge Regression&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s recall the ridge regression model. We have some training data of the form:&lt;/p&gt;
&lt;p&gt;$$
(x_1, y_1), \ldots, (x_n, y_n) \in (\X, \Y)
$$
where $\X = \R^d$ and $\Y = \R$. We represent the training data more succinctly by letting $\matr{y} = [y_1, \ldots, y_n]^\trans$ and $\matr{X}$ be an $n \times d$ matrix whose $i$&lt;sup&gt;th&lt;/sup&gt; row is $x_i^\trans$. Then in ridge regression the likelihood model is&lt;/p&gt;
&lt;p&gt;$$
\matr{y} \sim \NN(\matr{X} \matr{w}, \sigma^2 \matr{I}_n)
$$
where $\matr{w}$ is a $d$-dimensional column vector representing the weights to learned, and $\sigma^2 &amp;gt; 0$. We also assume a Gaussian prior on $\matr{w}$&lt;/p&gt;
&lt;p&gt;$$
\matr{w} \sim \NN\left(0, \frac{1}{\lambda} \matr{I}_d\right)
$$&lt;/p&gt;
&lt;p&gt;Then the maximum a posteriori (MAP) estimate is given by&lt;/p&gt;
&lt;p&gt;\begin{align}
\matr{w}_{\text{MAP}} &amp;amp;= \argmax _{\matr{w}} \; \ln p(\matr{w} | \matr{y}, \matr{X}) \\
&amp;amp;= \argmax _{\matr{w}} \; \ln p(\matr{y} | \matr{w}, \matr{X}) + \ln p(\matr{w}) \\
&amp;amp;= \argmax _{\matr{w}} \; \underbrace{-\frac{1}{2\sigma^2} (\matr{y} - \matr{X} \matr{w})^\trans (\matr{y} - \matr{X} \matr{w}) - \frac{\lambda}{2} \matr{w}^\trans \matr{w}} _{\L}
\end{align}&lt;/p&gt;
&lt;p&gt;If we call this objective $\L$, then $\matr{w}_{\text{MAP}}$ is given by the solution of the equation formed by equating the gradient of $\L$ with respect to $\matr{w}$ to $0$. Thus,&lt;/p&gt;
&lt;p&gt;$$
0 = \nabla_{\matr{w}} \L = \frac{1}{\sigma^2} \matr{X}^\trans \matr{y} - \frac{1}{\sigma^2} \matr{X}^\trans \matr{X} \matr{w} - \lambda \matr{w}
$$&lt;/p&gt;
&lt;p&gt;and we get&lt;/p&gt;
&lt;p&gt;$$
\matr{w}_{\text{MAP}} = (\lambda \sigma^2 \matr{I} + \matr{X}^\trans \matr{X})^{-1} \matr{X}^\trans \matr{y}
$$&lt;/p&gt;
&lt;p&gt;which is called the ridge regression solution, and we denote it by $\matr{w}_{\text{RR}}$. We are inverting a $d \times d$ matrix taking $\O(d^3)$ time, and matrix multiplication of a $d \times d$ matrix with a $d \times n$ matrix taking $\O(d^2 n)$ time.&lt;/p&gt;
&lt;h2 id=&#34;kernel-ridge-regression-1&#34;&gt;Kernel Ridge Regression&lt;/h2&gt;
&lt;p&gt;The previous model suffers from limited expressiveness. As we have seen before, the idea of kernel methods is to define a map which takes our inputs from $\X$ to an RKHS $\H$:&lt;/p&gt;
&lt;p&gt;$$
\phi : \X \to \H
$$&lt;/p&gt;
&lt;p&gt;and then apply the linear model of ridge regression above to $\phi(x_i)$ instead of $x_i$. $\H$ could be an infinite-dimensional vector space, but for now suppose it is $D$-dimensional. Let $\matr{\Phi}$ represent the $n \times D$ matrix whose $i$&lt;sup&gt;th&lt;/sup&gt; row is $\phi(x_i)^\trans$. Then the ridge regression solution is given by&lt;/p&gt;
&lt;p&gt;\begin{equation}
\matr{w}_{\text{RR}} = (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans \matr{y}
\end{equation}&lt;/p&gt;
&lt;p&gt;We immediately realize the problem here: we are inverting a $D \times D$ matrix and $D$ can be huge! Fortunately, we have a matrix trick that we can exploit, the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Woodbury_matrix_identity#Discussion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;push-though identity&lt;/a&gt;. It looks like this&lt;/p&gt;
&lt;p&gt;\begin{align}
\matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) &amp;amp;= (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi}) \matr{\Phi}^\trans \quad \text{(can be seen by expanding)} \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) &amp;amp;= (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi}) \matr{\Phi}^\trans \quad \text{(left multiplying by } (\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \text{ )} \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) &amp;amp;= \matr{\Phi}^\trans \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans) (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} &amp;amp;= \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \quad \text{(right multiplying by } (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \text{ )} \\&lt;br&gt;
(\lambda \sigma^2 \matr{I}_D + \matr{\Phi}^\trans \matr{\Phi})^{-1} \matr{\Phi}^\trans &amp;amp;= \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1}
\end{align}&lt;/p&gt;
&lt;p&gt;And thus we get
$$
\matr{w}_{\text{RR}} = \matr{\Phi}^\trans (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \matr{y}
$$&lt;/p&gt;
&lt;p&gt;We are now inverting an $n \times n$ matrix taking $\O(n^3)$ time, and matrix multiplication of a $D \times n$ matrix with a $n \times n$ matrix taking $\O(D n^2)$ time.&lt;/p&gt;
&lt;p&gt;Notice that $\matr{\Phi} \matr{\Phi}^\trans$ is the gram matrix, and let&amp;rsquo;s denote it by $\matr{K}$. It&amp;rsquo;s $(i,j)$&lt;sup&gt;th&lt;/sup&gt; entry is $\matr{K}_{i,j} = \inner{\phi(x_i)}{\phi(x_j)} = K(x_i, x_j)$, where $K : \R^d \times \R^d \to \C$ is the kernel function corresponding to the RKHS $\H$.&lt;/p&gt;
&lt;p&gt;If we denote&lt;/p&gt;
&lt;p&gt;$$
\matr{\alpha} = (\lambda \sigma^2 \matr{I}_n + \matr{\Phi} \matr{\Phi}^\trans)^{-1} \matr{y} = (\lambda \sigma^2 \matr{I}_n + \matr{K})^{-1} \matr{y}
$$&lt;/p&gt;
&lt;p&gt;then we have&lt;/p&gt;
&lt;p&gt;$$
\matr{w}_{\text{RR}} = \matr{\Phi}^\trans \matr{\alpha} = \sum _{i=1}^n \alpha_i \phi(x_i)
$$&lt;/p&gt;
&lt;p&gt;and prediction for a test data point, $x$, is&lt;/p&gt;
&lt;p&gt;$$
\matr{w}_{\text{RR}}^\trans \phi(x) = \sum _{i=1}^n \alpha_i \phi(x_i)^\trans \phi(x) = \sum _{i=1}^n \alpha_i k _{x_i}(x)
$$&lt;/p&gt;
&lt;p&gt;where $k_{x_i}$ denotes the reproducing kernel for the point $x_i$.&lt;/p&gt;
&lt;p&gt;If this looks suspiciously similar to the 
&lt;a href=&#34;https://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Representer theorem&lt;/a&gt; (Theorem-8), then that&amp;rsquo;s because it is an instance of it! So prediction takes $\O(n)$ time.&lt;/p&gt;
&lt;p&gt;The point of this discussion was to show that Kernel Ridge Regression is computationally expensive. Simply storing the gram matrix, $\matr{K}$, takes $\O(n^2)$ space. We need to find methods which can circumvent using $\matr{K}$ if we are to apply kernel methods to anything other than the smallest of the data sets.&lt;/p&gt;
&lt;h1 id=&#34;kernel-approximation&#34;&gt;Kernel approximation&lt;/h1&gt;
&lt;p&gt;The idea behind kernel approximation methods is to replace the gram matrix, $\matr{K}$, with a low rank approximation, i.e., find an $n \times S$ matrix, $\matr{Z}$, such that&lt;/p&gt;
&lt;p&gt;$$
\matr{K} \approx \matr{Z} \matr{Z}^\trans
$$&lt;/p&gt;
&lt;p&gt;We want $S \ll n$, and thus we significantly reduce our time and space complexity. $\matr{Z}$ takes $\O(nS)$ space. Inversion of $(\lambda \sigma^2 \matr{I}_n + \matr{Z} \matr{Z}^\trans)$ takes $\O(n S^2)$ time.&lt;/p&gt;
&lt;p&gt;One way to think about this is we want to find a mapping of $x_i$&#39;s to some latent space such that the inner product in this latent space approximates the kernel&lt;/p&gt;
&lt;p&gt;$$
\matr{K}_{i,j} = K(x_i, x_j) \approx z_i^\trans z_j
$$&lt;/p&gt;
&lt;p&gt;where $z_i$ denotes the $i$&lt;sup&gt;th&lt;/sup&gt; row of $\matr{Z}$.
One way to do that is NystrÃ¶m approximation, which I won&amp;rsquo;t be discussing at all. The other method is to use Random Fourier Features. This approach was introduced by Rahimi and Recht in their seminal 2007 paper 
&lt;a href=&#34;https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random Features for Large-Scale Kernel Machines&lt;/a&gt;. It relies on a result from functional analysis called Bochner&amp;rsquo;s theorem, so let&amp;rsquo;s digress and discuss that first.&lt;/p&gt;
&lt;h1 id=&#34;bochners-theorem&#34;&gt;Bochner&amp;rsquo;s theorem&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Definition 28:&lt;/strong&gt; A function $K : \R^n \to \C$, is said to be &lt;em&gt;positive-definite&lt;/em&gt; if&lt;/p&gt;
&lt;p&gt;$$
\sum _{i,j = 1}^n \alpha_i \conj{\alpha_j} K(x_i - x_j) \geq 0
$$&lt;/p&gt;
&lt;p&gt;for every choice of $x_1, \ldots, x_n \in \R^n$ and for every choice of complex numbers $\alpha_1, \ldots, \alpha_n$.&lt;/p&gt;
&lt;p&gt;Notice how similar this definition is to the 
&lt;a href=&#34;https://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;definition of a kernel function&lt;/a&gt;. Positive-definite functions have some nice properties:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3:&lt;/strong&gt; If $K: \R^n \to \C$ is a positive-definite function, then $K(-x) = \conj{K(x)}$ for every $x \in \R^n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; It is easy to see that $K(0) \geq 0$. In the definition above, let $n = 2$, $x_1 = x$, $x_2 = 0$, $\alpha_1$ be an arbitrary complex number, and $\alpha_2 = 1$. Then applying the definition of a positive-definite function, we get&lt;/p&gt;
&lt;p&gt;$$
(1 + |\alpha_1|^2)K(0) + \alpha_1 K(x) + \conj{\alpha_1}K(-x) \geq 0
$$&lt;/p&gt;
&lt;p&gt;Let $\alpha_1 = 1$, then
$$
2K(0) + K(x) + K(-x) \geq 0
$$&lt;/p&gt;
&lt;p&gt;In particular, $K(x) + K(-x)$ is real, which implies&lt;/p&gt;
&lt;p&gt;$$
K(x) + K(-x) = \conj{K(x)} +\conj{K(-x)}
$$&lt;/p&gt;
&lt;p&gt;Similarly, letting $\alpha_1 = i$, we get $i(K(x) - K(-x))$ is real, which implies&lt;/p&gt;
&lt;p&gt;$$
K(x) - K(-x) = -\conj{K(x)} + \conj{K(-x)}
$$&lt;/p&gt;
&lt;p&gt;Adding these two equations, we get $K(-x) = \conj{K(x)}$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 4:&lt;/strong&gt; If $K: \R^n \to \C$ is a positive-definite function, then $K$ is bounded. In particular, $|K(x)| \leq K(0)$ for every $x \in \R^n$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; From lemma-3, $K(0)$ must be real. In the definition above, let $n = 2$, $x_1 = 0$, $x_2 = x$, $\alpha_1 = |K(x)|$, and $\alpha_2 = -\conj{K(x)}$. Then applying the definition of a positive-definite function, we get&lt;/p&gt;
&lt;p&gt;$$
2K(0) |K(x)|^2 - |K(x)| K(x) K(-x) - \conj{K(x)} |K(x)| K(x) \geq 0
$$&lt;/p&gt;
&lt;p&gt;Now use lemma-3 to substitute $\conj{K(x)}$ for $K(-x)$ in the middle term to get&lt;/p&gt;
&lt;p&gt;$$
2K(0) |K(x)|^2 - 2 |K(x)|^3 \geq 0
$$&lt;/p&gt;
&lt;p&gt;If $|K(x)| = 0$, then we obviously have our result, since we can easily show $K(0) \geq 0$, otherwise we can divide by $2|K(x)|^2$ to get&lt;/p&gt;
&lt;p&gt;$$
K(0) - |K(x)| \geq 0
$$&lt;/p&gt;
&lt;p&gt;which is our desired result. $\square$&lt;/p&gt;
&lt;p&gt;The following theorem is the converse of Bochner&amp;rsquo;s theorem. We state it first since it is easier to prove.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 9 [Converse of Bochner&amp;rsquo;s theorem]:&lt;/strong&gt; The Fourier transform of every finite Borel measure on $\R^n$ is positive-definite.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let $\mu$ be a finite Borel measure on $\R^n$. Let $K : \R^n \to \C$ be the Fourier transform of $\mu$, i.e.,&lt;/p&gt;
&lt;p&gt;$$
K(x) = \int_{\R^n} \exp(-i \omega^\trans x) \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;Let $x_1, \ldots, x_n \in \R^n$ and $\alpha_1, \ldots, \alpha_n \in \C$ be arbitrary. Then&lt;/p&gt;
&lt;p&gt;\begin{align}
\sum _{i,j = 1}^n \alpha_i \conj{\alpha_j} K(x_i - x_j) &amp;amp;= \sum _{i,j = 1}^n \alpha_i \conj{\alpha_j} \int _{\R^n} \exp\{-i \omega^\trans (x_i - x_j)\} \dmu(\omega) \\&lt;br&gt;
&amp;amp;= \int _{\R^n} \left| \sum _{i = 1}^n \alpha_i \exp(-i \omega^\trans x_i) \right|^2 \dmu(\omega) \\&lt;br&gt;
&amp;amp;\geq 0
\end{align}&lt;/p&gt;
&lt;p&gt;Thus, $K$ is positive-definite. $\square$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 10 [Bochner&amp;rsquo;s theorem]:&lt;/strong&gt; If $K$ is continuous and positive-definite, then $K$ is the Fourier transform of a finite positive Borel measure.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof since it is beyond my understanding. The proof can be easily found on the internet, see 
&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.917.270&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; or 
&lt;a href=&#34;http://individual.utoronto.ca/jordanbell/notes/bochnertheorem.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for example.&lt;/p&gt;
&lt;p&gt;This finite positive Borel measure is often called as &lt;em&gt;spectral measure&lt;/em&gt;. It is easy to see that $K(0) = \mu(\R^n)$, and thus if we assume $K(0) = 1$ then the spectral measure is a probability measure.&lt;/p&gt;
&lt;h1 id=&#34;random-fourier-features&#34;&gt;Random Fourier Features&lt;/h1&gt;
&lt;p&gt;Picking up where we left: We want to find a low-dimensional mapping of $x_i$&#39;s into a latent space such that we can approximate the kernel computation with an inner product in this latent space. We will use Bochner&amp;rsquo;s theorem for this. To apply this theorem we need to limit ourselves to a special class of kernels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 29:&lt;/strong&gt; A kernel function $K : \R^d \times \R^d \to \C$ is called &lt;em&gt;shift-invariant&lt;/em&gt; if $K(x, y) = K(x-y, 0)$.&lt;/p&gt;
&lt;p&gt;Gaussian and Laplacian kernels are examples of shift-invariant kernels.
Note that the function $K&amp;rsquo;: \R^d \to \C$ defined by $K&amp;rsquo;(x) = K(x, 0)$ is positive-definite if $K$ is a shift-invariant kernel. Bochner&amp;rsquo;s theorem then implies the existence of a spectral measure, $\mu$, such that&lt;/p&gt;
&lt;p&gt;$$
K(x, y) = K(x-y, 0) = K&amp;rsquo;(x-y) = \int _{\R^d} \exp\{-i\omega^\trans (x-y)\} \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;Let $p$ denote the Radon-Nikodym derivative of $\mu$ with respect to the Lebesgue measure on $\R^d$, then we can write the equation above as&lt;/p&gt;
&lt;p&gt;\begin{equation}
K(x, y) = \int _{\R^d} \exp\{-i\omega^\trans (x-y)\} p(\omega) \dom \tag{$\star$}
\end{equation}&lt;/p&gt;
&lt;p&gt;For example, if we take the Gaussian kernel&lt;/p&gt;
&lt;p&gt;$$
K(x,y) = \exp\left(-\frac{\norm{x-y}^2_2}{2 \sigma^2}\right)
$$&lt;/p&gt;
&lt;p&gt;then since $K(0,0) = 1$, $p$ is a probability density, and can be easily shown to be Gaussian&lt;/p&gt;
&lt;p&gt;$$
p \sim \NN\left(0, \frac{1}{\sigma^2} \matr{I}_d\right)
$$&lt;/p&gt;
&lt;p&gt;We now see an obvious way to approximate the kernel from equation $(\star)$: use Monte Carlo approximation. If we take $S$ samples, $\omega_1, \ldots, \omega_S \sim p(\omega)$,&lt;/p&gt;
&lt;p&gt;\begin{align}
K(x, y) &amp;amp;= \int _{\R^d} \exp\{-i\omega^\trans (x-y)\} p(\omega) \dom \\&lt;br&gt;
&amp;amp;\approx \frac{1}{S} \sum _{s=1}^S \exp\{-i\omega^\trans_s (x-y)\} \\&lt;br&gt;
&amp;amp;= \inner{z(x)}{z(y)}
\end{align}&lt;/p&gt;
&lt;p&gt;where $z : \R^d \to \R^S$ is the latent mapping given by&lt;/p&gt;
&lt;p&gt;$$
z(x) = \frac{1}{\sqrt{S}}[\exp(-i \omega ^\trans _1 x), \ldots, \exp(-i \omega ^\trans _S x)]^\trans
$$&lt;/p&gt;
&lt;p&gt;We can get another mapping by noting that if the kernel is real, then from $(\star)$ we can see that the RHS must also be real and we can thus write it as&lt;/p&gt;
&lt;p&gt;\begin{equation}
K(x, y) = \int _{\R^d} \cos\{\omega^\trans (x-y)\} p(\omega) \dom
\end{equation}&lt;/p&gt;
&lt;p&gt;Using the fact that $\cos(a - b) = \cos(a) \cos(b) + \sin(a) \sin(b)$, we can write $\cos\{\omega^\trans (x-y)\} = \cos(\omega^\trans x) \cos(\omega^\trans y) + \sin(\omega^\trans x) \sin(\omega^\trans y)$&lt;/p&gt;
&lt;p&gt;Thus, if we define $z_1 : \R^d \to \R^{2S}$ by&lt;/p&gt;
&lt;p&gt;$$
z_1(x) = \frac{1}{\sqrt{S}}[\cos(\omega^\trans_1 x), \ldots, \cos(\omega^\trans_S x), \sin(\omega^\trans_1 x), \ldots, \sin(\omega^\trans_S x)]^\trans
$$&lt;/p&gt;
&lt;p&gt;we have $\inner{z_1(x)}{z_1(y)} \approx K(x, y)$.&lt;/p&gt;
&lt;p&gt;Another mapping that is used is $z_2 : \R^d \to \R^{S}$ defined by&lt;/p&gt;
&lt;p&gt;$$
z_2(x) = \sqrt{\frac{2}{S}}[\cos(\omega^\trans_1 x + b_1), \ldots, \cos(\omega^\trans_S x + b_S)]^\trans
$$&lt;/p&gt;
&lt;p&gt;where $b_1, \ldots, b_S \sim \text{Unif}[0, 2\pi]$. It is not too difficult to show that&lt;/p&gt;
&lt;p&gt;$$
\E_{\omega, b}[z_2(x)^\trans z_2(y)] = \E_{\omega}[z_1(x)^\trans z_1(y)]
$$&lt;/p&gt;
&lt;p&gt;There exist many modifications to the basic RFF approach described here. For example, we can do Quasi-Monte Carlo approximations instead of Monte Carlo approximations. See the paper 
&lt;a href=&#34;https://jmlr.org/papers/volume17/14-538/14-538.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quasi-Monte Carlo Feature Maps for Shift-Invariant Kernels&lt;/a&gt; by Avron, Sindhwani, Yang and Mahoney (2016). Or we could sample from a modified distribution in Fourier space, given by the leverage function of the kernel. See the paper 
&lt;a href=&#34;https://arxiv.org/pdf/1804.09893.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees&lt;/a&gt; by Avron et al (2017).&lt;/p&gt;
&lt;h1 id=&#34;extension-to-a-more-general-class-of-kernels&#34;&gt;Extension to a more general class of kernels&lt;/h1&gt;
&lt;p&gt;I have often seen kernels being represented as an integral of a product of two functions neatly separating the dependence on $x$ and $y$:&lt;/p&gt;
&lt;p&gt;$$
K(x,y) = \int_{\Omega} \varphi(\omega, x) \varphi(\omega, y) \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;for some function $\varphi(\cdot, \cdot)$ and measure $\mu$ on $\Omega$. I found a good explanation of when this is possible in the papers by Bach (2017) 
&lt;a href=&#34;https://jmlr.org/papers/volume18/14-546/14-546.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Breaking the Curse of Dimensionality with Convex Neural Networks&lt;/a&gt; and  Li et al (2019) 
&lt;a href=&#34;https://arxiv.org/pdf/1806.09178.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards a Unified Analysis of Random Fourier Features&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let $\mu$ be a Borel probability measure on the compact space $\Omega$, and $\varphi: \Omega \times \X \to \R$ be a function such that the functions $\varphi(\cdot, x) : \Omega \to \R$ are measurable for all $x \in \X$, i.e., they are random variables. Now define the set $\H$ to consist of all functions $f$ that can written as&lt;/p&gt;
&lt;p&gt;$$
f(x) = \int_{\Omega} h(\omega) \varphi(\omega, x) \dmu(\omega) \quad \text{for all } x \in \X
$$&lt;/p&gt;
&lt;p&gt;for some $h: \Omega \to \R$ such that $\int_{\Omega} h^2 \dmu &amp;lt; \infty$. Let us define the squared norm, $\norm{f}_{\H}^2$, as the infimum of $\int_{\Omega} h^2 \dmu$ over all functions $h$ for which $f$ can be decomposed as above. Then it can be shown that $\H$ is an RKHS with the kernel&lt;/p&gt;
&lt;p&gt;$$
K(x,y) = \int_{\Omega} \varphi(\omega, x) \varphi(\omega, y) \dmu(\omega)
$$&lt;/p&gt;
&lt;p&gt;Therefore, we can again approximate this kernel with a Monte Carlo approximation:&lt;/p&gt;
&lt;p&gt;\begin{align}
K(x,y) &amp;amp;= \int_{\Omega} \varphi(\omega, x) \varphi(\omega, y) \dmu(\omega) \\&lt;br&gt;
&amp;amp;\approx \frac{1}{S} \sum_{s=1}^S \varphi(\omega_s, x) \varphi(\omega_s, y)
\end{align}&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;Random Fourier Features is an easy to implement approach that allows us to apply kernel methods on large data sets. I haven&amp;rsquo;t discussed the number of samples, $S$, needed to approximate the kernel function within an error bound. You can find such results in the papers linked above.&lt;/p&gt;
&lt;p&gt;With this article I conclude the series on kernels. I had initially set out to write a short piece on Random Fourier Features but the subject of kernel theory is vast and beautiful, and that short piece metastasised to three articles.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Zorn&#39;s lemma</title>
      <link>https://makkar.github.io/post/zorn/</link>
      <pubDate>Fri, 07 Aug 2020 18:59:12 -0400</pubDate>
      <guid>https://makkar.github.io/post/zorn/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\P}{\mathscr{P}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\Ch}{\mathscr{C}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\S}{\mathcal{S}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\se}[1]{\left\{ #1 \right\}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;I have been self-studying some functional analysis recently, and Zorn&amp;rsquo;s lemma comes up often in proofs (for example, showing that every nontrivial vector space has a Hamel basis (I show this fundamental result below) or in the proof of Hahn-Banach theorem). Since I have no formal background in mathematics (my undergrad was in Mechanical engineering), it was my first time hearing about Zorn&amp;rsquo;s lemma. I read its statement from the Wikipedia article, naively assuming I understood this extremely powerful tool. But every usage of Zorn&amp;rsquo;s lemma seemed contrived to me, and it was only in retrospect that I could see how Zorn&amp;rsquo;s lemma seems like an obvious tool to apply. An excellent article which helped me understand Zorn&amp;rsquo;s lemma is 
&lt;a href=&#34;https://gowers.wordpress.com/2008/08/12/how-to-use-zorns-lemma/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to use Zorn&amp;rsquo;s lemma&lt;/a&gt; by Timothy Gowers. If you didn&amp;rsquo;t know about Gowers&amp;rsquo;s article, then mentioning it is the biggest contribution of my article and I recommend you read it.&lt;/p&gt;
&lt;p&gt;In this article, I want to show how to use Zorn&amp;rsquo;s lemma by stating two theorems and discussing their proofs. Things &amp;ldquo;clicked&amp;rdquo; for me when I proved the first theorem discussed below. I will therefore try to be verbose, and go through my thinking process in detail. But before we do that let me define some important concepts.&lt;/p&gt;
&lt;h1 id=&#34;preliminaries&#34;&gt;Preliminaries&lt;/h1&gt;
&lt;p&gt;Recall the concept of &lt;em&gt;relation&lt;/em&gt; I defined 
&lt;a href=&#34;https://makkar.github.io/post/equivalence-relations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 1: &lt;/span&gt; Let $P$ be a nonempty set. A &lt;em&gt;partial order relation&lt;/em&gt; in $P$ is a relation which is symbolized by $\preceq$ and that satisfies the following properties for all $x,y,z \in P$:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reflexivity: $x \preceq x$;&lt;/li&gt;
&lt;li&gt;Antisymmetry: $x \preceq y$ and $y \preceq x$ implies $x = y$;&lt;/li&gt;
&lt;li&gt;Transitivity: $x \preceq y$ and $y \preceq z$ implies $x \preceq z$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The set $P$ is then called a &lt;em&gt;partially ordered set&lt;/em&gt;. If in addition to these three properties, the set $P$ also satisfies&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;any two elements are comparable, i.e., either $x \preceq y$ or $y \preceq x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;then the relation is called a &lt;em&gt;total order relation&lt;/em&gt; and the set $P$ is called a &lt;em&gt;totally ordered set&lt;/em&gt; or a &lt;em&gt;chain&lt;/em&gt;. We often write $x \prec y$ to mean $x \preceq y$ but $x \neq y$.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Let $P$ be the set of all positive integers, and let $m \preceq n$ mean that $m$ divides $n$. Then $P$ is a partially ordered set. It is not a chain as $2$ and $3$ are not comparable, for example.&lt;/li&gt;
&lt;li&gt;Let $P$ be the set of all real numbers, and let $x \preceq y$ mean that $y-x$ is nonnegative. Then $P$ is a chain. Note that with this choice of ordering we have our usual ordering of real numbers, which we denote by $x \leq y$.&lt;/li&gt;
&lt;li&gt;Let $P$ be the class of all subsets of some universal set $U$, and let $A \preceq B$ for $A,B \in P$ mean that $A \subseteq B$. Then $P$ is a partially ordered set. It is not a chain because if $U$ contains at least two elements, then we can find two subsets of $U$ neither of which is a subset of the other.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 2: &lt;/span&gt; If $P$ is a partially ordered set, an element $x \in P$ is said to be &lt;em&gt;maximal&lt;/em&gt; if for any $y \in P$ for which $x \preceq y$, we must have $x = y$.&lt;/p&gt;
&lt;p&gt;Note that a maximal element does not have to be bigger than everything else: it just must not be smaller than anything else. A maximal element may not exist, and if it exists it may not be unique.
Examples 1 and 2 above have no maximal elements. Example 3 has one maximal element: $U$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 3: &lt;/span&gt; Let $Q$ be a nonempty subset of a partially ordered set $P$. An element $x \in P$ is called an &lt;em&gt;upper bound&lt;/em&gt; of $Q$ if $y \preceq x$ for every $y \in Q$. An upper bound of $Q$ is called a &lt;em&gt;least upper bound&lt;/em&gt; of $Q$ if it is less than or equal to every upper bound of $Q$.&lt;/p&gt;
&lt;p&gt;Similarly for &lt;em&gt;lower bound&lt;/em&gt; and &lt;em&gt;greatest lower bound&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note how similar these definitions are to the definitions of supremum and infimum for real numbers. That&amp;rsquo;s because in that case they are the same. In Example 1 above, if we take $Q$ to be any finite subset of $P$, then the greatest lower bound is the greatest common divisor of all the elements of $Q$ and the least upper bound is the least common multiple. In Example 3 above, let $Q$ be any nonempty subset of $P$. Then the least upper bound is the union of all the sets in $Q$, and the greatest lower bound is the intersection of all the sets in $Q$.&lt;/p&gt;
&lt;p&gt;We are now ready to state the Zorn&amp;rsquo;s lemma.&lt;/p&gt;
&lt;h1 id=&#34;zorns-lemma&#34;&gt;Zorn&amp;rsquo;s lemma&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Zorn&amp;rsquo;s lemma: &lt;/span&gt; If $P$ is a partially ordered set in which every chain has an upper bound, then $P$ possesses a maximal element.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll mention in passing that Zorn&amp;rsquo;s lemma is equivalent to 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Axiom_of_choice&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;axiom of choice&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;example-application-1&#34;&gt;Example application 1&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 1: &lt;/span&gt; Any infinite set $X$ can be represented as a union of a disjoint class of countably infinite subsets.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; The theorem is trivial if $X$ is countably infinite, so assume that it is uncountably infinite.&lt;/p&gt;
&lt;p&gt;The first thing that we need to do is realize Zorn&amp;rsquo;s lemma can be applied here. I use the following description taken from Gowers&amp;rsquo;s article as a guiding principle:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you are building a mathematical object in stages and find that (i) you have not finished even after infinitely many stages, and (ii) there seems to be nothing to stop you continuing to build, then Zornâ€™s lemma may well be able to help you.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The fact that we need to build $X$ by taking a union of some sets, and it could be an uncountable union and therefore we might not finish even in countably infinite number of stages, suggests Zorn&amp;rsquo;s lemma might be helpful here. Zorn&amp;rsquo;s lemma will prove the existence of a maximal element in a partially ordered set, therefore, we want to construct a partially ordered set such that we can prove that its maximal element constructs $X$.&lt;/p&gt;
&lt;p&gt;We want to build our partially ordered set to be consisting of elements of this form: disjoint class of countably infinite subsets of $X$. The reason we want to define our partially ordered set this way is because, first, the subset operation makes it a partially ordered set, and second, it seems intuitively true that the maximal element of this partially ordered set, if it exists, will be such that the union of its elements is $X$.&lt;/p&gt;
&lt;p&gt;To this end, let $\P$ be the set of all disjoint classes of countably infinite subsets of $X$. $\P$ is partially ordered by $\subseteq$ as we saw in Example 3 above.&lt;/p&gt;
&lt;p&gt;To be able to apply Zorn&amp;rsquo;s lemma we need to show that every chain in $\P$ has an upper bound. Let $\Ch$ be a chain in $\P$. A natural guess for the upper bound of $\Ch$ is $\U = \bigcup _{\S \in \Ch} \S$. To show that $\U$ is an upper bound of $\Ch$ we need to show that $\U \in \P$ and $\S \subseteq \U$ for every $\S \in \Ch$. The second claim is trivial from $\U$&#39;s definition. It is easy to see that $\U$ is a class of countably infinite subsets of $X$ because this is true for each $\S \in \Ch$. To show that $\U$ is a disjoint class, let $A, B \in \U$ be distinct elements of $\U$. There exist elements $\S_A, \S_B \in \Ch$ such that $A \in \S_A$ and $B \in \S_B$. Since $\Ch$ is a chain, either $\S_A \subseteq \S_B$ or $\S_B \subseteq \S_A$. Without loss of generality, $\S_A \subseteq \S_B$. Then $A, B \in \S_B$ and this implies $A$ and $B$ are disjoint because this is true for the elements of our chain. Thus $\U$ is a disjoint class of countably infinite subsets and hence belongs to $\P$.&lt;/p&gt;
&lt;p&gt;By the Zorn&amp;rsquo;s lemma, $\P$ possesses a maximal element, $\M$. If $\bigcup _{E \in \M} E = X$ then we are done because $\M$ is the required disjoint class of countably infinite subsets whose union is $X$. But it can still happen that some elements of $X$ are not present in this union. In this case we can show that these leftover elements form a finite set and we can get our required disjoint class by adding these leftover elements to any element of $\M$. To see that the leftover elements must be finite, denote the set of leftover elements by $Y = X \setminus \bigcup _{E \in \M} E$. If $Y$ is infinite it must contain a countably infinite subset $Z \subseteq Y$. Consider the class $\M \cup \{Z\}$. It is an element of $\P$ because it is a disjoint class of countably infinite subsets of $X$. But $\M$ is a strict subset of $\M \cup \{Z\}$ which contradicts the fact that $\M$ is a maximal element of $\P$.&lt;/p&gt;
&lt;p&gt;We are done! $\square$&lt;/p&gt;
&lt;h1 id=&#34;example-application-2&#34;&gt;Example application 2&lt;/h1&gt;
&lt;p&gt;Let us end by showing a fundamental theorem in functional analysis.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Definition 4: &lt;/span&gt; Let $X$ be a nontrivial vector space (i.e., $X \neq \se{0}$) over the field $\K$. Then a &lt;em&gt;Hamel basis&lt;/em&gt; of $X$ is any family $\se{e_i}_{i \in I}$ of vectors $e_i \in X$ that satisfy&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The family is linearly independent, i.e., given any finite subfamily $\se{e_j}_ {j \in J}$ of $\se{e_i}_{i \in I}$ and any scalars $\se{\alpha_j} _{j \in J} \subseteq \K$ such that $\sum_{j \in J} \alpha_j e_j = 0$, then $\alpha_j = 0$, $j \in J$.&lt;/li&gt;
&lt;li&gt;$\text{Span}\left(\se{e_i} _{i \in I}\right) = X$, i.e., given any vector $x \in X$, there exists a finite subfamily $\se{e_j} _ {j \in J}$ of $\se{e_i} _ {i \in I}$ and there exist scalars $\se{x_j} _ {j \in J} \subseteq \K$, such that $x = \sum _ {j \in J} x_j e_j.$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Theorem 2: &lt;/span&gt; Let $X$ be a nontrivial vector space, then there exists a Hamel basis of $X$.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:darkgray; font-size: 16pt&#34;&gt;Proof: &lt;/span&gt; Let $\cP$ denote the set formed by all linearly independent families of vectors of $X$. Hence, $\cP$ is nonempty, since $\cP$ contains $\se{x}$, where $x$ is any nonzero vector of $X$. We define a partial order on the elements of $\cP$ as follows: if $E = \se{e_i} _ {i \in I}$ and $F = \se{e_j} _ {j \in J}$ are any two elements of $\cP$, then $E \preceq F$ iff $E \subseteq F$.&lt;/p&gt;
&lt;p&gt;The next step is to show that if $\mathcal{C}$ is any chain in $\cP$, then $\mathcal{C}$ has an upper bound in $\cP$. To this end, define $U = \bigcup_{S \in \mathcal{C}} S$. We claim that this is the desired upper bound. $U$ is in $\cP$ because any finite subfamily $\se{e_i} _ {i=1}^n$ of $U$ is a subfamily of some $S \in \mathcal{C}$ since $\mathcal{C}$ is a chain, and therefore the vectors $\se{e_i} _ {i=1}^n$ are linearly independent. Also, $U$ is clearly an upper bound of $\mathcal{C}$, since $S \subseteq U$ for all $S \in \mathcal{C}$.&lt;/p&gt;
&lt;p&gt;By the Zorn&amp;rsquo;s lemma, $\cP$ possesses a maximal element, $M$, which we claim to be a Hamel basis of $X$. For otherwise, there would exist a nonzero vector $x \in X$ that cannot be written as a linear combination of elements of $M$. But then $M \cup \se{x}$ would be an element of $\cP$ that satisfies $M \prec M \cup \se{x}$, a contradiction. $\square$&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kernels - Part 1</title>
      <link>https://makkar.github.io/post/kernels1/</link>
      <pubDate>Thu, 16 Jul 2020 16:20:23 -0400</pubDate>
      <guid>https://makkar.github.io/post/kernels1/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\modu}[1]{\lvert#1\rvert}
\newcommand{\conj}[1]{\overline{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is the second blog post in the series of blog posts on kernels. In the 
&lt;a href=&#34;http://makkar.github.io/post/kernels/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first part&lt;/a&gt; I introduced the functional analysis background for kernel theory. I highly recommend you read it before continuing. I will frequently refer to it and use the same notation. In this blog post I aim to introduce the fundamental theorems like Mercer&amp;rsquo;s theorem and Representer theorem.&lt;/p&gt;
&lt;h1 id=&#34;characterization-of-reproducing-kernels&#34;&gt;Characterization of reproducing kernels&lt;/h1&gt;
&lt;p&gt;We already defined the notion of reproducing kernels for an RKHS (Definition 20). We now turn our attention to obtaining necessary and sufficient conditions for a function $K : \X \times \X \to \C$ to be the reproducing kernel for some RKHS. But first we must define kernel functions.&lt;/p&gt;
&lt;h2 id=&#34;kernel-functions&#34;&gt;Kernel functions&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s start by recalling a basic definition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 21:&lt;/strong&gt; Let $A = (a_{i,j})$ be an $n \times n$ complex matrix. Then $A$ is called &lt;em&gt;positive&lt;/em&gt; if for every $\alpha_1, \ldots, \alpha_n \in \C$ we have
$$ \sum_{i,j = 1}^{n} \conj{\alpha_i}\alpha_j a_{i,j} \geq 0 $$
We denote this by $A \geq 0$.&lt;/p&gt;
&lt;p&gt;Note that if we define a vector $x \in \C^n$ to be such that its $i$&lt;sup&gt;th&lt;/sup&gt; component is $\alpha_i$, then the condition above can rewritten as
$$ \inner{Ax}{x} \geq 0 $$&lt;/p&gt;
&lt;p&gt;Also note that if $A \geq 0$ then $A = A^* $, where $A^* $ denotes the Hermitian matrix (also known as the self-adjoint matrix), $\conj{A^T}$. Therefore, positivity gives self-adjoint property for free if we are dealing with complex matrices. Things aren&amp;rsquo;t so elegant for real matrices. For the real case we need to explicitly state that the matrix $A$ is also symmetric apart from what&amp;rsquo;s stated in the definition above. Therefore, we often use the following as the definition of positive matrices:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 21&amp;rsquo;:&lt;/strong&gt; An $n \times n$ matrix $A$ is positive, in symbols $A \geq 0$, if it is self-adjoint and if $\inner{Ax}{x} \geq 0$ for all $x \in \C^n$.&lt;/p&gt;
&lt;p&gt;Positive matrices are also alternatively called &lt;em&gt; positive semidefinite&lt;/em&gt; or &lt;em&gt;nonnegative&lt;/em&gt; matrices.&lt;/p&gt;
&lt;p&gt;The following lemma connects the concept of positive matrices to its eigenvalues.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2:&lt;/strong&gt; A matrix $A \geq 0$ if and only if $A = A^*$ and every eigenvalue of $A$ is nonnegative.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let us first suppose $A \geq 0$, then $A = A^*$ by definition. Now if $\lambda$ is an eigenvalue of $A$ and $v$ is an eigenvector of $A$ corresponding to $\lambda$, we have
$$ 0 \leq \inner{Av}{v} = \inner{\lambda v}{v} = \lambda \inner{v}{v} $$
Thus, $\lambda$ is a nonnegative number.&lt;/p&gt;
&lt;p&gt;For the other side, find an orthonormal basis $v_1, \ldots, v_n$ consisting of eigenvectors of $A$ (it exists by the Spectral theorem). Let $v_i$ be the eigenvector corresponding to the eigenvalue $\lambda_i$. Then for any $x = \sum_i \alpha_i v_i$ we have $\inner{Ax}{x} = \sum_i \lambda_i \modu{\alpha_i}^2$. Since $\lambda_i \geq 0$, $\inner{Ax}{x} \geq 0$ and $A$ must be positive. $\square$&lt;/p&gt;
&lt;p&gt;We are now ready to define kernel function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 22:&lt;/strong&gt; Let $\X$ be a set, then $K : \X \times \X \to \C$ is called a &lt;em&gt;kernel function&lt;/em&gt; if for every $n \in \N$ and for every choice of $\{x_1, \ldots, x_n\} \subseteq \X$, the matrix $(K(x_i, x_j)) \geq 0$. We will use the notation $K \geq 0$ to denote that the function $K$ is a kernel function.&lt;/p&gt;
&lt;p&gt;Kernel functions are alternatively called &lt;em&gt;positive definite functions&lt;/em&gt; or &lt;em&gt;positive semidefinite functions&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 23:&lt;/strong&gt; Given a kernel function $K : \X \times \X \to \C$ and points $x_1, \ldots, x_n \in \X$, the $n \times n$ matrix $(K(x_i, x_j))$ is called the Gram matrix of $K$ with respect to $x_1, \ldots, x_n$.&lt;/p&gt;
&lt;p&gt;Some examples follow:&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Linear kernels&lt;/em&gt;: When $\X = \R^d$, we can define the linear kernel function as
$$K(x, y) = \inner{x}{y} $$
It is clearly a symmetric function of its arguments, and hence self-adjoint. To prove positivity, let $x_1, \ldots, x_n \in \R^d$ be an arbitrary collection of points, and consider its gram matrix $\matr{K}$, i.e., $\matr{K}_{i,j} = K(x_i, x_j) = \inner{x_i}{x_j}$. Then for any $\alpha \in \R^n$, we have&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$ \inner{\matr{K} \alpha}{\alpha} = \alpha^T \matr{K}^T \alpha = \alpha^T \matr{K} \alpha = \sum_{i,j = 1}^n \alpha_i \alpha_j \inner{x_i}{x_j} = \left\lVert \sum_{i=1}^n \alpha_i x_i \right\rVert^2 \geq 0 $$&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Polynomial kernels&lt;/em&gt;: A natural generalization of the linear kernel on $\R^d$ is the homogeneous polynomial kernel
$$ K(x, y) = (\inner{x}{y})^m $$
of degree $m \geq 2$, also defined on $\R^d$. It is clearly a symmetric function. To prove positivity, note that&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \left( \sum_{i=1}^d x_i y_i \right)^m $$&lt;/p&gt;
&lt;p&gt;This will have $D = \binom{m+d-1}{m}$ monomials, so to simplify the analysis let&amp;rsquo;s take $m=2$. Then&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \sum_{i=1}^d x_i^2 y_i^2 + 2 \sum_{i &amp;lt; j} x_i x_j y_i y_j $$
In this case $D = \binom{d+1}{d} = d + \binom{d}{2}$. Define a mapping $\Phi: \R^d \to \R^D$ such that&lt;/p&gt;
&lt;p&gt;$$ \Phi(x) = [x_1^2, \ldots, x_d^2, \sqrt{2}x_1 x_2, \ldots, \sqrt{2} x_{d-1} x_d ]^T $$&lt;/p&gt;
&lt;p&gt;Then&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \inner{\Phi(x)}{\Phi(y)} $$&lt;/p&gt;
&lt;p&gt;Following the same argument as the first example, we can verify that the gram matrix thus formed is positive.&lt;/p&gt;
&lt;p&gt;The mapping $x \mapsto \Phi(x)$ is often referred to as a &lt;em&gt;feature map&lt;/em&gt;. We see that dealing with elements in    the feature space, i.e. the range of $\Phi$, is computationally expensive. The relation $K(x,y) = \inner{\Phi(x)}{\Phi(y)}$ allows us compute the inner products using the kernel function instead of actually taking the inner product in a very high dimensional space. We will see that this &amp;ldquo;kernel trick&amp;rdquo; holds for very many kernel functions when we discuss Mercer&amp;rsquo;s theorem.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Gaussian kernels&lt;/em&gt;: Given some compact subset $\X \subseteq \R^d$, consider the Gaussian kernel&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \exp{\left( -\frac{1}{2 \sigma^2} \norm{x-y}^2 \right)} $$&lt;/p&gt;
&lt;p&gt;It is not obvious why this is a kernel function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;equivalence-between-kernel-function-and-reproducing-kernel&#34;&gt;Equivalence between kernel function and reproducing kernel&lt;/h2&gt;
&lt;p&gt;Let us return to the characterization of reproducing kernels. We will now prove that a function is a kernel function if and only if there is an RKHS for which it is the reproducing kernel. At this point recall Theorem-3 which states that an RKHS admits a unique reproducing kernel.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 4:&lt;/strong&gt; Let $\X$ be a set and let $\H$ be an RKHS on $\X$ with reproducing kernel $K$. Then $K$ is a kernel function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; For some arbitrary $n \in \N$ fix some arbitrary collection $x_1, \ldots, x_n \in \X$ and $\alpha \in \C^n$. Then if we denote by $\matr{K}$ the gram matrix of $K$ with respect to $x_1, \ldots, x_n$, we have
$$\inner{\matr{K} \alpha}{\alpha} =  \sum_{i,j = 1}^n \conj{\alpha_i}\alpha_j K(x_i, x_j) = \sum_{i,j = 1}^n \conj{\alpha_i}\alpha_j \inner{k_{x_j}}{k_{x_i}} = \left\lVert \sum_{i=1}^n \alpha_i k_{x_i} \right\rVert^2 \geq 0 \quad$$&lt;/p&gt;
&lt;p&gt;And thus $K$ is a kernel function. $\square$&lt;/p&gt;
&lt;p&gt;What does it mean in the above proof if we have an equality? That is, if $\inner{\matr{K} \alpha}{\alpha} = 0$? This happens if and only if $\left\lVert \sum_{i=1}^n \alpha_i k_{x_i} \right\rVert = 0$. But this means that for every $f \in \H$ we have $\sum_{i=1}^n \conj{\alpha_i} f(x_i) = \inner{f}{\sum_i \alpha_i k_{x_i}} = 0$. Thus, in this case there is an equation of linear dependence between the values of every function in $\H$ at this finite set of points.&lt;/p&gt;
&lt;p&gt;Now let us state the converse of Theorem-4. It is a deep result in RKHS theory known as the Mooreâ€“Aronszajn theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 5 [Mooreâ€“Aronszajn theorem]:&lt;/strong&gt; Let $\X$ be a set and let $K : \X \times \X \to \C$ be a kernel function, then there exists a reproducing kernel Hilbert space $\H$ of functions on $\X$ such that $K$ is the reproducing kernel of $\H$.&lt;/p&gt;
&lt;p&gt;For a proof see 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#Moore%E2%80%93Aronszajn_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here on Wikipedia&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In light of these two theorems we have the following notation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 24:&lt;/strong&gt; Given a kernel function $K : \X \times \X \to \C$, we let $\H(K)$ denote the unique RKHS with the reproducing kernel $K$.&lt;/p&gt;
&lt;p&gt;It is not an easy problem to start with a kernel function $K$ on some set $\X$ and give a concrete description of $\H(K)$. I will not be discussing this here, but there are plenty of resources available where you can see this problem getting discussed. I recommend Paulsen and Raghupathi&amp;rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces.&lt;/p&gt;
&lt;h1 id=&#34;mercers-theorem&#34;&gt;Mercer&amp;rsquo;s theorem&lt;/h1&gt;
&lt;p&gt;Recall the Spectral theorem for finite-dimensional vector spaces:  A linear operator $T: V \to V$ for some finite dimensional vector space $V$ on $\C$ is normal, i.e., $T T^* = T^* T$, if and only if $V$ has an orthonormal basis consisting of eigenvectors of $T$. This implies that if $\matr{U} = [v_1, \ldots, v_n]$ is a unitary matrix containing the $i$&lt;sup&gt;th&lt;/sup&gt; eigenvector in its $i$&lt;sup&gt;th&lt;/sup&gt; column and $\matr{\Lambda} = \text{diag}(\lambda_1, \ldots, \lambda_n)$ is a diagonal matrix containing the corresponding eigenvalues then if $\matr{K}$ is a normal matrix then it can written as
$$\matr{K} = \matr{U} \matr{\Lambda} \matr{U}^T = \sum_{i=1}^n \lambda_i v_i v_i^T$$&lt;/p&gt;
&lt;p&gt;Mercer&amp;rsquo;s theorem generalizes this decomposition to kernel functions. Let us start by defining a special type of kernel function.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 25:&lt;/strong&gt; Let $\X$ be a compact metric space. A function $K : \X \times \X \to \C$ is called a &lt;em&gt;Mercer kernel&lt;/em&gt; if it is a continuous kernel function.&lt;/p&gt;
&lt;p&gt;Recall the space $L^2(\mu)$ from Definition-8 where we now take $\X$ (written as $X$ there) to be a compact metric space.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 26:&lt;/strong&gt; Given a Mercer kernel $K : \X \times \X \to \C$, we define a linear operator $T_{K} : L^2(\mu) \to L^2(\mu)$ as
$$ T_K(f)(x) := \int_{\X} K(x, y) f(y) \dmu(y), \quad (x \in \X) $$&lt;/p&gt;
&lt;p&gt;We assume that the Mercer kernel satisfies the &lt;em&gt;Hilbert-Schmidt condition&lt;/em&gt;, stated as&lt;/p&gt;
&lt;p&gt;$$ \int_{\X \times \X} \left\lvert K(x,y) \right\rvert^2 \dmu(x) \dmu(y) &amp;lt; \infty $$&lt;/p&gt;
&lt;p&gt;which ensures that $T_K$ is a bounded linear operator on $L^2(\mu)$. Indeed, we have&lt;/p&gt;
&lt;p&gt;$$ \lVert T_K(f) \rVert^{2} = \int_{\X} \left\lvert \int_{\X} K(x, y) f(y) \dmu(y) \right\rvert^2 \dmu(x) \leq \norm{f}^2 \int_{\X \times \X} \left\lvert K(x,y) \right\rvert^2 \dmu(x) \dmu(y) $$&lt;/p&gt;
&lt;p&gt;where we have applied Schwarz inequality (Theorem-1) as follows
$$ \left\lvert \int_{\X} K(x, y) f(y) \dmu(y) \right\rvert^2 = \left\lvert T_K(f)(x) \right\rvert^2 = \left\lvert \inner{K(x, \cdot)}{f} \right\rvert^2 \leq \norm{K(x, \cdot)}^2 \norm{f}^2$$&lt;/p&gt;
&lt;p&gt;Operators of this type are known as &lt;em&gt;Hilbert-Schmidt operators&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We are now ready to state the Mercer&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 5 [Mercer&amp;rsquo;s theorem]:&lt;/strong&gt; Suppose that $\X$ is a compact metric space, and $K : \X \times \X \to \C$ is a Mercer&amp;rsquo;s kernel that satisfies the Hilbert-Schmidt condition.
Then there exists an at most countable set of eigenfunctions $ (e_{i})_{i} $ for $ T_K $ that form an orthonormal basis of $L^2(\mu)$, and a corresponding set of non-negative eigenvalues $ (\lambda_{i})_{i} $ such that&lt;/p&gt;
&lt;p&gt;$$ T_K(e_i) = \lambda_i e_i, \quad (i \in \N) $$&lt;/p&gt;
&lt;p&gt;Moreover, $K$ has the expansion&lt;/p&gt;
&lt;p&gt;$$ K(x,y) = \sum_{i} \lambda_i e_i(x) e_i(y), \quad (x,y \in \X) $$&lt;/p&gt;
&lt;p&gt;where the convergence of the series above holds absolutely and uniformly.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof.&lt;/p&gt;
&lt;p&gt;Among other things, Mercer&amp;rsquo;s theorem provides a framework for embedding an element of $\X$ into an element of $\ell^2(\N)$ (for its definition, see Example-4 in the section on Hilbert spaces in the 
&lt;a href=&#34;http://makkar.github.io/post/kernels/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first part&lt;/a&gt;). More concretely, given the eigenfunctions and eigenvalues guaranteed by Mercer&amp;rsquo;s theorem, we may define a mapping $\Phi : \X \to \ell^2(\N)$ as follows&lt;/p&gt;
&lt;p&gt;$$ x \mapsto \left( \sqrt{\lambda_i} e_i(x) \right)_{i \in \N} $$&lt;/p&gt;
&lt;p&gt;Therefore, we have&lt;/p&gt;
&lt;p&gt;$$\inner{\Phi(x)}{\Phi(y)} = \sum_{i=1}^{\infty} \lambda_i e_i(x) e_i(y) = K(x,y) $$&lt;/p&gt;
&lt;p&gt;This is the well-known &amp;ldquo;kernel trick&amp;rdquo;. Let us connect Mercer&amp;rsquo;s theorem to the Spectral theorem.&lt;/p&gt;
&lt;p&gt;Let $\X = [d] := \{1, 2, \ldots, d\}$ along with the Hamming metric be our compact metric space. Let $\mu(\{i\}) = 1$ for all $i \in [d]$ be the counting measure on $\X$. Any function $f : \X \to \C$ is equivalent to the $d$-dimensional vector $[f(1), \ldots, f(d)]$, and any kernel function $K : \X \times \X \to \C$ is continuous, satisfies the Hilbert-Schmidt condition, and is equivalent to a $d \times d$ normal matrix $\matr{K}$ where $\matr{K}_{i,j} = K(i, j)$. The Hilbert-Schmidt operator reduces to&lt;/p&gt;
&lt;p&gt;$$ T_K(f)(x) = \int_{\X} K(x, y) f(y) \dmu(y) = \sum_{i=1}^{d} K(x,y) f(y) $$&lt;/p&gt;
&lt;p&gt;Mercer&amp;rsquo;s theorem then states that there exists a set of eigenfunctions $v_1, \ldots, v_d$ (for our $\X$ they are equivalent to vectors) and the corresponding eigenvalues $\lambda_1, \ldots, \lambda_d$ such that&lt;/p&gt;
&lt;p&gt;$$ \matr{K} = \sum_{i=1}^{d} \lambda_i v_i v_i^T $$&lt;/p&gt;
&lt;p&gt;which is exactly the spectral theorem.&lt;/p&gt;
&lt;h1 id=&#34;operations-on-kernels&#34;&gt;Operations on kernels&lt;/h1&gt;
&lt;p&gt;Let us now consider how various algebraic operations on kernels affect the corresponding Hilbert spaces. All this (and a lot more) can be found in the seminal paper by Aronszajn &amp;ldquo;Theory of Reproducing Kernels&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;I state the following theorems without proof to illustrate how operations on kernels are done.&lt;/p&gt;
&lt;h2 id=&#34;sums-of-kernels&#34;&gt;Sums of kernels&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem 6:&lt;/strong&gt; Suppose that $\H_1$ and $\H_2$ are both RKHSs with kernels $K_1$ and $K_2$, respectively. Then the space&lt;/p&gt;
&lt;p&gt;$$\H = \H_1 + \H_2 := \{f_1 + f_2 \, : \, f_1 \in \H_1 \text{ and } f_2 \in \H_2 \}$$&lt;/p&gt;
&lt;p&gt;with the norm&lt;/p&gt;
&lt;p&gt;$$ \norm{f}^{2}_{\H} := \inf \left\{ \lVert f_1 \rVert^{2} _ {\H_1} + \norm{f_2}^{2} _{\H_2} \, : \, f = f_1 + f_2, f_1 \in \H_1, f_2 \in \H_2 \right\} $$&lt;/p&gt;
&lt;p&gt;is an RKHS with the kernel $K = K_1 + K_2$.&lt;/p&gt;
&lt;h2 id=&#34;products-of-kernels&#34;&gt;Products of kernels&lt;/h2&gt;
&lt;p&gt;Let us first define the notion of tensor product of two (separable) Hilbert spaces $\H_1$ and $\H_2$ of functions, say with domains $\X_1$ and $\X_2$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 27:&lt;/strong&gt; Consider the set of functions $h : \X_1 \times \X_2 \to \C$ satisfying&lt;/p&gt;
&lt;p&gt;$$\H = \left\{ h = \sum_{i=1}^{n} u_i v_i \, : \, n \in \N \text{ and } u_i \in \H_1, v_i \in \H_2 \text{ for all } i \in [n] \right\} $$&lt;/p&gt;
&lt;p&gt;We define an inner product on $\H$ as follows: for $h = \sum_{i=1}^{n} u_i v_i$ and $g = \sum_{j=1}^{m} w_j x_j$ in $\H$ define&lt;/p&gt;
&lt;p&gt;$$ \inner{h}{g} := \sum_{i=1}^{n} \sum_{j=1}^{m} \inner{u_i}{w_j}_{\H_1} \inner{v_i}{x_j}_{\H_2} $$&lt;/p&gt;
&lt;p&gt;Then $\H$ is a Hilbert space and is called the &lt;em&gt;tensor product&lt;/em&gt; of $\H_1$ and $\H_2$. We denote it by $\H = \H_1 \otimes \H_2$.&lt;/p&gt;
&lt;p&gt;We can now state the theorem for product of kernels.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 7:&lt;/strong&gt; Suppose that $\H_1$ and $\H_2$ are RKHSs of real-valued functions with domains $\X_1$ and $\X_2$, and equipped with kernels $K_1$ and $K_2$, respectively. Then the tensor product space $\H = \H_1 \otimes \H_2$ is an RKHS of functions with domain $\X_1 \times \X_2$, and with kernel function $K : (\X_1 \times \X_2) \times (\X_1 \times \X_2) \to \C$ defined by&lt;/p&gt;
&lt;p&gt;$$ K((x,s), (y,t)) := K_1(x,y) K_2(s,t) $$&lt;/p&gt;
&lt;p&gt;$K$ is called the tensor product of the kernels $K_1$ and $K_2$, and denoted by $K = K_1 \otimes K_2$.&lt;/p&gt;
&lt;h2 id=&#34;other-operations&#34;&gt;Other operations&lt;/h2&gt;
&lt;p&gt;We can similarly define more operations on kernels:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If $K$ is a valid kernel and $\alpha \geq 0$, then $\alpha K$ is a valid kernel.&lt;/li&gt;
&lt;li&gt;If $K$ is a valid kernel and $\alpha \geq 0$, then $K + \alpha$ is a valid kernel.&lt;/li&gt;
&lt;li&gt;We can easily see from all these results that a linear combination or more generally for any polynomial $P$ with positive coefficients, the composition $P \circ K$ is a valid kernel if $K$ is a valid kernel.&lt;/li&gt;
&lt;li&gt;If $K$ is a valid kernel, then $\exp(K)$ is a valid kernel.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;representer-theorem&#34;&gt;Representer theorem&lt;/h1&gt;
&lt;p&gt;We are now at a stage where we can put all this theory to use in machine learning. Specifically we will develop Representer theorem which allows many optimization problems over the RKHS to be reduced to relatively simple calculations involving the gram matrix.&lt;/p&gt;
&lt;p&gt;Let us start with a functional analytic viewpoint of supervised learning. Suppose we are given empirical data&lt;/p&gt;
&lt;p&gt;$$ (x_1, y_1), \ldots, (x_n, y_n) \in \X \times \Y $$&lt;/p&gt;
&lt;p&gt;where $\X$ is a nonempty set. For now let $\Y = \R$. They are from an unknown function, $g : \X \to \R$, i.e., we assume&lt;/p&gt;
&lt;p&gt;$$ y_i = g(x_i), \quad (i \in [n]) $$&lt;/p&gt;
&lt;p&gt;We need to find some function $f^*$ which &amp;ldquo;best&amp;rdquo; approximates $g$. A natural way to formalize the notion of &amp;ldquo;best&amp;rdquo; is to limit ourselves to an RKHS $\H$ which contains functions of the form $f : \X \to \R$ and choose&lt;/p&gt;
&lt;p&gt;$$ f^* = \argmin_{f \in \H} \norm{f} \quad \text{ such that } f^*(x_i) = y_i \text{ for } i \in [n]$$&lt;/p&gt;
&lt;p&gt;This optimization problem is feasible whenever there exists at least one function $f \in \H$ that fits the data exactly. Denote by $y$ the vector $[y_1, \ldots, y_n]^T$. It can be shown that if $\matr{K}$ is the gram matrix of the kernel $K$ with respect to $x_1, \ldots, x_n$, then the feasibility is equivalent to $y \in \text{range}(\matr{K})$. This is a special of the representer theorem.&lt;/p&gt;
&lt;p&gt;In a realistic setting we assume that we have noisy observations, i.e.,&lt;/p&gt;
&lt;p&gt;$$y_i = g(x_i) + \e_i, \quad (i \in [n])$$&lt;/p&gt;
&lt;p&gt;where $\e_i$&#39;s denote the noise. Then the constraint of exact fitting is no longer desirable, and we model the &amp;ldquo;best&amp;rdquo; approximation by introducing a loss function which represents how close our approximation is to the observed outputs. More concretely, let $L_y : \R^n \to \R$ be a continuous function. Then we can define our cost function as&lt;/p&gt;
&lt;p&gt;$$ J(f) = \norm{f}^2 + L_y(f(x_1), \ldots, f(x_n)) $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 8 [Representer theorem]:&lt;/strong&gt; If $f^*$ is a function such that&lt;/p&gt;
&lt;p&gt;$$ J(f^*) = \inf_{f \in \H} J(f) $$&lt;/p&gt;
&lt;p&gt;then $f^*$ is in the span of the functions $k_{x_1}, \ldots, k_{x_n}$, i.e.,&lt;/p&gt;
&lt;p&gt;$$ f^*(\cdot) = \sum_{i=1}^{n} \alpha_i k_{x_i}(\cdot) \quad \text{for some } \alpha_1, \ldots, \alpha_n \in \C$$&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof which can be found in Paulsen and Raghupathi&amp;rsquo;s An Introduction to the Theory of Reproducing Kernel Hilbert Spaces or in SchÃ¶lkopf, Herbrich and Smola&amp;rsquo;s A Generalized Representer Theorem.&lt;/p&gt;
&lt;p&gt;As an example $L$ could be the squared loss $L_y(f(x_1), \ldots, f(x_n)) = \sum_{i=1}^n (y_i - f(x_i))^2$. If we assume $L$ to be convex then the solution exists and is unique. Recall that $E \subseteq \R^k$ is a &lt;em&gt;convex set&lt;/em&gt; if $\lambda x + (1-\lambda) y \in E$ whenever $x,y \in E$ and $0 &amp;lt; \lambda &amp;lt; 1$. A real-valued function $L$ defined on a convex set $E$ is a &lt;em&gt;convex function&lt;/em&gt; if $L(\lambda x + (1-\lambda) y) \leq \lambda L(x) + (1-\lambda) L(y)$ whenever $x, y \in E$ and $0 &amp;lt; \lambda &amp;lt; 1$. Note that a convex function is continuous.&lt;/p&gt;
&lt;p&gt;The Representer theorem allows us to reduce the infinite-dimensional problem of optimizing over an RKHS to an $n$-dimensional problem.&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;I barely scratched the surface of reproducing kernel Hilbert space theory. Some resources I recommend that do a much better job than me in explaining this theory and which I used as reference are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Paulsen V and Raghupathi M: An Introduction to the Theory of Reproducing Kernel Hilbert Spaces&lt;/li&gt;
&lt;li&gt;Wainwright M: High-Dimensional Statistics&lt;/li&gt;
&lt;li&gt;SchÃ¶lkopf B, Herbrich R and Smola A.J: A Generalized Representer Theorem (COLT 2001)&lt;/li&gt;
&lt;li&gt;Aronszajn N: Theory of Reproducing Kernels (1950)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the 
&lt;a href=&#34;https://makkar.github.io/post/kernels2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next article&lt;/a&gt; I will discuss kernel approximation methods. In particular I will focus on random Fourier Features developed by Rahimi and Recht which I am using in my work.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Kernels - Part 0</title>
      <link>https://makkar.github.io/post/kernels/</link>
      <pubDate>Fri, 03 Jul 2020 21:35:10 -0400</pubDate>
      <guid>https://makkar.github.io/post/kernels/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\H}{\mathcal{H}}
\newcommand{\e}{\epsilon}
\newcommand{\limn}{\lim_{n \to \infty}}
\newcommand\inner[2]{\langle #1, #2 \rangle}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\conj}[1]{\overline{#1}}
\DeclareMathOperator{\dx}{dx}
\DeclareMathOperator{\dmu}{d\mu}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The word &amp;ldquo;kernel&amp;rdquo; is 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kernel&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;heavily overloaded&lt;/a&gt;, but for our purposes it is, intuitively, a similarity measure that can be thought of as an inner product in some feature space. Kernel methods provide an elegant, theoretically well-founded, and powerful approach to solving many learning problems.&lt;/p&gt;
&lt;p&gt;We usually have the following framework: The input space $\X$ which contains our observations/inputs/features is either not rich enough (for example, if there is no linear boundary separating the two classes in a binary classification problem) or not convenient (for example, if our inputs are strings), and therefore we want to work is some other space we call feature space $\H$. Suppose we have a map which takes our inputs from $\X$ to $\H$:
$$ \Phi : \X \to \H $$
Then the class of kernels we are interested in are those for which is it possible to write
$$ K(x,y) = \langle \Phi(x), \Phi(y) \rangle, \quad x,y \in \X $$
What kind of functions, $K : \H \times \H \to \C$ admit such a representation? We aim to be able to answer this question and many others in this series of articles on kernels.&lt;/p&gt;
&lt;p&gt;In this article, I aim to introduce the necessary concepts from analysis, linear algebra, and functional analysis so as to understand Reproducing Kernel Hilbert Spaces (RKHS) and a few of their basic properties. In the 
&lt;a href=&#34;http://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next article&lt;/a&gt; I will discuss the kernel trick and some important theorems like Mercer&amp;rsquo;s theorem and Representer theorem.&lt;/p&gt;
&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;p&gt;A topic like this requires quite a bit of background before we can get to the interesting results like the one above. It&amp;rsquo;s always easy to get lazy and assume all the necessary background from the reader and get straight to the meat, but I want to write an article which I would have found useful had I had it when I started learning about kernel theory. With that being said, I am under no illusion and believe that a much better way to learn this background would be to read a functional analysis text if you have the time.&lt;/p&gt;
&lt;h2 id=&#34;linear-spaces&#34;&gt;Linear spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; A &lt;em&gt;linear space&lt;/em&gt;, or alternatively a &lt;em&gt;vector space&lt;/em&gt;, over a field $\F$ ($\F$ is $\R$ or $\C$ for our purposes) is a set $V$ of elements called &lt;em&gt;vectors&lt;/em&gt; (the elements of $\F$ are called &lt;em&gt;scalars&lt;/em&gt;) satisfying:&lt;/p&gt;
&lt;p&gt;(A) To every pair, $x$ and $y$, of vectors in $V$ there corresponds a vector $x+y$, called the sum of $x$ and $y$, in such a way that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;addition is commutative, $x+y = y+x$,&lt;/li&gt;
&lt;li&gt;addition is associative, $x+(y+z) = (x+y)+z$,&lt;/li&gt;
&lt;li&gt;there exists in $V$ a unique vector $0$ such that $x+0=x$ for every vector $x$, and&lt;/li&gt;
&lt;li&gt;to every vector $x \in V$ there corresponds a unique vector $-x$ such that $x+(-x)=0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(B) To every pair, $\alpha \in \F$ and $x \in V$, there corresponds a vector $\alpha x \in V$, called the product of $\alpha$ and $x$, in such a way that&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;multiplication by scalars is associative, $\alpha(\beta x) = (\alpha \beta)x$, and&lt;/li&gt;
&lt;li&gt;$1x = x$ for every vector $x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(C) Finally the distributive properties&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\alpha(x+y) = \alpha x + \alpha y$, and&lt;/li&gt;
&lt;li&gt;$(\alpha + \beta) x = \alpha x + \beta x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;The Euclidean spaces $\R^n$ are vector spaces over the real field.&lt;/li&gt;
&lt;li&gt;$\C^n$ are vector spaces over $\C$.&lt;/li&gt;
&lt;li&gt;The set of all polynomials, with complex coefficients, in a variable $t$ is a vector space over $\C$.&lt;/li&gt;
&lt;li&gt;The set $C$ of all continuous complex functions on the unit interval $[0,1]$ is a vector space over $\C$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; A &lt;em&gt;linear transformation&lt;/em&gt; of a linear space $V$ into a linear space $W$ is a mapping $T: V \to W$ such that
$$ T(\alpha x + \beta y) = \alpha T(x) + \beta T(y), \quad  (x,y \in V;\; \alpha, \beta \in \F) $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3:&lt;/strong&gt; In the special case in which $W$ above is a field, $T$ is called a &lt;em&gt;linear functional&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note that we often write $Tx$ instead of $T(x)$, if $T$ is linear, hinting that a linear transformation mapping a finite dimensional vector to another finite dimensional vector space is equivalent to a matrix vector product.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 4:&lt;/strong&gt; Let $\mu$ be a positive measure on an arbitrary measurable space $X$. We define $L^1(\mu)$ to be the collection of all complex measurable functions $f$ on $X$ for which
$$ \int_X |f| \dmu &amp;lt; \infty $$&lt;/p&gt;
&lt;p&gt;It can be shown that for every $f, g \in L^1(\mu)$ and for every $\alpha, \beta \in \C$, we have $\alpha f + \beta g \in L^1(\mu)$, and
$$ \int_X (\alpha f + \beta g) \dmu = \alpha \int_X f \dmu + \beta \int_X g \dmu $$&lt;/p&gt;
&lt;p&gt;Thus, $L^1(\mu)$ is a vector space, and the mapping $F: L^1(\mu) \to \R$ defined by
$$ F(f) = \int_X |f| \dmu, \quad f \in L^1(\mu) $$
is a linear functional.&lt;/p&gt;
&lt;h2 id=&#34;inner-products-and-norms&#34;&gt;Inner products and norms&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition 5:&lt;/strong&gt; If $V$ be a linear space over $\C$, an &lt;em&gt;inner product&lt;/em&gt; on $V$ is a function $\langle \cdot , \cdot \rangle: V \times V \to \C$ such that for all $\alpha, \beta \in \C$, and all $x,y,z \in V$, the following are satisfied:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Linearity in the first argument: $\langle \alpha x + \beta y, z \rangle = \alpha \langle x,z \rangle + \beta \langle y,z \rangle$,&lt;/li&gt;
&lt;li&gt;Conjugate symmetry: $\inner{x}{y} = \conj{\inner{y}{x}}$,&lt;/li&gt;
&lt;li&gt;Positivity: $\inner{x}{x} \geq 0$,&lt;/li&gt;
&lt;li&gt;If $\inner{x}{x} = 0$, then $x=0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A function satisfying only the first three properties is called a &lt;em&gt;semi-inner product&lt;/em&gt; on $V$.&lt;/p&gt;
&lt;p&gt;An immediate consequences of this definition: For every $y \in V$, the mapping $F: V \to \C$ defined by
$$ F(x) = \inner{x}{y}, \quad (x \in V) $$
is a linear functional on $V$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 6:&lt;/strong&gt; If $V$ be a linear space over $\C$, a &lt;em&gt;norm&lt;/em&gt; on $V$ is a non-negative function $\norm{\cdot} : V \to \R$ such that for all $\alpha \in \C$, and all $x,y \in V$, the following are satisfied:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Subadditivity: $\norm{x+y} \leq \norm{x} + \norm{y}$,&lt;/li&gt;
&lt;li&gt;Absolutely homogenous: $\norm{\alpha x} = |\alpha| \norm{x}$,&lt;/li&gt;
&lt;li&gt;Positive definite: $\norm{x} = 0 \implies x = 0$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given an inner product, we can define a &lt;em&gt;norm&lt;/em&gt; as follows:
$$ \norm{x} = \sqrt{\inner{x}{x}} $$&lt;/p&gt;
&lt;p&gt;A classic result useful in many proofs:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1:&lt;/strong&gt; Schwarz inequality
$$|\inner{x}{y}| \leq \norm{x} \, \lVert y \rVert$$
Equality hold for $y = \alpha x$ or $y=0$.&lt;/p&gt;
&lt;p&gt;The proof is not too difficult.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 7:&lt;/strong&gt; The virtue of norm on a vector space $V$ is that
$$ d(x,y) := \norm{ x-y } $$
defines a &lt;em&gt;metric&lt;/em&gt; on $V$ so that $V$ becomes a metric space.&lt;/p&gt;
&lt;p&gt;Note that technically the tuple $(V, d)$ defines a metric space, but I will often write just $V$ instead of $(V, d)$ when it&amp;rsquo;s clear from context what the metric $d$ is.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 8:&lt;/strong&gt; If $0 &amp;lt; p &amp;lt; \infty$, $f$ is a complex measurable function on $X$, and $\mu$ is a nonnegative measure on $X$, define
$$ \norm{f}_p := \left( \int_X |f|^p \dmu \right)^{1/p} $$
and let $L^p(\mu)$ consist of all $f$ for which $\norm{f}_p &amp;lt; \infty$. We call $\norm{f}_p$ the $L^p$-norm of $f$.&lt;/p&gt;
&lt;h2 id=&#34;hilbert-spaces&#34;&gt;Hilbert spaces&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition 9:&lt;/strong&gt; An &lt;em&gt;inner product space&lt;/em&gt;, or alternatively a &lt;em&gt;pre-Hilbert space&lt;/em&gt;, is a linear space with an inner product defined on it.&lt;/p&gt;
&lt;p&gt;We need the concept of completeness to define Hilbert space. But before that let me define Cauchy sequences.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 10:&lt;/strong&gt; Given a metric space $(M, d)$, a sequence $(x_n)_{n \in \N}$ of elements in $M$ is called a &lt;em&gt;Cauchy sequence&lt;/em&gt; if for every positive real number $\e &amp;gt; 0$ there exists a positive integer $N \in \N$ such that $m, n &amp;gt; N$ implies that $d(x_m, x_n) &amp;lt; \e$.&lt;/p&gt;
&lt;p&gt;Recall that we say a sequence $(x_n)_{n \in \N}$ in a metric space $(M, d)$ &lt;em&gt;converges&lt;/em&gt; if there exists a point $x \in M$ with the following property: for every $\e &amp;gt; 0$ there exists a positive integer $N \in \N$ such that $n &amp;gt; N$ implies that $d(x_n, x) &amp;lt; \e$.&lt;/p&gt;
&lt;p&gt;It can be shown that every convergent sequence is a Cauchy sequence: Let the sequence $(x_n)_{n \in \N}$ in a metric space $(M, d)$ converge to $x \in M$. If $\e &amp;gt; 0$, there is an integer $N \in \N$ such that $d(x_n, x) &amp;lt; \e$ for all $n &amp;gt; N$. Hence&lt;/p&gt;
&lt;p&gt;$$d(x_n, x_m) \leq d(x_n, x) + d(x, x_m) &amp;lt; 2\e$$&lt;/p&gt;
&lt;p&gt;if $n,m &amp;gt; N$. Thus $(x_n)_{n \in \N}$ is a Cauchy sequence.&lt;/p&gt;
&lt;p&gt;The converse is not necessarily true. But if it holds in some space, we anoint the space with a special name.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 11:&lt;/strong&gt; A metric space $(M, d)$ is called &lt;em&gt;complete&lt;/em&gt; if every Cauchy sequence of points in $M$ has a limit that is also in $M$ or, alternatively, if every Cauchy sequence in $M$ converges in $M$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 12:&lt;/strong&gt; A pre-Hilbert space $\H$ is called a &lt;em&gt;Hilbert space&lt;/em&gt; if it is complete in the metric $d$ (see Definition 7).&lt;/p&gt;
&lt;h3 id=&#34;examples-1&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The sets $\R^n$ and $\C^n$ are Hilbert spaces if we define $\inner{x}{y} := \sum_{i=1}^n x_i \conj{y_i}$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The set $C$ of all continuous complex functions on the unit interval $[0,1]$ defined above is an inner product space if we define
$$ \inner{f}{g} := \int_0^1 f(x) \conj{g(x)} \dx $$
but is not a Hilbert space.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$L^2(\mu)$ is a Hilbert space, with inner product
$$ \inner{f}{g} := \int_X f \, \conj{g} \dmu $$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The space of square-summable real-valued sequences, namely
$$ \ell^2(\N) := \left\{ (x_n)_{n \in \N} \; : \; x_n \in \R,\, \sum_n x_n^2 &amp;lt; \infty \right\} $$&lt;/p&gt;
&lt;p&gt;This set, when endowed with the inner product $\inner{x}{y} := \sum_{n \in \N} x_n y_n$, defines a Hilbert space. It will play an important role in our discussion of eigenfunctions for Reproducing Kernel Hilbert spaces.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Definition 13:&lt;/strong&gt; Consider a linear space $\FF$ of functions each of which is a mapping from a set $X$ into $\F$. For $x \in X$, a &lt;em&gt;linear evaluation functional&lt;/em&gt; is a linear functional $E_x$ that is defined as
$$ E_x(f) = f(x), \quad (f \in \FF) $$
In other words, a linear evaluation functional with respect to $x \in X$ evaluates each function at $x$.&lt;/p&gt;
&lt;p&gt;In general, the evaluation functional is not continuous. This means we can have $f_n \to f$ but $E_x(f_n)$ does not converge to $E_x(f)$. Intuitively, this is because Hilbert spaces can contain very unsmooth functions. We will later consider a special type of Hilbert space, Reproducing Kernel Hilbert Space where evaluation functional is continuous.&lt;/p&gt;
&lt;p&gt;A lemma that will be useful later on:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt; Let $\H$ be a Hilbert space and $L:\H \to \F$ a linear functional. The following statements are equivalent:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$L$ is continuous.&lt;/li&gt;
&lt;li&gt;$L$ is continuous at $0$.&lt;/li&gt;
&lt;li&gt;$L$ is continuous at some point.&lt;/li&gt;
&lt;li&gt;$L$ is bounded, i.e., there is a constant $c &amp;gt; 0$ such that $|L(f)| \leq c \norm{f}$ for every $f \in \H$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; It is clear that $(1) \implies (2) \implies (3)$, and $(4) \implies (2)$. Let&amp;rsquo;s show that $(3) \implies (1)$, and $(2) \implies (4)$.&lt;/p&gt;
&lt;p&gt;$(3) \implies (1)$: Suppose $L$ is continuous at $f$ and $g$ is any point in $\H$. If $g_n \to g$ in $\H$, then $g_n - g + f \to f$. By assumption $L(f) = \limn L(g_n - g + f) = \limn L(g_n) - L(g) + L(f)$. Hence $L(g) = \limn L(g_n)$.&lt;/p&gt;
&lt;p&gt;$(2) \implies (4)$: The definition of continuity at $0$ implies that $L^{-1}(\{\alpha \in \F : |\alpha| &amp;lt; 1\})$ contains an open ball centered at $0$. Let $\delta &amp;gt; 0$ be the radius of that open ball centered at $0$. Then for $f \in \H$ and $\norm{f} &amp;lt; \delta$ we have $|L(f)| &amp;lt; 1$. If $f$ is an arbitrary element of $\H$ and $\e &amp;gt; 0$, then
$$ \left\lVert \frac{\delta f}{\norm{f} + \e} \right\rVert &amp;lt; \delta $$
Hence,
$$ 1 &amp;gt; \left\lvert L\left( \frac{\delta f}{\norm{f} + \e} \right) \right\rvert = \frac{\delta }{\norm{f} + \e} |L(f)| $$
Letting $\e \to 0$ we see that $(4)$ holds with $c = 1/\delta$. $\square$&lt;/p&gt;
&lt;h3 id=&#34;orthonormal-bases&#34;&gt;Orthonormal bases&lt;/h3&gt;
&lt;p&gt;We generalize the idea of orthonormal basis to infinite dimensional case. This will be needed when we discuss Mercer&amp;rsquo;s theorem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 14:&lt;/strong&gt; A collection of vectors $\{v_{\alpha} \, : \, \alpha \in A \}$ in a Hilbert space $\H$ for some index set $A$ is called &lt;em&gt;orthonormal&lt;/em&gt; if it satisfies $\inner{v_{\alpha}}{v_{\beta}} = \delta_{\alpha \beta}$ where $\delta_{\alpha \beta}$ is the Kronecker delta, which equals $1$ if $\alpha = \beta$ and $0$ otherwise.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 15:&lt;/strong&gt; A collection of vectors $\{v_{\alpha} \, : \, \alpha \in A \}$ in a Hilbert space $\H$ is &lt;em&gt;complete&lt;/em&gt; if for any $u \in \H$, $\inner{u}{v_{\alpha}} = 0$ for all $\alpha \in A$ implies that $u = 0$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 16:&lt;/strong&gt; An &lt;em&gt;orthonormal basis&lt;/em&gt; a complete orthonormal system.&lt;/p&gt;
&lt;p&gt;Note, we can also define an orthonormal basis as a maximal orthonormal set in $\H$. To say $\{v_{\alpha}\}$ is maximal means that no vector of $\H$ can be added to $\{v_{\alpha}\}$ in such a way that the resulting set is still orthonormal. This happens precisely when there is no $u \neq 0$ in $\H$ that is orthogonal to every $v_{\alpha}$.&lt;/p&gt;
&lt;h3 id=&#34;separable-hilbert-spaces&#34;&gt;Separable Hilbert spaces&lt;/h3&gt;
&lt;p&gt;Another idea we need is separability. Let us define that now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 17:&lt;/strong&gt; A topological space is called &lt;em&gt;separable&lt;/em&gt; if it contains a countable, dense subset; that is, there exists a sequence $(x_{n})_{n=1}^{\infty }$ of elements of the space such that every nonempty open subset of the space contains at least one element of the sequence.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 18:&lt;/strong&gt; A Hilbert space is separable if and only if it has a countable orthonormal basis. It follows that any separable, infinite-dimensional Hilbert space is isometric to the space $\ell^2(\N)$ of square-summable sequences.&lt;/p&gt;
&lt;p&gt;We will be dealing with separable Hilbert spaces in our discussion.&lt;/p&gt;
&lt;h2 id=&#34;riesz-representation-theorem&#34;&gt;Riesz representation theorem&lt;/h2&gt;
&lt;p&gt;We now come to a very important theorem called the Riesz representation theorem. The name Riesz has 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Riesz_theorem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many theorems&lt;/a&gt; attached to it, but the one relevant to us is the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2:&lt;/strong&gt; For each continuous linear functional $L$ on a Hilbert space $\H$, there exists a unique $g \in \H$ such that
$$L(f) = \inner{f}{g}\, , \quad (f \in \H) $$&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll skip the proof as it&amp;rsquo;s not easy and will unnecessarily make this article abstruse.&lt;/p&gt;
&lt;p&gt;Side-note: In the mathematical treatment of quantum mechanics, this theorem can be seen as a justification for the popular braâ€“ket notation.&lt;/p&gt;
&lt;h1 id=&#34;reproducing-kernel-hilbert-spaces-rkhs&#34;&gt;Reproducing Kernel Hilbert Spaces (RKHS)&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Definition 19:&lt;/strong&gt; Let $\X$ be a set. We will call a set $\H$ of functions from $\X$ to $\F$ a &lt;em&gt;Reproducing Kernel Hilbert Space (RKHS)&lt;/em&gt; on $\X$ if&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\H$ is a vector space,&lt;/li&gt;
&lt;li&gt;$\H$ is endowed with an inner product, $\inner{\cdot}{\cdot}$, with respect to which $\H$ is a Hilbert space,&lt;/li&gt;
&lt;li&gt;for every $x \in \X$, the linear evaluation functional $E_x : \H \to \F$, is bounded (or continuous, as seen from Lemma-1).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If $\H$ is an RKHS on $\X$, then an application of the Riesz representation theorem shows that the linear evaluation functional is given by the inner product with a unique vector in $\H$. Therefore, for each $x \in \X$, there exists a unique vector $k_x \in \H$, such that for every $f \in \H$,
$$ f(x) = E_x(f) = \inner{f}{k_x} $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 20:&lt;/strong&gt; The function $k_x$ is called the &lt;em&gt;reproducing kernel for the point $x$&lt;/em&gt;. The function $K: \X \times \X \to \F$ defined by
$$ K(x,y) = k_y(x) $$
is called the &lt;em&gt;reproducing kernel for $\H$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Note that we have
$$ K(x,y) = k_y(x) = \inner{k_y}{k_x} = \conj{\inner{k_x}{k_y}} = \conj{K(y,x)}$$&lt;/p&gt;
&lt;p&gt;Also,
$$ \norm{E_y}^2 = \norm{k_y}^2 = \inner{k_y}{k_y} = K(y,y) $$&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;p&gt;The first question that comes to mind is if any Reproducing Kernel Hilbert Spaces exist. The following example answers this question in the affirmative.&lt;/p&gt;
&lt;p&gt;We saw before that $\C^n$ is a Hilbert space. We can show that $\C^n$ is in fact an RKHS. Let $\X = \{1, 2, \ldots, n\}$, then we can view $v \in \C$ as a function $V : \X \to \C$, where $V(j) = v_j$. The linear evaluation functionals are of course bounded for every $x \in \X$ and we have
$$ V(j) = v_j = \inner{V}{e_j}\,, \quad (j \in \X) $$
where $e_j$ is a vector with $1$ at $j$&lt;sup&gt;th&lt;/sup&gt; position and $0$ everywhere else. Therefore, the reproducing kernel for the point $x \in \X$ is $e_x$ and the reproducing kernel can be thought as the identity matrix.&lt;/p&gt;
&lt;p&gt;Can there be multiple reproducing kernels for an RKHS? The following theorem answers this question.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3:&lt;/strong&gt; If an RKHS $\H$ of functions on a set $\X$ admits a reproducing kernel, $K$, then  $K$ is uniquely determined by $\H$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Suppose that there exists another reproducing kernel $K&#39;$ for $\H$. Then
$$
\norm{k_y - k&amp;rsquo;_y} = \inner{k_y - k&amp;rsquo;_y}{k_y - k&amp;rsquo;_y} = \inner{k_y - k&amp;rsquo;_y}{k_y} - \inner{k_y - k&amp;rsquo;_y}{k&amp;rsquo;_y} = (k_y - k&amp;rsquo;_y)(y) - (k_y - k&amp;rsquo;_y)(y) = 0
$$
for any $y \in \X$. In other words, $k_y(x) = k&amp;rsquo;_y(x)$ for every $x \in \X$ by the positive definite property of norms and hence the kernel is unique. $\square$&lt;/p&gt;
&lt;h1 id=&#34;epilogue&#34;&gt;Epilogue&lt;/h1&gt;
&lt;p&gt;We covered quite a lot of ground in this blog post but I didn&amp;rsquo;t even define a kernel as we commonly use in machine learning!
In the 
&lt;a href=&#34;http://makkar.github.io/post/kernels1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next post&lt;/a&gt; I will do that and cover its fundamental properties.&lt;/p&gt;
&lt;p&gt;Some resources I recommend to go into more depth on what&amp;rsquo;s covered here are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Halmos, P: Finite-Dimensional Vector Spaces.&lt;/li&gt;
&lt;li&gt;Rudin, W: Real and Complex Analysis. (Chapter - 4)&lt;/li&gt;
&lt;li&gt;Rudin, W: Functional Analysis.&lt;/li&gt;
&lt;li&gt;Conway, J: A course in functional analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Partitions and Equivalence Relations</title>
      <link>https://makkar.github.io/post/equivalence-relations/</link>
      <pubDate>Sat, 20 Jun 2020 19:16:45 -0400</pubDate>
      <guid>https://makkar.github.io/post/equivalence-relations/</guid>
      <description>&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;I want to show how these two concepts are a single mathematical idea.&lt;/p&gt;
&lt;h2 id=&#34;partition&#34;&gt;Partition&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;partition&lt;/em&gt; of a non-empty set, $X$, is a disjoint class $\{X_i\}_{i \in I}$ of non-empty subsets of $X$ whose union is $X$. The $X_i$&#39;s are called the &lt;em&gt;partition sets&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;For example, if $X = \mathbb{R}$, then $X$ can be partitioned as
$$
X = \bigcup_{n \in \mathbb{Z}}[n, n+1)
$$&lt;/p&gt;
&lt;h2 id=&#34;binary-relation&#34;&gt;Binary Relation&lt;/h2&gt;
&lt;p&gt;A &lt;em&gt;binary relation&lt;/em&gt;, or simply &lt;em&gt;relation&lt;/em&gt;, $R$, in the set $X$ is a subset of $X \times X$.&lt;/p&gt;
&lt;p&gt;For $x, y \in X$, we denote the fact $(x,y) \in R$ by writing $x R y$. A function may be defined as a special kind of binary relation.&lt;/p&gt;
&lt;h2 id=&#34;equivalence-relation&#34;&gt;Equivalence relation&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s assume that a partition of our non-empty set $X$ is given, and we associate with this partition a relation, $\sim$, in $X$ defined as follows: $x \sim y$ if $x$ and $y$ belong to the same partition set. It can easily checked that the relation $\sim$ satisfies:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$x \sim x$ for every $x \in X$ (&lt;em&gt;reflexivity&lt;/em&gt;);&lt;/li&gt;
&lt;li&gt;$x \sim y \implies y \sim x$ (&lt;em&gt;symmetry&lt;/em&gt;);&lt;/li&gt;
&lt;li&gt;$x \sim y$ and $y \sim z$ $\implies$ $x \sim z$ (&lt;em&gt;transitivity&lt;/em&gt;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Any relation in $X$ which possesses these three properties is called an &lt;em&gt;equivalence relation&lt;/em&gt; in $X$.&lt;/p&gt;
&lt;h3 id=&#34;examples&#34;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Let $X = \mathbb{Z}$ and let $x \sim y$ if $2 | x-y$ for $x,y \in X$. Then clearly $\sim$ is an equivalence relation in $X$.&lt;/li&gt;
&lt;li&gt;Let $X$, $Y$ be any non-empty sets and $f$ be a mapping from $X$ onto $Y$. Let $x \sim y$ if $f(x) = f(y)$ for $x,y \in A$. This defines an equivalence relation in $X$. Indeed, $f(x) = f(x)$, and so $x \sim x$. If $f(x) = f(y)$ then $f(y) = f(x)$, and so $x \sim y \implies y \sim x$. Finally, if $f(x) = f(y)$ and $f(y) = f(z)$ then $f(x) = f(z)$, and so $x \sim y$ and $y \sim z$ $\implies$ $x \sim z$. The first example is a special case of this one if we take $X = \mathbb{Z}$, $Y = \{0,1\}$ and $f(x) = x \mod 2$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;relation-to-partition&#34;&gt;&amp;ldquo;Relation&amp;rdquo; to partition&lt;/h3&gt;
&lt;p&gt;We have just seen that each partition of $X$ has associated with it a natural equivalence relation in $X$. Let us now reverse the situation and show that a given equivalence relation in $X$ determines a natural partition of $X$.&lt;/p&gt;
&lt;p&gt;Let $\sim$ be an equivalence relation in $X$. For every $x \in X$ define the set
$$
[x] := \{ y \in X : y \sim x\}
$$
called the &lt;em&gt;equivalence set&lt;/em&gt; of $x$. We show that the class of all distinct equivalence sets forms a partition of $X$.&lt;/p&gt;
&lt;p&gt;By reflexivity, $x \in [x]$ for every $x \in X$, and thus each equivalence set is non-empty and their union is $X$. We now need to show that any two equivalence sets $[x_1]$ and $[x_2]$ are either disjoint or identical. We prove this by showing that if $[x_1]$ and $[x_2]$ are not disjoint then they are identical. To this end, let $z$ be a common element of $[x_1]$ and $[x_2]$. Let $y$ be any element of $[x_1]$. Using transitivity,
$$
y \sim x_1 \sim z \sim x_2
$$
Therefore, $y \in [x_2]$. Since $y$ was an arbitrary element of $[x_1]$, we get $[x_1] \subseteq [x_2]$. We can similarly show that $[x_2] \subseteq [x_1]$. In short, $[x_1] = [x_2]$.&lt;/p&gt;
&lt;p&gt;We have shown that there is no real distinction between partitions of a set and equivalence relations in the set. They are two &amp;ldquo;equivalent&amp;rdquo; approaches for the same mathematical idea. The approach we choose in an application depends entirely on our own convenience.&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Upper and Lower Limits</title>
      <link>https://makkar.github.io/post/upper-and-lower-limits/</link>
      <pubDate>Wed, 17 Jun 2020 16:33:53 -0400</pubDate>
      <guid>https://makkar.github.io/post/upper-and-lower-limits/</guid>
      <description>&lt;p&gt;$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\RR}{\overline{\mathbb{R}}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\e}{\epsilon}
\newcommand{\limm}{\lim_{n \to \infty}}
\newcommand{\lims}{\limsup_{n \to \infty}}
\newcommand{\limi}{\liminf_{n \to \infty}}
\newcommand{\sn}{\{s_n\}}
\newcommand{\snk}{\{s_{n_k}\}}
\newcommand{\supk}{\sup_{k \geq n}}
\newcommand{\infk}{\inf_{k \geq n}}
\newcommand{\sups}{\sup_{k \geq n} s_k}
\newcommand{\infs}{\inf_{k \geq n} s_k}
\newcommand{\cc}{\mathsf{c}}
$$&lt;/p&gt;
&lt;div style=&#34;text-align: justify&#34;&gt;
&lt;p&gt;Most of what follows is taken from Rudin&amp;rsquo;s Principles of Mathematical Analysis.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The concept of upper and lower limits (commonly denoted by $\limsup$ and $\liminf$ respectively) shows up routinely when discussing the limiting behaviour of a sequence. For example, consider the Big O notation, $\mathcal{O}$, defined as $f(n) = \mathcal{O}(g(n))$ iff there exists a positive real number $c$ and an integer $N$ such that for every $n \in \Z$, $n &amp;gt; N$, we have $|f(n)| \leq c g(n)$. This can be succinctly written as
$$
\lims \frac{|f(n)|}{g(n)} &amp;lt; \infty
$$&lt;/p&gt;
&lt;p&gt;In this post, I aim to expound on the concept of upper and lower limits so that the second formulation of Big O notation above becomes easier to understand than the first one.&lt;/p&gt;
&lt;h2 id=&#34;definitions&#34;&gt;Definitions&lt;/h2&gt;
&lt;p&gt;I start with some definitions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 1:&lt;/strong&gt; A &lt;em&gt;sequence&lt;/em&gt; is function defined on the set $\N$ of positive integers. If $f(n) = x_n$, for $n \in \N$, we denote the sequence $f$ by the symbol $\{x_n\}$, or sometimes by $x_1, x_2, \ldots$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 2:&lt;/strong&gt; A sequence $\{p_n\}$ in a metric space $X$ is said to &lt;em&gt;converge&lt;/em&gt; if there is a point $p \in X$ with the following property: For every $\e &amp;gt; 0$ there is an integer $N$ such that $n \geq N$ implies that $d(p_n, p) &amp;lt; \e$. We write this as $\limm p_n = p$ or as $p_n \to p$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 3:&lt;/strong&gt; Let $\{s_n\}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$. We then write $s_n \to +\infty$. Similarly for $s_n \to -\infty$.&lt;/p&gt;
&lt;p&gt;Note: The symbol $\to$ is now used for certain type of divergent sequences as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 4:&lt;/strong&gt; Given a sequence $\{p_n\}$, consider a sequence $\{n_k\}$ of positive integers, such that $n_1 &amp;lt; n_2 &amp;lt; \cdots$. Then the sequence $\{p_{n_k}\}$, which is a composition of the functions $\{n_k\}$ and $\{p_n\}$, is called a &lt;em&gt;subsequence&lt;/em&gt; of ${p_n}$. If $\{p_{n_k}\}$ converges, its limit is called a &lt;em&gt;subsequential limit&lt;/em&gt; of $\{p_n\}$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 5:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system, i.e., $x \in \RR := \R \cup \{+\infty, -\infty\}$) such that $s_{n_k} \to x$ for some subsequence $\{s_{n_k}\}$. Therefore, this set contains all the subsequential limits of $\sn$ plus possibly the numbers $+\infty$ and $-\infty$. We define
$$
s^* = \sup E
$$
$$
s_* = \inf E
$$
The numbers $s^*$ and $s_*$ are called the &lt;em&gt;upper&lt;/em&gt; and &lt;em&gt;lower limits&lt;/em&gt; of $\sn$. We use the notation
$$
\lims s_n = s^*
$$
$$
\limi s_n = s_*
$$
It immediately follows that $s_* \leq s^*$.&lt;/p&gt;
&lt;p&gt;The fact that $E$ is non-empty (and thus taking $\sup$ or $\inf$ makes sense) follows from the observation that either $\sn$ is bounded or unbounded. If it is bounded then it must contain a convergent subsequence (Bolzanoâ€“Weierstrass theorem) and thus at least one element, or if it is unbounded then it must contain either $+\infty$ or $-\infty$.&lt;/p&gt;
&lt;h2 id=&#34;some-useful-lemmas&#34;&gt;Some useful lemmas&lt;/h2&gt;
&lt;p&gt;Now let us prove some interesting lemmas that will be useful later.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1:&lt;/strong&gt; The subsequential limits of a sequence $\{p_n\}$ in a metric space $X$ form a closed subset of $X$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let $E$ be the set of all subsequential limits of $\{p_n\}$ and let $q$ be a limit point of $E$. We have to show that $q \in E$.
To show this we will construct a subsequence of $\{p_n\}$ which converges to $q$.&lt;/p&gt;
&lt;p&gt;Choose $n_1$ so that $p_{n_1} \neq q$. If no such $n_1$ exists, then $E$ has only one element, $q = p_1 = p_2 = \cdots$, and there is nothing to prove. Define $\delta = d(q, p_{n_1})$. Suppose $n_1, \ldots, n_{i-1}$ are chosen. Since $q$ is a limit point of $E$, there is an $x \in E$ with $d(q, x) &amp;lt; \frac{\delta}{2^i}$. Since $x \in E$, there is an $n_i &amp;gt; n_{i-1}$ such that $d(x, p_{n_i}) &amp;lt; \frac{\delta}{2^i}$. Thus
$$
d(q, p_{n_i}) \leq d(q, x) + d(x, p_{n_i}) &amp;lt; \frac{\delta}{2^{i-1}} \quad \text{for } i = 1,2,\ldots
$$
This implies $p_{n_k} \to q$, and thus $q \in E$. $\square$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lemma 2:&lt;/strong&gt; Let $F$ be a nonempty closed set of real numbers which is bounded above. Let $\alpha = \sup F$. Then $\alpha \in F$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Assume for the sake of the contradiction that $\alpha \notin F$. Then since $F^\cc$ is an open set (because $F$ is closed) there exists an $\e &amp;gt; 0$ such that $(\alpha - \e, \alpha + \e) \subset F^\cc$. But this implies $\alpha - \frac{\e}{2}$ is an upper bound for $F$ which is lower that $\alpha$. This gives us our required contradiction. $\square$&lt;/p&gt;
&lt;h2 id=&#34;properties&#34;&gt;Properties&lt;/h2&gt;
&lt;p&gt;We now have all the tools to prove a very useful characterization of upper and lower limits and the highlight of this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Let $E$ and $s^*$ have the same meaning as in Definition 5. Then $s^*$ has the following two properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$s^* \in E$.&lt;/li&gt;
&lt;li&gt;If $x &amp;gt; s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n &amp;lt; x$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Moreover, $s^*$ is the only number with these two properties.&lt;/p&gt;
&lt;p&gt;Of course, an analogous result is true for $s_*$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; We start by showing the two properties.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We divide it into three cases depending on what value $s^*$ takes:&lt;/p&gt;
&lt;p&gt;If $s^* = +\infty$, then $E$ is not bounded above; hence $\sn$ is not bounded above, and thus there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k} \to +\infty$. Therefore, $+\infty \in E$ and thus $s^* \in E$.&lt;/p&gt;
&lt;p&gt;If $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists by the definition of $\sup$. Therefore, $s^* \in E$ follows from the Lemmas 1 and 2, and the fact that $s^* = \sup E$.&lt;/p&gt;
&lt;p&gt;If $s^* = -\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Thus, $s^* \in E$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Suppose for the sake of contradiction that there is a number $x &amp;gt; s^*$ such that $s_n \geq x$ for infinitely many values of $n$. Let&amp;rsquo;s denote this set of $n$&#39;s with $\mathcal{K}$ and let $\{s_k\}$$_{k \in \mathcal{K}}$ be this subsequence. If $\{s_k\}_{k \in \mathcal{K}}$ is unbounded then $s^* = +\infty$ contradicting the fact that there exists an $x &amp;gt; s^*$. And if $\{s_k\}_{k \in \mathcal{K}}$ is bounded that it contains a convergent subsequence (Bolzanoâ€“Weierstrass theorem). Suppose this convergent subsequence converges to $y$. Then $y \geq x &amp;gt; s^*$. This contradicts the definition of $s^*$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To show the uniqueness, suppose there are two distinct numbers, $p$ and $q$, which satisfy the two properties, and suppose $p &amp;lt; q$. Choose $x$ such that $p &amp;lt; x &amp;lt; q$. Since $p$ satisfies the second property, we have $s_n &amp;lt; x$ for $n \geq N$. But then $q$ cannot satisfy the second property. $\square$&lt;/p&gt;
&lt;p&gt;An intuitive theorem:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2:&lt;/strong&gt; If $s_n \leq t_n$ for $n \geq N$, where $N \in \N$ is fixed, then
$$\limi s_n \leq \limi t_n,$$
$$\lims s_n \leq \lims t_n$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; Let $s^* = \lims s_n$ and $t^* = \lims t_n$. Suppose for the sake of contradiction $t^* &amp;lt; s^*$. Choose $x$ such that $t^* &amp;lt; x &amp;lt; s^*$. Then by the second property of Theorem 1 there is an integer $N_1$ such that $n \geq N_1$ implies $t_n &amp;lt; x$. Also by the first property there exists a subsequence $\snk$ such that $s_{n_k} \to s^*$. This implies that there exists an integer $N_2$ such that $n \geq N_2$ implies $x &amp;lt; s_n$. But then for $n \geq \max\{N_1, N_2\}$ we have $t_n &amp;lt; x &amp;lt; s_n$.
This gives us our required contradiction.&lt;/p&gt;
&lt;p&gt;A similar argument can be made for the $\liminf$ case. $\square$&lt;/p&gt;
&lt;p&gt;Next we give a necessary and sufficient condition for the convergence of a sequence in terms of its $\liminf$ and $\limsup$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 3:&lt;/strong&gt; For a real-valued sequence $\sn$, $\limm s_n = s \in \RR$ if and only if
$$\lims s_n = \limi s_n = s$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;
We divide the analysis into three cases.&lt;/p&gt;
&lt;p&gt;First, let $s \in \R$. Then if $\lims s_n = \limi s_n = s$, Theorem 1 implies that for any $\e &amp;gt; 0$ we have $s_n \in (s-\e, s+\e)$ for all but finitely many $n$, which means $s_n \to s$. On the other hand if $s_n \to s$ then every subsequence $\snk$ must converge to $s$ and hence $\lims s_n = \limi s_n = s$.&lt;/p&gt;
&lt;p&gt;Now let $s = +\infty$. Then $s_n \to s$, i.e., for every $M \in \R$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$ if and only $\limi s_n = +\infty$, and then $\lims s_n = +\infty$ since $\limi s_n \leq \lims s_n$.&lt;/p&gt;
&lt;p&gt;Lastly, let $s = -\infty$. Then $s_n \to s$, i.e., for every $M \in \R$ there is an integer $N$ such that $n \geq N$ implies $s_n \leq M$ if and only $\lims s_n = -\infty$, and then $\limi s_n = -\infty$ since $\limi s_n \leq \lims s_n$. $\square$&lt;/p&gt;
&lt;h2 id=&#34;upper-and-lower-limits---a-reprise&#34;&gt;Upper and lower limits - a reprise&lt;/h2&gt;
&lt;p&gt;There is an equivalent way to express upper and lower limits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Definition 6:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. We define the notation
$$\sups := \sup \{ s_k : k \geq n\}$$
$$\infs := \inf \{ s_k : k \geq n\}$$&lt;/p&gt;
&lt;p&gt;We note that the sequence $\{ \sups \}$$_{n \in \N}$ is monotonically decreasing and the sequence $\{ \infs \}$$_{n \in \N}$ is monotonically increasing, and thus their limits exist in $\RR$.&lt;/p&gt;
&lt;p&gt;We now show the equivalence of the two ways of looking at upper and lower limits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 4:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Then
$$\limm \sups = \lims s_n$$
$$\limm \infs = \limi s_n$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt;
We will prove the first equation. The proof for the second is similar. We prove the equation in two steps.&lt;/p&gt;
&lt;p&gt;Let $S = \{ s_n : n \in \N \}$. Let&amp;rsquo;s show that $\sup S = +\infty$ if and only if $\lims s_n = + \infty$. Suppose first that $\sup S = + \infty$. Then we construct a subsequence $\snk$ as follows. We let $n_1 = 1.$ Suppose $n_1, \ldots, n_{k}$ are chosen and let
$$S_k = \{ n \in \N : s_n \geq \max\{s_{n_1}, \ldots, s_{n_k}, k\} + 1 \}$$
Notice that $S_k$ is infinite as otherwise we can find an $M \in \R$ such that $s_n \leq M$ for all $n \geq 1$, contradicting the fact that $\sup S = + \infty$. We pick $n_{k+1}$ to be the smallest element of $S_k$ which is bigger than $n_k$. The resulting subsequence satisfies the condition that $s_{n_k} \geq k$ for $k \geq 2$ and thus we conclude that $s_{n_k} \to +\infty$ which gives $\lims s_n = +\infty$. Now suppose that $\lims s_n = +\infty$. From Theorem 1 we can conclude that there exists a subsequence $\snk$ such that $s_{n_k} \to +\infty$. This immediately implies $\sup S = +\infty$.&lt;/p&gt;
&lt;p&gt;For the second step, suppose $\sup S &amp;lt; + \infty$. Let
$$a_n = \sups$$
Notice that $a_1 &amp;lt; +\infty$ and $\{a_n\}$ is a monotonically decreasing sequence. Therefore, we have that either $\{a_n\}$ is lower bounded in which case it converges to, say, $a$ or it is not in which case $\limm a_n = -\infty$. Since $s_n \leq a_n$, by Theorem 2 we can conclude
$$
\lims s_n \leq \lims a_n = \limm a_n
$$
where the last equality follows from Theorem 3. We will now show that
$$
\lims s_n \geq \limm a_n
$$
which will give us our required equality. If $\limm a_n = -\infty$ there is nothing to prove and so we assume that $a &amp;gt; -\infty$. Let $\e &amp;gt; 0$ be given and let
$$
B = \{ n \in \N : s_n  \geq a - \e \}
$$
We claim that $B$ is infinite. Indeed, if $B$ were finite we can find $N \in \N$ so that $N \geq \max(B)$. This will imply that $s_n \leq a - \e$ for all $n \geq N$ and so $a_n \leq a - \e$ for $n \geq N$. But then by Theorem 2 we would conclude $a = \limm a_n \leq a-\e$, which is absurd. Thus $B$ is infinite and we let $\snk$ be a subsequence of $\sn$ with $n_k \in B$. Notice that $\lims s_{n_k} \leq \lims s_n$, since any subsequential limit of $\snk$ is also a subsequential limit of $\sn$. This along with Theorem 2 give
$$
a - \e \leq \lims s_{n_k} \leq \lims s_n
$$
Since $\e$ was arbitrary we conclude that  $\lims s_n \geq a$, which is what we wanted. $\square$&lt;/p&gt;
&lt;h2 id=&#34;more-properties&#34;&gt;More properties&lt;/h2&gt;
&lt;p&gt;These theorems open new avenues to discover more properties of upper and lower limits.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 5:&lt;/strong&gt; Let $\sn$ be a sequence of real numbers. Then
$$\limi s_n = - \lims (-s_n)$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; TODO&lt;/p&gt;
&lt;p&gt;Subadditivity of $\limsup$:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 6:&lt;/strong&gt; For any two real sequences $\{a_n\}$ and $\{b_n\}$,
$$
\lims (a_n + b_n) \leq \lims a_n + \lims b_n
$$
provided the sum on the right is not of the form $\infty - \infty$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; If $\lims a_n = \infty$ then as by assumption the right side is not $\infty - \infty$, it is $\infty$ and there is nothing to prove. Similarly for the case $\lims b_n = \infty$. We may thus assume that
$$
\lims a_n = A &amp;lt; \infty \text{ and } \lims b_n = B &amp;lt; \infty
$$
We note that $\sup_{k \geq 1} a_k &amp;lt; \infty$, $\sup_{k \geq 1} b_k &amp;lt; \infty$, and so TODO&lt;/p&gt;
&lt;p&gt;Superadditivity of $\liminf$:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem 7:&lt;/strong&gt; For any two real sequences $\{a_n\}$ and $\{b_n\}$,
$$
\limi (a_n + b_n) \geq \limi a_n + \limi b_n
$$
provided the sum on the right is not of the form $\infty - \infty$.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof:&lt;/strong&gt; TODO&lt;/p&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Hilbert&#39;s Hotel</title>
      <link>https://makkar.github.io/hilbert/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://makkar.github.io/hilbert/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://makkar.github.io/research/bayesian-ensemble/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://makkar.github.io/research/bayesian-ensemble/</guid>
      <description>&lt;p&gt;Title:  Bayesian nonparametric ensemble
Date: June 17, 2020
Tags:  bayesian
Summary: This work improves upon the BNE model of Liu et al. (2019) using the methods proposed by Rahimi and Recht (2007).&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;$$
\mathbb{E}[X] = \int_{\Omega} X ;\text{dP}
$$&lt;/p&gt;
&lt;h1 id=&#34;problem-setting&#34;&gt;Problem Setting&lt;/h1&gt;
&lt;h2 id=&#34;bne&#34;&gt;BNE&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
